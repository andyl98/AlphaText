{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTR9DFWwYolq",
        "outputId": "75bffc23-74b4-4d79-a17d-bb659da9a779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import numpy as np  \n",
        "import pandas as pd \n",
        "import re\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords  \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXET-GeTY9I9"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/papers.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rROibTvtdlxA",
        "outputId": "c1e05c50-b78e-40ad-a2c1-cc6b8a5fb692"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['id', 'year', 'title', 'event_type', 'pdf_name', 'abstract', 'paper_text']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0Fc2-4qdoyp",
        "outputId": "8c38293d-5d69-4044-8d9a-119c04e2575d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Abstract Missing',\n",
              "       'Non-negative matrix factorization (NMF) has previously been shown to \\r\\nbe a useful decomposition for multivariate data. Two different multi- \\r\\nplicative algorithms for NMF are analyzed. They differ only slightly in \\r\\nthe multiplicative factor used in the update rules. One algorithm can be \\r\\nshown to minimize the conventional least squares error while the other \\r\\nminimizes the generalized Kullback-Leibler divergence. The monotonic \\r\\nconvergence of both algorithms can be proven using an auxiliary func- \\r\\ntion analogous to that used for proving convergence of the Expectation- \\r\\nMaximization algorithm. The algorithms can also be interpreted as diag- \\r\\nonally rescaled gradient descent, where the rescaling factor is optimally \\r\\nchosen to ensure convergence. ',\n",
              "       'Spike-triggered averaging techniques are effective for linear characterization of neural responses. But neurons exhibit important nonlinear behaviors, such as gain control, that are not captured by such analyses. We describe a spike-triggered covariance method for retrieving suppressive components of the gain control signal in a neuron. We demonstrate the method in simulation and on retinal ganglion cell data. Analysis of physiological data reveals significant suppressive axes and explains neural nonlinearities. This method should be applicable to other sensory areas and modalities.',\n",
              "       ...,\n",
              "       'Community detection, which focuses on clustering nodes or detecting communities in (mostly) a single network, is a problem of considerable practical interest and has received a great deal of attention in the  research community. While being able to cluster within a network is important, there are emerging needs to be able to \\\\emph{cluster multiple networks}. This is largely motivated by the routine collection of network data that are generated from potentially different populations. These networks may or may not have node correspondence. When node correspondence is present, we cluster networks by summarizing a network by its graphon estimate, whereas when node correspondence is not present, we propose a novel solution for clustering such networks by associating a computationally feasible feature vector to each network based on trace of powers of the adjacency matrix. We illustrate our methods using both simulated and real data sets, and theoretical justifications are provided in terms of consistency.',\n",
              "       \"We propose a general framework for interactively learning models, such as (binary or non-binary) classifiers, orderings/rankings of items, or clusterings of data points. Our framework is based on a generalization of Angluin's equivalence query model and Littlestone's online learning model: in each iteration, the algorithm proposes a model, and the user either accepts it or reveals a specific mistake in the proposal. The feedback is correct only with probability p > 1/2 (and adversarially incorrect with probability 1 - p), i.e., the algorithm must be able to learn in the presence of arbitrary noise. The algorithm's goal is to learn the ground truth model using few iterations.  Our general framework is based on a graph representation of the models and user feedback. To be able to learn efficiently, it is sufficient that there be a graph G whose nodes are the models, and (weighted) edges capture the user feedback, with the property that if s, s* are the proposed and target models, respectively, then any (correct) user feedback s' must lie on a shortest s-s* path in G. Under this one assumption, there is a natural algorithm, reminiscent of the Multiplicative Weights Update algorithm, which will efficiently learn s* even in the presence of noise in the user's feedback.  From this general result, we rederive with barely any extra effort classic results on learning of classifiers and a recent result on interactive clustering; in addition, we easily obtain new interactive learning algorithms for ordering/ranking.\",\n",
              "       'We consider maximum likelihood estimation of linear dynamical systems with generalized-linear observation models. Maximum likelihood is typically considered to be hard in this setting since latent states and transition parameters must be inferred jointly. Given that expectation-maximization does not scale and is prone to local minima, moment-matching approaches from the subspace identification literature have become standard, despite known statistical efficiency issues. In this paper, we instead reconsider likelihood maximization and develop an optimization based strategy for recovering the latent states and transition parameters. Key to the approach is a two-view reformulation of maximum likelihood estimation for linear dynamical systems that enables the use of global optimization algorithms for matrix factorization. We show that the proposed estimation strategy outperforms widely-used identification algorithms such as subspace identification methods, both in terms of accuracy and runtime.'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.abstract.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwVeGpHpd2SV"
      },
      "outputs": [],
      "source": [
        "data=data[data.abstract!='Abstract Missing']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ezcmt78nd-_Q",
        "outputId": "47df928a-58ea-453d-8ec7-37281cceb10a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Algorithms for Non-negative Matrix\\nFactorization\\n\\nDaniel D. Lee*\\n*BelJ Laboratories\\nLucent Technologies\\nMurray Hill, NJ 07974\\n\\nH. Sebastian Seung*t\\ntDept. of Brain and Cog. Sci.\\nMassachusetts Institute of Technology\\nCambridge, MA 02138\\n\\nAbstract\\nNon-negative matrix factorization (NMF) has previously been shown to\\nbe a useful decomposition for multivariate data. Two different multiplicative algorithms for NMF are analyzed. They differ only slightly in\\nthe multiplicative factor used in the update rules. One algorithm can be\\nshown to minimize the conventional least squares error while the other\\nminimizes the generalized Kullback-Leibler divergence. The monotonic\\nconvergence of both algorithms can be proven using an auxiliary function analogous to that used for proving convergence of the ExpectationMaximization algorithm. The algorithms can also be interpreted as diagonally rescaled gradient descent, where the rescaling factor is optimally\\nchosen to ensure convergence.\\n\\n1 Introduction\\nUnsupervised learning algorithms such as principal components analysis and vector quantization can be understood as factorizing a data matrix subject to different constraints. Depending upon the constraints utilized, the resulting factors can be shown to have very different representational properties. Principal components analysis enforces only a weak orthogonality constraint, resulting in a very distributed representation that uses cancellations\\nto generate variability [1, 2]. On the other hand, vector quantization uses a hard winnertake-all constraint that results in clustering the data into mutually exclusive prototypes [3].\\nWe have previously shown that nonnegativity is a useful constraint for matrix factorization\\nthat can learn a parts representation of the data [4, 5]. The nonnegative basis vectors that are\\nlearned are used in distributed, yet still sparse combinations to generate expressiveness in\\nthe reconstructions [6, 7]. In this submission, we analyze in detail two numerical algorithms\\nfor learning the optimal nonnegative factors from data.\\n\\n2 Non-negative matrix factorization\\nWe formally consider algorithms for solving the following problem:\\nNon-negative matrix factorization (NMF) Given a non-negative matrix\\nV, find non-negative matrix factors Wand H such that:\\nV~WH\\n\\n(1)\\n\\n\\x0cNMF can be applied to the statistical analysis of multivariate data in the following manner.\\nGiven a set of of multivariate n-dimensional data vectors, the vectors are placed in the\\ncolumns of an n x m matrix V where m is the number of examples in the data set. This\\nmatrix is then approximately factorized into an n x r matrix Wand an r x m matrix H.\\nUsually r is chosen to be smaller than nor m , so that Wand H are smaller than the original\\nmatrix V. This results in a compressed version of the original data matrix.\\nWhat is the significance of the approximation in Eq. (1)? It can be rewritten column by\\ncolumn as v ~ Wh, where v and h are the corresponding columns of V and H. In other\\nwords, each data vector v is approximated by a linear combination of the columns of W,\\nweighted by the components of h. Therefore W can be regarded as containing a basis\\nthat is optimized for the linear approximation of the data in V. Since relatively few basis\\nvectors are used to represent many data vectors, good approximation can only be achieved\\nif the basis vectors discover structure that is latent in the data.\\nThe present submission is not about applications of NMF, but focuses instead on the technical aspects of finding non-negative matrix factorizations. Of course, other types of matrix factorizations have been extensively studied in numerical linear algebra, but the nonnegativity constraint makes much of this previous work inapplicable to the present case\\n[8].\\n\\nHere we discuss two algorithms for NMF based on iterative updates of Wand H. Because\\nthese algorithms are easy to implement and their convergence properties are guaranteed,\\nwe have found them very useful in practical applications. Other algorithms may possibly\\nbe more efficient in overall computation time, but are more difficult to implement and may\\nnot generalize to different cost functions. Algorithms similar to ours where only one of the\\nfactors is adapted have previously been used for the deconvolution of emission tomography\\nand astronomical images [9, 10, 11, 12].\\nAt each iteration of our algorithms, the new value of W or H is found by multiplying the\\ncurrent value by some factor that depends on the quality ofthe approximation in Eq. (1). We\\nprove that the quality of the approximation improves monotonically with the application\\nof these multiplicative update rules. In practice, this means that repeated iteration of the\\nupdate rules is guaranteed to converge to a locally optimal matrix factorization.\\n\\n3 Cost functions\\nTo find an approximate factorization V ~ W H, we first need to define cost functions\\nthat quantify the quality of the approximation. Such a cost function can be constructed\\nusing some measure of distance between two non-negative matrices A and B . One useful\\nmeasure is simply the square of the Euclidean distance between A and B [13],\\n\\nIIA - BI12 = L(Aij -\\n\\nBij)2\\n\\n(2)\\n\\nij\\n\\nThis is lower bounded by zero, and clearly vanishes if and only if A = B .\\nAnother useful measure is\\nD(AIIB)\\n\\n=\\n\\n2:\\n\\nk?\\n( Aij log B:~\\n- Aij\\n\\n+ Bij )\\n\\n(3)\\n\\n\"J\\n\\nLike the Euclidean distance this is also lower bounded by zero, and vanishes if and only\\nif A = B . But it cannot be called a \"distance\", because it is not symmetric in A and B,\\nso we will refer to it as the \"divergence\" of A from B. It reduces to the Kullback-Leibler\\ndivergence, or relative entropy, when 2:ij Aij = 2:ij Bij = 1, so that A and B can be\\nregarded as normalized probability distributions.\\n\\n\\x0cWe now consider two alternative formulations of NMF as optimization problems:\\nProblem 1 Minimize\\n\\nIIV -\\n\\nW\\n\\nHI12 with\\n\\nrespect to Wand H, subject to the constraints\\n\\nW,H~O.\\n\\nProblem 2 Minimize D(VIIW H) with re.lpect to Wand H, subject to the constraints\\nW,H~O.\\n\\nAlthough the functions IIV - W HI12 and D(VIIW H) are convex in W only or H only, they\\nare not convex in both variables together. Therefore it is unrealistic to expect an algorithm\\nto solve Problems 1 and 2 in the sense of finding global minima. However, there are many\\ntechniques from numerical optimization that can be applied to find local minima.\\nGradient descent is perhaps the simplest technique to implement, but convergence can be\\nslow. Other methods such as conjugate gradient have faster convergence, at least in the\\nvicinity of local minima, but are more complicated to implement than gradient descent\\n[8] . The convergence of gradient based methods also have the disadvantage of being very\\nsensitive to the choice of step size, which can be very inconvenient for large applications.\\n\\n4 Multiplicative update rules\\nWe have found that the following \"multiplicative update rules\" are a good compromise\\nbetween speed and ease of implementation for solving Problems 1 and 2.\\nTheorem 1 The Euclidean distance II V - W H II is non increasing under the update rules\\n(WTV)att\\nHal\\' +- Hal\\' (WTWH)att\\n\\n(V HT)ia\\nWia +- Wia(WHHT)ia\\n\\n(4)\\n\\nThe Euclidean distance is invariant under these updates if and only if Wand H are at a\\nstationary point of the distance.\\n\\nTheorem 2 The divergence D(VIIW H) is nonincreasing under the update rules\\nH\\n\\natt +-\\n\\nH\\natt\\n\\n2:i WiaVitt/(WH)itt\\n\" W\\nL..Jk\\n\\nka\\n\\nWia +- Wia\\n\\n2:1\\' HattVitt/(WH)itt\\n\" H\\nL..Jv\\n\\nav\\n\\n(5)\\n\\nThe divergence is invariant under these updates if and only ifW and H are at a stationary\\npoint of the divergence.\\n\\nProofs of these theorems are given in a later section. For now, we note that each update\\nconsists of multiplication by a factor. In particular, it is straightforward to see that this\\nmultiplicative factor is unity when V = W H, so that perfect reconstruction is necessarily\\na fixed point of the update rules.\\n\\n5 Multiplicative versus additive update rules\\nIt is useful to contrast these multiplicative updates with those arising from gradient descent\\n[14]. In particular, a simple additive update for H that reduces the squared distance can be\\nwritten as\\n\\n(6)\\nIf \\'flatt are all set equal to some small positive number, this is equivalent to conventional\\ngradient descent. As long as this number is sufficiently small, the update should reduce\\nIIV - WHII?\\n\\n\\x0cNow if we diagonally rescale the variables and set\\n\\nHalt\\n\\n\"Ialt\\n\\n(7)\\n\\n= (WTW H)alt \\'\\n\\nthen we obtain the update rule for H that is given in Theorem 1. Note that this rescaling\\nresults in a multiplicative factor with the positive component of the gradient in the denominator and the absolute value of the negative component in the numerator of the factor.\\nFor the divergence, diagonally rescaled gradient descent takes the form\\n\\nHalt\\n\\nf-\\n\\nHalt\\n\\n+ \"Ialt\\n\\n[~Wia (:;;)ilt - ~ Wia].\\n\\n(8)\\n\\nAgain, if the \"Ialt are small and positive, this update should reduce D (V II W H). If we now\\nset\\n\\nHalt\\n\"Ialt= ui\\n~ W. \\'\\nza\\n\\n(9)\\n\\nthen we obtain the update rule for H that is given in Theorem 2. This rescaling can also\\nbe interpretated as a multiplicative rule with the positive component of the gradient in the\\ndenominator and negative component as the numerator of the multiplicative factor.\\nSince our choices for \"Ialt are not small, it may seem that there is no guarantee that such a\\nrescaled gradient descent should cause the cost function to decrease. Surprisingly, this is\\nindeed the case as shown in the next section.\\n\\n6 Proofs of convergence\\nTo prove Theorems 1 and 2, we will make use of an auxiliary function similar to that used\\nin the Expectation-Maximization algorithm [15, 16].\\nDefinition 1 G(h, h\\') is an auxiliary functionfor F(h)\\nG(h, h\\') ~ F(h),\\n\\nG(h, h)\\n\\nif the conditions\\n\\n= F(h)\\n\\n(10)\\n\\nare satisfied.\\n\\nThe auxiliary function is a useful concept because of the following lemma, which is also\\ngraphically illustrated in Fig. 1.\\nLemma 1 IfG is an auxiliary junction, then F is nonincreasing under the update\\nht+1 = argmlnG (h,ht )\\nProof: F(ht+1) ~ G(ht+1, ht) ~ G(ht, ht)\\n\\n(11)\\n\\n= F(ht) ?\\n\\nNote that F(ht+1) = F(ht) only if ht is a local minimum of G(h, ht). If the derivatives\\nof F exist and are continuous in a small neighborhood of ht , this also implies that the\\nderivatives \\'V F(ht) = O. Thus, by iterating the update in Eq. (11) we obtain a sequence\\nof estimates that converge to a local minimum h min = argminh F(h) of the objective\\nfunction:\\n\\nWe will show that by defining the appropriate auxiliary functions G(h, ht) for both IIV W HII and D(V, W H), the update rules in Theorems 1 and 2 easily follow from Eq. (11).\\n\\n\\x0cFigure 1: Minimizing the auxiliary function G(h, ht)\\nF(ht) for h n+1 = argminh G(h, ht).\\n\\n2:: F(h) guarantees that F(ht+1) :::;\\n\\nLemma 2 If K(ht) is the diagonal matrix\\nKab(ht) = <5ab(WTwht)a/h~\\nthen\\nG(h, ht)\\n\\n= F(ht) + (h -\\n\\n+ ~(h -\\n\\nht)T\\\\l F(ht)\\n\\n(13)\\n\\nht)T K(ht)(h - ht)\\n\\n(14)\\n\\nis an auxiliary function for\\nF(h) =\\n\\n~ ~)Vi -\\n\\na\\n\\nProof: Since G(h, h) = F(h) is obvious, we need only show that G(h, ht)\\ndo this, we compare\\nF(h) = F(ht)\\n\\n+ (h -\\n\\nhtf\\\\l F(ht)\\n\\n+ ~(h -\\n\\n2:: F(h). To\\n\\nht)T(WTW)(h - ht)\\n\\n2\\nwith Eq. (14) to find that G(h, ht) 2:: F(h) is equivalent to\\n\\n0:::;\\n\\n(15)\\n\\nW ia h a )2\\n\\nL\\n\\ni\\n\\n(h - htf[K(ht) - WTW](h - ht)\\n\\n(16)\\n\\n(17)\\n\\nTo prove positive semidefiniteness, consider the matrix 1:\\n(18)\\n\\nwhich is just a rescaling of the components of K - WTW. Then K - WTW is positive\\nsemidefinite if and only if M is, and\\nVT M v\\n\\n=\\n\\nL VaMabVb\\nab\\n\\n(19)\\n\\nL h~(WTW)abh~v~ - vah~(WTW)abh~Vb\\nab\\n\\n(20)\\n\\n\"\\nt t\\nL...J(W T W ) abhahb\\n\\n[1 + 1\\n2\" v a2\\n\\n2\" Vb2 - VaVb ]\\n\\n(21)\\n\\nab\\n\\n= ~ L(WTW)abh~h~(va -\\n\\nVb)2\\n\\n(22)\\n\\nab\\n\\n> 0\\n\\n(23)\\n\\n\\'One can also show that K - WTW is positive semidefinite by considering the matrix K (I1\\n2.\\nThen v. /M(WT W ht ) a is a positive eigenvector of K- 21 W T W K- with\\nunity eigenvalue, and application of the Frobenius-Perron theorem shows that Eq. 17 holds.\\n\\nK- 21 W TW K- 21) K\\n\\n\\x0c?\\n\\nWe can now demonstrate the convergence of Theorem 1:\\nProof of Theorem 1 Replacing G(h, ht) in Eq. (11) by Eq. (14) results in the update rule:\\nht+1 = ht - K(ht)-l\\\\1F(ht)\\n(24)\\nSince Eq. (14) is an auxiliary function, F is nonincreasing under this update rule, according\\nto Lemma 1. Writing the components of this equation explicitly, we obtain\\nht +1 = ht (WT V )a\\na\\na (WTWht)a .\\n\\n(25)\\n\\nBy reversing the roles of Wand H in Lemma 1 and 2, F can similarly be shown to be\\nnonincreasing under the update rules for W .?\\nWe now consider the following auxiliary function for the divergence cost function:\\nLemma 3 Define\\nG(h,ht)\\n\\n(26)\\n\\nia\\n\"\\nWiah~ (\\nWiah~ )\\n- ~ Vi,\"\", W - ht logWiaha -log,\"\", W - ht\\nia\\nub ,b b\\nub ,b b\\n\\n(27)\\n\\nThis is an auxiliary function for\\n\\nF(h)\\n\\n=L\\n\\nVi log\\ni\\n\\n(~ ~_\\na\\n\\n\\'l,a\\n\\nh ) - Vi\\n\\n+ LWiaha\\n\\na\\n\\nProof: It is straightforward to verify that G(h, h) = F(h) . To show that G(h, ht)\\nwe use convexity of the log function to derive the inequality\\nW\\niaha\\n-log \"~ Wiaha ::; - \"\\n~\\nQ a log - a\\n\\n(28)\\n\\na\\n\\n2: F(h),\\n(29)\\n\\nQa\\n\\na\\n\\nwhich holds for all nonnegative Q a that sum to unity. Setting\\nWiah~\\n\\nQ\\n\\na\\n\\n(30)\\n\\n= \\'ub\\n\"\\'\" Wibhbt\\n\\nwe obtain\\n-log \"~ Wiaha ::; - \"~ \\'\"\\'\"Wiah~\\nW- ht ( log Wiaha - log,\"\",Wiah~\\nW- ht )\\na\\na ub ,b b\\nub ,b b\\n\\n(31)\\n\\nFrom this inequality it follows that F(h) ::; G(h, ht) . ?\\nTheorem 2 then follows from the application of Lemma 1:\\nProof of Theorem 2: The minimum of G(h, ht) with respect to h is determined by setting\\nthe gradient to zero:\\n_dG---,(,---,h,_h--,-t) __ \"\\n_ Wiah~ 1\\n~v,\\nt\\ndha\\n_\\n, ~b Wibhb ha\\n\\n\"W- - 0\\n\\n+~\\n\\n,-\\n\\nza-\\n\\n(32)\\n\\nThus, the update rule of Eq. (11) takes the form\\nt+1\\nha\\n\\nh~\"\\n\\nVi\\n\\n= ub\\n\\'\"\\'\" wkb ~\\n\\'\"\\'\" W-,b htbW ia ?\\ni\\nub\\n\\n(33)\\n\\nSince G is an auxiliary function, F in Eq. (28) is nonincreasing under this update. Rewritten in matrix form, this is equivalent to the update rule in Eq. (5). By reversing the roles of\\nHand W, the update rule for W can similarly be shown to be nonincreasing .?\\n\\n\\x0c7 Discussion\\nWe have shown that application of the update rules in Eqs. (4) and (5) are guaranteed to\\nfind at least locally optimal solutions of Problems 1 and 2, respectively. The convergence\\nproofs rely upon defining an appropriate auxiliary function . We are currently working to\\ngeneralize these theorems to more complex constraints. The update rules themselves are\\nextremely easy to implement computationally, and will hopefully be utilized by others for\\na wide variety of applications.\\nWe acknowledge the support of Bell Laboratories. We would also like to thank Carlos\\nBrody, Ken Clarkson, Corinna Cortes, Roland Freund, Linda Kaufman, Yann Le Cun, Sam\\nRowei s, Larry Saul, and Margaret Wright for helpful discussions.\\n\\nReferences\\n[1] Jolliffe, IT (1986). Principal Component Analysis. New York: Springer-Verlag.\\n[2] Turk, M & Pentland, A (1991). Eigenfaces for recognition. J. Cogn. Neurosci. 3, 71- 86.\\n[3] Gersho, A & Gray, RM (1992). Vector Quantization and Signal Compression. Kluwer Acad.\\nPress.\\n[4] Lee, DD & Seung, HS . Unsupervised learning by convex and conic coding (1997). Proceedings\\nof the Conference on Neural Information Processing Systems 9, 515- 521.\\n[5] Lee, DD & Seung, HS (1999). Learning the parts of objects by non-negative matrix factorization. Nature 401, 788- 791.\\n[6] Field, DJ (1994). What is the goal of sensory coding? Neural Comput. 6, 559-601.\\n[7] Foldiak, P & Young, M (1995). Sparse coding in the primate cortex. The Handbook of Brain\\nTheory and Neural Networks, 895- 898. (MIT Press, Cambridge, MA).\\n[8] Press, WH, Teukolsky, SA, Vetterling, WT & Flannery, BP (1993). Numerical recipes: the art\\nof scientific computing. (Cambridge University Press, Cambridge, England).\\n[9] Shepp, LA & Vardi, Y (1982) . Maximum likelihood reconstruction for emission tomography.\\nIEEE Trans . MI-2, 113- 122.\\n[10] Richardson, WH (1972) . Bayesian-based iterative method of image restoration. 1. Opt. Soc.\\nAm. 62, 55- 59.\\n\\n[11] Lucy, LB (1974). An iterative technique for the rectification of observed distributions. Astron.\\nJ. 74, 745- 754.\\n[12] Bouman, CA & Sauer, K (1996). A unified approach to statistical tomography using coordinate\\ndescent optimization. IEEE Trans. Image Proc. 5, 480--492.\\n[13] Paatero, P & Tapper, U (1997). Least squares formulation of robust non-negative factor analysis. Chemometr. Intell. Lab. 37, 23- 35.\\n[14] Kivinen, J & Warmuth, M (1997). Additive versus exponentiated gradient updates for linear\\nprediction. Journal of Tnformation and Computation 132, 1-64.\\n[15] Dempster, AP, Laird, NM & Rubin, DB (1977). Maximum likelihood from incomplete data via\\nthe EM algorithm. J. Royal Stat. Soc. 39, 1-38.\\n[16] Saul, L & Pereira, F (1997). Aggregate and mixed-order Markov models for statistical language\\nprocessing. In C. Cardie and R. Weischedel (eds). Proceedings of the Second Conference on\\nEmpirical Methods in Natural Language Processing, 81- 89. ACL Press.\\n\\n\\x0c',\n",
              "       'Characterizing neural gain control using\\nspike-triggered covariance\\n\\nOdelia Schwartz\\nCenter for Neural Science\\nNew York University\\nodelia@cns.nyu.edu\\n\\nE. J. Chichilnisky\\nSystems Neurobiology\\nThe Salk Institute\\nej@salk.edu\\n\\nEero P. Simoncelli\\nHoward Hughes Medical Inst.\\nCenter for Neural Science\\nNew York University\\neero.simoncelli@nyu.edu\\n\\nAbstract\\nSpike-triggered averaging techniques are effective for linear characterization of neural responses. But neurons exhibit important nonlinear behaviors, such as gain control, that are not captured by such analyses.\\nWe describe a spike-triggered covariance method for retrieving suppressive components of the gain control signal in a neuron. We demonstrate\\nthe method in simulation and on retinal ganglion cell data. Analysis\\nof physiological data reveals significant suppressive axes and explains\\nneural nonlinearities. This method should be applicable to other sensory\\nareas and modalities.\\nWhite noise analysis has emerged as a powerful technique for characterizing response properties of spiking neurons. A sequence of stimuli are drawn randomly from an ensemble and\\npresented in rapid succession, and one examines the subset that elicit action potentials. This\\n?spike-triggered? stimulus ensemble can provide information about the neuron?s response\\ncharacteristics. In the most widely used form of this analysis, one estimates an excitatory\\nlinear kernel by computing the spike-triggered average (STA); that is, the mean stimulus\\nthat elicited a spike [e.g., 1, 2]. Under the assumption that spikes are generated by a\\nPoisson process with instantaneous rate determined by linear projection onto a kernel followed by a static nonlinearity, the STA provides an unbiased estimate of this kernel [3].\\nRecently, a number of authors have developed interesting extensions of white noise analysis. Some have examined spike-triggered averages in a reduced linear subspace of input\\nstimuli [e.g., 4]. Others have recovered excitatory subspaces, by computing the spiketriggered covariance (STC), followed by an eigenvector analysis to determine the subspace\\naxes [e.g., 5, 6].\\nSensory neurons exhibit striking nonlinear behaviors that are not explained by fundamentally linear mechanisms. For example, the response of a neuron typically saturates for large\\namplitude stimuli; the response to the optimal stimulus is often suppressed by the presence\\nof a non-optimal mask [e.g., 7]; and the kernel recovered from STA analysis may change\\nshape as a function of stimulus amplitude [e.g., 8, 9]. A variety of these nonlinear behaviors can be attributed to gain control [e.g., 8, 10, 11, 12, 13, 14], in which neural responses\\nare suppressively modulated by a gain signal derived from the stimulus. Although the underlying mechanisms and time scales associated with such gain control are current topics\\nof research, the basic functional properties appear to be ubiquitous, occurring throughout\\nthe nervous system.\\n\\n\\x0ca\\n\\nb\\n\\n0\\n\\nk0\\n\\n0\\n\\nFigure 1: Geometric depiction of spike-triggered analyses. a, Spike-triggered averaging\\nwith two-dimensional stimuli. Black points indicate raw stimuli. White points indicate stim\\x01\\x03\\x02\\nuli eliciting a spike, and the STA (black vector), which provides an estimate of , corresponds to their center of mass. b, Spike-triggered covariance analysis of suppressive axes.\\n\\x01 \\x02\\nShown are a set of stimuli lying on a plane perpendicular to the excitatory kernel, . Within\\nthe plane, stimuli eliciting a spike are concentrated in an elliptical region. The minor axis of\\nthe ellipse corresponds to a suppressive stimulus direction: stimuli with a significant component along this axis are less likely to elicit spikes. The stimulus component along the major\\naxis of the ellipse has no influence on spiking.\\n\\nHere we develop a white noise methodology for characterizing a neuron with gain control.\\nWe show that a set of suppressive kernels may be recovered by finding the eigenvectors of\\nthe spike-triggered covariance matrix associated with smallest variance. We apply the technique to electrophysiological data obtained from ganglion cells in salamander and macaque\\nretina, and recover a set of axes that are shown to reduce responses in the neuron. Moreover, when we fit a gain control model to the data using a maximum likelihood procedure\\nwithin this subspace, the model accounts for changes in the STA as a function of contrast.\\n\\n1 Characterizing suppressive axes\\nAs in all white noise approaches, we assume that stimuli correspond to vectors, \\x05 \\x04 , in some\\nfinite-dimensional space (e.g., a neighborhood of pixels or an interval of time samples).\\nWe assume a gain control model in which the probability of a stimulus eliciting a spike\\ngrows monotonically with the halfwave-rectified projection onto an excitatory linear kernel,\\n\\x06 \\x07\\t\\x08\\x0b\\n \\x05\\n\\x0c\\n\\x04\\n\\x04 , and is suppressively modulated by the fullwave-rectified projection onto a set of\\n\\x07\\t\\x0f\\x10\\n \\x05\\nlinear kernels, \\x0e \\x04\\n\\x04 \\x0e.\\n\\x07 \\x08\\n\\nFirst, we recover the excitatory kernel, \\x04 . This is achieved by presenting spherically symmetric input stimuli (e.g., Gaussian white noise) to the neuron and computing the STA\\n(Fig. 1a). STA correctly recovers the excitatory kernel, under the assumption that each\\nof the gain control kernels are orthogonal (or equal) to the excitatory kernel. The proof\\nis essentially the same as that given for recovering the kernel of a linear model followed\\nby a monotonic nonlinearity [3]. In particular, any stimulus can be decomposed into a\\ncomponent in the direction of the excitatory kernel, and a component in a perpendicular\\ndirection. This can be paired with another stimulus that is identical, except that its component in the perpendicular direction is negated. The two stimuli are equally likely to occur\\nin a spherically Gaussian stimulus set (since they are equidistant from the origin), and they\\nare equally likely to elicit a spike (since their excitatory components are equal, and their\\nrectified perpendicular components are equal). Their vector average lies in the direction of\\nthe excitatory kernel. Thus, the STA (which is an average over all such stimuli, or all such\\nstimulus pairs) must also lie in that direction. In a subsequent section we explain how to\\n\\n\\x0cModel:\\n\\nRetrieved:\\n\\nExcitatory:\\n\\nExcitatory:\\n\\nEigenvalues:\\n\\n{\\n1.5 {\\n2{\\n2.5 {\\n3{\\n1\\n\\nVariance (eigenvalue)\\n\\nSuppressive: Suppressive:\\nWeights\\n\\n1\\nArbitrary\\n\\n0\\n\\nAxis number\\n\\n350\\n\\nFigure 2: Estimation of kernels from a simulated model (equation 2). Left: Model kernels.\\nRight: Sorted eigenvalues of covariance matrix of stimuli eliciting spikes (STC). Five eigenvalues fall significantly below the others. Middle: STA (excitatory kernel) and eigenvectors\\n(suppressive kernels) associated with the lowest eigenvalues.\\n\\nrecover the excitatory kernel when it is not orthogonal to the suppressive kernels.\\nNext, we recover the suppressive subspace, assuming the excitatory kernel is known. Consider the stimuli lying on a plane perpendicular to this kernel. These stimuli all elicit the\\nsame response in the excitatory kernel, but they may produce different amounts of suppression. Figure 1b illustrates the behavior in a three-dimensional stimulus space, in which one\\naxis is assumed to be suppressive. The distribution of raw stimuli on the plane is spherically symmetric about the origin. But the distribution of stimuli eliciting a spike is narrower\\nalong the suppressive direction: these stimuli have a component along the suppressive axis\\nand are therefore less likely to elicit a spike. This behavior is easily generalized from this\\nplane to the entire stimulus space. If we assume that the suppressive axes are fixed, then\\nwe expect to see reductions in variance in the same directions for any level of numerator\\nexcitation.\\nGiven this behavior of the spike-triggered stimulus ensemble, we can recover the suppressive subspace using principal component analysis. We construct the sample covariance\\nmatrix of the stimuli eliciting a spike:\\n\\n\\x04\\x05\\n\\n\\x02\\x01 \\x04\\x06\\x05\\x08\\x03 \\x07 \\t \\x05 \\x04 \\x05\\x15\\x04 \\x14\\x08\\x16\\n\\x03 \\x05\\x0c\\n \\x0e\\x0b \\n\\x10\\x0f \\x11\\x13\\x12\\n\\n(1)\\n\\n\\x15\\x17\\x19\\x18\\x1b\\x1a \\x1c\\x1e\\x1d\\n\\nwhere\\nis the number of spikes. To ensure the estimated suppressive subspace is or\\x07 \\x08\\nthogonal to the estimated \\x04 (as in Figure 1b), the stimuli \\x05 \\x04\\nare first projected onto the\\n\\x07\\t\\x04 \\x08\\nsubspace perpendicular to the estimated . The principal axes (eigenvectors) of that are\\nassociated with small variance (eigenvalues) correspond to directions in which the response\\nof the neuron is modulated suppressively.\\nWe illustrate the technique on simulated data for a neuron with a spatio-temporal receptive\\nfield. The kernels are a set of orthogonal bandpass filters. The stimulus vectors \\x05 \\x04 of this\\ninput sequence are defined over a 18-sample spatial region and a 18-sample time window\\n(i.e., a\\n-dimensional space). Spikes are generated using a Poisson process with mean\\nrate determined by a specific form of gain control [14]:\\n\\n\\x1f!#\"\\n\\n$&% \\x05\\x1e\\')( \\x07+* \\x0e -\\x05 \\x04 , \\x01 / \\x0f \\x06 \\x07 \\x04 \\x07 \\x08 \\x0f\\x10\\n \\n\\x05 \\x04 \\x0c\\x15.\\n(2)\\n\\x0f10\\n\\x05 \\x04 \\x0e .1243 .65\\n\\x0e\\x04\\n\\x07 \\x08\\nThe goal of simulation is to recover excitatory kernel \\x04 , the suppressive subspace spanned\\n\\x07\\t\\x04 \\x0f\\n\\x0f\\n0\\nby , weights , and constant 3 .\\n\\n\\x0cRetrieved kernels:\\n\\nEigenvalues:\\nactual\\n95 % confidence\\n\\n0\\n\\nVariance (eigenvalue)\\n\\nExcitatory:\\n\\nSuppressive:\\n\\n1\\n\\nArbitrary\\n\\n0\\n\\nAxis number\\n\\n26\\n\\nFigure 3: Left: Retrieved kernels from STA and STC analysis of ganglion cell data from a\\nsalamander retina (cell 1999-11-12-B6A). Right: sorted eigenvalues of the spike-triggered\\ncovariance matrix, with corresponding eigenvectors. Low eigenvalues correspond to suppressive directions, while other eigenvalues correspond to arbitrary (ignored) directions. Raw\\nstimulus ensemble was sphered (whitened) prior to analysis and low-variance axes underrepresented in stimulus set were discarded.\\n\\nFigure 2 shows the original and estimated kernels for a model simulation with 600K input\\nsamples and 36.98K spikes. First, we note that STA recovers an accurate estimate of the\\nexcitatory kernel. Next, consider the sorted eigenvalues of , as plotted in Figure 2. The\\nmajority of the eigenvalues descend gradually (the covariance matrix of the white noise\\nsource should have constant eigenvalues, but remember that those in Figure 2 are computed\\nfrom a finite set of samples). The last five eigenvalues are significantly below the values\\none would obtain with randomly selected stimulus subsets. The eigenvectors associated\\nwith these lowest eigenvalues span approximately the same subspace as the suppressive\\nkernels. Note that some eigenvectors correspond to mixtures of the original suppressive\\nkernels, due to non-uniqueness of the eigenvector decomposition. In contrast, eigenvectors\\ncorresponding to eigenvalues in the gradually-descending region appear arbitrary in their\\nstructure.\\n\\n0\\n\\n\\x0f\\n\\n3\\n\\nFinally, we can recover the scalar parameters of this specific model ( and ) by selecting\\nthem to maximize the likelihood of the spike data according to equation (2). Note that a\\ndirect maximum likelihood solution on the raw data would have been impractical due to\\nthe high dimensionality of the stimulus space.\\n\\n2 Suppressive Axes in Retinal Ganglion Cells\\nRetinal ganglion cells exhibit rapid [8, 15] as well as slow [9, 16, 17] gain control. We now\\ndemonstrate that we can recover a rapid gain control signal by applying the method to data\\nfrom salamander retina [9]. The input sequence consists of 80K time samples of full-field\\n33Hz flickering binary white noise (contrast 8.5%). The stimulus vectors \\x05 \\x04 of this sequence\\nare defined over a 60-segment time window. Since stimuli are finite in number and binary,\\nthey are not spherically distributed. To correct for this, we discard low-variance axes and\\nwhiten the stimuli within the remaining axes.\\nFigure 3 depicts the kernels estimated from the 623 stimulus vectors eliciting spikes. Similar to the model simulation, the eigenvalues gradually fall off, but four of the eigenvalues\\nappear to drop significantly below the rest. To make this more concrete, we test the hypothesis that the majority of the eigenvalues are consistent with those of randomly selected\\nstimulus vectors, but that the last eigenvalues fall significantly below this range. Specifically, we perform a Monte Carlo simulation, drawing (with replacement) random subsets\\nof 623 stimuli from the full set of raw stimuli. We also randomly select (orthogonal)\\n\\n\"\\n\\n\"\\n\\n\\x0c0\\n\\nprojection onto suppressive kernel\\n\\nprojection onto excitatory kernel\\n\\na\\n0. 5\\n\\nb\\n0. 5\\n\\n0\\n\\n-0. 5\\n\\n-0. 5\\n-0. 5\\n\\n0\\n\\n0. 5\\n\\nprojection onto arbitrary kernel\\n\\n-0. 5\\n\\n0\\n\\n0. 5\\n\\nprojection onto arbitrary kernel\\n\\nFigure 4: Scatter plots from salamander ganglion cell data (cell 1999-11-12-B6A). Black\\npoints indicate the raw stimulus set. White points indicate stimuli eliciting a spike. a, Projection of stimuli onto estimated excitatory kernel vs. arbitrary kernel. b, Projection of\\nstimuli onto an estimated suppressive kernel vs. arbitrary kernel.\\n\\naxes, representing a suppressive subspace, and project this subspace out of the set of randomly chosen stimuli. We then compute the eigenvalues of the sample covariance matrix\\nof these stimuli. We repeat this \\x01\\x02\\x03 times, and estimate a 95 percent confidence interval\\nfor each of the eigenvalues. The figure shows that the first eigenvalues lie within the confidence interval. In practice, we repeat this process in a nested fashion, assuming initially no\\ndirections are significantly suppressive, then one direction, and so on up to four directions.\\n\\n\\x03\\n\\nThese low eigenvalues correspond to eigenvectors that are concentrated in recent time (as is\\nthe estimated excitatory kernel). The remaining eigenvectors appear to be arbitrary, spanning the full temporal window. We emphasize that these kernels should not be interpreted\\nto correspond to receptive fields of individual neurons underlying the suppressive signal,\\nbut merely provide an orthogonal basis for a suppressive subspace.\\nWe can now verify that the recovered STA axis is in fact excitatory, and the kernels corresponding to the lowest eigenvalues are suppressive. Figure 4a shows a scatter plot of the\\nstimuli projected onto the excitatory axis vs. an arbitrary axis. Spikes are seen to occur\\nonly when the component along the excitatory axis is high, as expected. Figure 4b is a\\nscatter plot of the stimuli projected onto one of the suppressive axes vs. an arbitrary (ignored) axis. The spiking stimuli lie within an ellipse, with the minor axis corresponding to\\nthe suppressive kernel. This is exactly what we would expect in a suppressive gain control\\nsystem (see Figure 1b).\\nFigure 5 illustrates recovery of a two-dimensional suppressive subspace for a macaque retinal ganglion cell. The subspace was computed from the 36.43K stimulus vectors eliciting\\nspikes out of a total of 284.74K vectors. The data are qualitatively similar to those of the\\nsalamander cell, although both the strength of suppression and specific shapes of the scatter\\nplots differs. In addition to suppression, the method recovers facilitation (i.e., high-variance\\naxes) in some cells (not shown here).\\n\\n3 Correcting for Bias in Kernel Estimates\\nThe kernels in the previous section were all recovered from stimuli of a single contrast.\\nHowever, when the STA is computed in a ganglion cell for low and high contrast stimuli,\\nthe low-contrast kernel shows a slower time course [9] (figure 7,a). This would appear\\ninconsistent with the method we describe, in which the STA is meant to provide an estimate\\nof a single excitatory kernel. This behavior can be explained by assuming a model of the\\nform given in equation 2, and in addition dropping the constraint that the gain control\\nkernels are orthogonal (or identical) to the excitatory kernel.\\n\\n\\x0ca\\n\\n1\\n\\n\\x02\\x01\\x04\\x03 \\x05\\x07\\x06\\t\\x08\\x0b\\n\\x07\\x0c\\t\\n\\x0b\\x0e\\n\\n60\\n\\n0.5\\n\\n0\\n\\nprojection onto suppressive kernel\\n\\nactual\\n95% confidence\\n\\n0\\n\\nc\\n\\nb\\nprojection onto excitatory kernel\\n\\n%\\x11&\\t\\'\\t\\' \" (\\x13)*)\\x13\\x1e +\\x13(\\x1b$\\n\\nVariance (eigenvalue)\\n\\n\\x0f\\x11\\x10\\x13\\x12 \\x14\\x16\\x15 \\x10\\x13\\x17\\x04\\x10\\x04\\x18\\n\\x19\\x1b\\x1a\\x1d\\x1c\\x1d\\x1e \\x1f \\x1d\\x1f !\\x0b\" #\\x0b$\\n\\n0.5\\n\\n0\\n\\n-0. 5\\n\\n-0. 5\\n0. 5\\n\\n0\\n\\n0.5\\n\\nprojection onto arbitrary kernel\\n\\n0. 5\\n\\n0\\n\\n0.5\\n\\nprojection onto arbitrary kernel\\n\\nFigure 5: a, Sorted eigenvalues of stimuli eliciting spikes from a macaque retina (cell 200109-29-E6A). b-c, Scatter plots of stimuli projected onto recovered axes.\\nk0\\n\\nGain kernel\\n\\nSTA estimate\\n\\nFigure 6: Demonstration of estimator bias. When a gain control kernel is not orthogonal to\\nthe excitatory kernel, the responses to one side of the excitatory kernel are suppressed more\\nthan those on the other side. The resulting STA estimate is thus biased away from the true\\n\\x01 \\x02\\nexcitatory kernel, .\\n\\nFirst we show that when the orthogonality constraint is dropped, the STA estimate of the\\nexcitatory kernel is biased by the gain control signal. Consider a situation in which a\\n\\x07 \\x08\\nsuppressive kernel contains a component in the direction of the excitatory kernel, \\x04 . We\\n,\\n\\x08\\n\\x07 \\x0f\\n\\x07\\x04\\n\\x07/\\x04 \\x0f .\\n\\x07/\\x0f .\\nwrite \\x04\\n, where \\x04 is perpendicular to the excitatory kernel. Then, for example,\\n\\x07 \\x04 \\x08 10 \\x072\\x04 \\x0f .\\n\\x07 \\x0f\\n\\x05\\na stimulus \\x04\\n, with 043 , produces a suppressive component along \\x04 equal to\\n\\x08\\n, \\x07 \\x08\\n\\x07\\n2\\n\\x07\\n2\\n\\x07\\n.\\n\\x0f\\n0 \\x04 \\x0f.\\n\\x04\\nproduces\\n\\x0e \\x0e \\x04 \\x0e \\x0e 50 \\x0e \\x0e \\x04 \\x0e \\x0e , but the corresponding paired stimulus vector \\x05 \\x04\\n, \\x07\\t\\x08\\n/\\n\\x07\\n.\\n\\x0f\\n0\\n\\x0e \\x0e \\x04 \\x0e \\x0e . Thus, the two stimuli are equally likely\\na suppressive component of \\x0e \\x0e \\x04 \\x0e \\x0e\\nto occur but not equally likely to elicit a spike. As a result, the STA will be biased in the\\n\\x072\\x0f .\\ndirection \\x04 . Figure 6 illustrates an example in which a non-orthogonal suppressive axis\\nbiases the estimate of the STA.\\n\\n\\x01\\n\\n.2\\n\\n\\x01 2 2\\n\\n\\x07\\n\\n.\\n\\n.\\x07\\n\\n.\\n\\n\\x01\\n\\n\\x07\\n\\nNow consider the model in equation 2 in the presence of a non-orthogonal suppressive\\nsubspace. Note that the bias is stronger for larger amplitude stimuli because the constant\\nterm\\ndominates the gain control signal for weak stimuli. Indeed, we have previously\\nhypothesized that changes in receptive field tuning can arise from divisive gain control\\nmodels that include an additive constant [14].\\n\\n3.\\n\\nEven when the STA estimate is biased by the gain control signal, we can still obtain an\\n(asymptotically) unbiased estimate of the excitatory kernel. Specifically, the true excitatory kernel lies within the subspace spanned by the estimated (biased) excitatory and\\nsuppressive kernels. So, assuming a particular gain control model, we can again maximize\\nthe likelihood of the data, but now allowing both the excitatory and suppressive kernels to\\nmove within the subspace spanned by the initial estimated kernels. The resulting suppres-\\n\\n\\x0ca\\n\\nb\\n\\n0.1\\n\\n0.1\\n\\n0\\n\\n0\\n\\nc\\nMn\\nExcitatory:\\n\\nSuppressive:\\n\\n-0.5\\n-1.8\\n\\nLow contrast STA\\nHigh contrast STA\\n\\nTime preceding spike (sec)\\n\\n-0.5\\n0\\n-1.8\\n\\n{\\n{\\n{\\n\\nLow contrast STA\\nHigh contrast STA\\n\\nTime preceding spike (sec)\\n\\n0.99\\n\\n0.97\\n\\n{\\n{\\n\\nWeights\\n0.52\\n\\n0.46\\n\\n0.87\\n\\n0\\n\\nFigure 7: STA kernels estimated from low (8.5%) and high (34%) contrast salamander retinal ganglion cell data (cell 1999-11-12-B6A). Kernels are normalized to unit energy. a, STA\\nkernels derived from ganglion cell spikes. b, STA kernels derived from simulated spikes\\nusing ML-estimated model. c, Kernels and corresponding weights of ML-estimated model.\\n\\nsive kernels need not be orthogonal to the excitatory kernel.\\nWe maximize the likelihood of the full two-contrast data set using a model that is a generalization of that given by equation (2):\\n\\n$ % \\x05\\x1e\\')( \\x07+* \\x0e \\x15\\x05 \\x04 , \\x01 /\\n% \\x0f10\\n\\n\\'\\n\\n\\x06 \\x07 \\x08 \\n \\x05 \\x0c\\x01\\n\\x04\\n\\x04\\n\\x02\\n\\x0f \\x07\\x04 \\x0f \\n \\x05\\n\\x0e\\n\\x04 \\x0e\\n\\n. , . 42 3\\n\\n(3)\\n\\nThe exponent is incorporated to allow for more realistic contrast-response functions.\\nThe excitatory axis is initially set to the STA and the suppressive axes are set to the\\nlow-eigenvalue eigenvectors of the STC, along with the STA (e.g., to allow for selfsuppression). The recovered\\naxes and weights are shown in Figure 7b, and remaining model\\n\\x04\\x03\\n\\x06\\x05 ,\\nparameters are:\\n\\x08\\x07 . Whereas the axes recovered from the STA/STC\\nanalysis are orthogonal, the axes determined during the maximum likelihood stage need not\\nbe (and in the data example are not) orthogonal. Figure 7b also demonstrates that the fitted\\nmodel accounts for the change in STA observed at different contrast levels. Specifically,\\nwe simulate responses of the model (equation (3) with Poisson spike generation) on each\\nof the two contrast stimulus sets, and then compute the STA based on these simulated spike\\ntrains. Although it is based on a single fixed excitatory kernel, the model exhibits a change\\nin STA shape as a function of contrast very much like the salamander neuron.\\n\\n\\' \\x01 \\x03 3 \\x01 \"\\x03\\n5\\n5\\n\\n4 Discussion\\nWe have described a spike-triggered covariance method for characterizing a neuron with\\ngain control, and demonstrated the plausibility of the technique through simulation and\\nanalysis of neural data. The suppressive axes recovered from retinal ganglion cell data\\nappear to be significant because: (1) As in the model simulation, a small number of eigenvalues are significantly below the rest; (2) The eigenvectors associated with these axes are\\nconcentrated in a temporal region immediately preceding the spike, unlike the remaining\\naxes; (3) Projection of the multi-dimensional stimulus vectors onto these axes reveal reductions of spike probability; (4) The full model, with parameters recovered through maximum\\nlikelihood, explains changes in STA as a function of contrast.\\nModels of retinal processing often incorporate gain control [e.g., 8, 10, 15, 17, 18]. We\\nhave shown for the first time how one can use white noise analysis to recover a gain control subspace. The kernels defining this subspace correspond to relatively short timescales.\\nThus, it is interesting to compare the recovered subspace to models of rapid gain control.\\nIn particular, Victor [15] proposed a retinal gain model in which the gain signal consists\\n\\n\\x0cof time-delayed copies of the excitatory kernel. In fact, for the cell shown in Figure 3,\\nthe recovered suppressive subspace lies within the space spanned by shifted copies of the\\nexcitatory kernel. The fact that we do not see evidence for slow gain control in the analysis\\nmight indicate that these signals do not lie within a low-dimensional stimulus subspace. In\\naddition, the analysis is not capable of distinguishing between physiological mechanisms\\nthat could underlie gain control behaviors. Potential candidates may include internal biochemical adjustments, non-Poisson spike generation mechanisms, synaptic depression, and\\nshunting inhibition due to other neurons.\\nThis technique should be applicable to a far wider range of neural data than has been\\nshown here. Future work will incorporate analysis of data gathered using stimuli that vary\\nin both time and space (as in the simulated example of Figure 2). We are also exploring\\napplicability of the technique to other visual areas.\\nAcknowledgments We thank Liam Paninski and Jonathan Pillow for helpful discussions\\nand comments, and Divya Chander for data collection.\\n\\nReferences\\n[1] E deBoer and P Kuyper. Triggered correlation. In IEEE Transact. Biomed. Eng., volume 15,\\npages 169?179, 1968.\\n[2] J P Jones and L A Palmer. The two-dimensional spatial structure of simple receptive fields in\\nthe cat striate cortex. J Neurophysiology, 58:1187?11211, 1987.\\n[3] E J Chichilnisky. A simple white noise analysis of neuronal light responses. Network: Computation in Neural Systems, 12(2):199?213, 2001.\\n[4] D L Ringach, G Sapiro, and R Shapley. A subspace reverse-correlation technique for the study\\nof visual neurons. Vision Research, 37:2455?2464, 1997.\\n[5] R de Ruyter van Steveninck and W Bialek. Coding and information transfer in short spike\\nsequences. In Proc.Soc. Lond. B. Biol. Sci., volume 234, pages 379?414, 1988.\\n[6] B A Y Arcas, A L Fairhall, and W Bialek. What can a single neuron compute? In Advances in\\nNeural Information Processing Systems, volume 13, pages 75?81, 2000.\\n[7] M Carandini, D J Heeger, and J A Movshon. Linearity and normalization in simple cells of the\\nmacaque primary visual cortex. Journal of Neuroscience, 17:8621?8644, 1997.\\n[8] R M Shapley and J D Victor. The effect of contrast on the transfer properties of cat retinal\\nganglion cells. J. Physiol. (Lond), 285:275?298, 1978.\\n[9] D Chander and E J Chichilnisky. Adaptation to temporal contrast in primate and salamander\\nretina. J Neurosci, 21(24):9904?9916, 2001.\\n[10] R Shapley and C Enroth-Cugell. Visual adaptation and retinal gain control. Progress in Retinal\\nResearch, 3:263?346, 1984.\\n[11] R F Lyon. Automatic gain control in cochlear mechanics. In P Dallos et al., editor, The Mechanics and Biophysics of Hearing, pages 395?420. Springer-Verlag, 1990.\\n[12] W S Geisler and D G Albrecht. Cortical neurons: Isolation of contrast gain control. Vision\\nResearch, 8:1409?1410, 1992.\\n[13] D J Heeger. Normalization of cell responses in cat striate cortex. Vis. Neuro., 9:181?198, 1992.\\n[14] O Schwartz and E P Simoncelli. Natural signal statistics and sensory gain control. Nature\\nNeuroscience, 4(8):819?825, August 2001.\\n[15] J D Victor. The dynamics of the cat retinal X cell centre. J. Physiol., 386:219?246, 1987.\\n[16] S M Smirnakis, M J Berry, David K Warland, W Bialek, and M Meister. Adaptation of retinal\\nprocessing to image contrast and spatial scale. Nature, 386:69?73, March 1997.\\n[17] K J Kim and F Rieke. Temporal contrast adaptation in the input and output signals of salamander\\nretinal ganglion cells. J. Neurosci., 21(1):287?299, 2001.\\n[18] M Meister and M J Berry. The neural code of the retina. Neuron, 22:435?450, 1999.\\n\\n\\x0c',\n",
              "       'Competition adds complexity\\n\\nJudy Goldsmith\\nDepartment of Computer Science\\nUniversity of Kentucky\\nLexington, KY\\ngoldsmit@cs.uky.edu\\n\\nMartin Mundhenk\\nFriedrich-Schiller-Universit?at Jena\\nJena, Germany\\nmundhenk@cs.uni-jena.de\\n\\nAbstract\\nIt is known that determinining whether a DEC-POMDP, namely, a cooperative\\npartially observable stochastic game (POSG), has a cooperative strategy with positive expected reward is complete for NEXP. It was not known until now how\\ncooperation affected that complexity. We show that, for competitive POSGs, the\\ncomplexity of determining whether one team has a positive-expected-reward strategy is complete for NEXPNP .\\n\\n1 Introduction\\nFrom online auctions to Texas Hold?em, AI is captivated by multi-agent interactions based on competition. The problem of finding a winning strategy harks back to the first days of chess programs.\\nNow, we are starting to have the capacity to handle issues like stochastic games, partial information, and real-time video inputs for human player modeling. This paper looks at the complexity of\\ncomputations involving the first two factors: partially observable stochastic games (POSGs).\\nThere are many factors that could affect the complexity of different POSG models: Do the players,\\ncollectively, have sufficient information to reconstruct a state? Do they communicate or cooperate?\\nIs the game zero sum, or do the players? individual utilities depend on other players? utilities? Do\\nthe players even have models for other players? utilities?\\nThe ultimate question is, what is the complexity of finding a winning strategy for a particular player,\\nwith no assumptions about joint observations or knowledge of other players? utilities. Since a special\\ncase of this is the DEC-POMDP, where finding an optimal (joint, cooperative) policy is known to be\\nNEXP-hard [1], this problem cannot be any easier than in NEXP.\\nWe show that one variant of this problem is hard for the class NEXPNP .\\n\\n2 Definitions and Preliminaries\\n2.1 Partially observable stochastic games\\nA partially observable stochastic game (POSG) describes multi-player stochastic game with imperfect information by its states and the consequences of the players actions on the system. We follow\\nthe definition from [2] and denote it as a tuple M = (I, S, s0 , A, O,t, o, r), where\\n? I is the finite set {1, 2, . . ., k} of agents (or players), S is a finite set of states, with distinguished initial state s0 ? S, A is a finite set of actions, and O is a finite set of observations\\n? t : S ? Ak ? S ? [0, 1] is the transition probability function, where t(s, a1 , . . . , ak , s? ) is the\\nprobability that state s? is reached from state s when each agent i chooses action ai\\n? o : S ? I ? O is the observation function , where o(s, i) is the observation made in state s\\nby agent i, and\\n1\\n\\n\\x0c? r : S ? Ak ? I ? Z is the reward function, where r(s, a1 , . . . , ak , i) is the reward gained by\\nagent i in state s, when the agents take actions a1 , . . . , ak . (Z is the set of integers.)\\nA POSG where all agents have the same reward function is called a decentralized partiallyobservable Markov decision process (see [1]).\\nLet M = (I, S, s0 , A, O,t, o, r) be a POSG. A step of M is a transition from one state to another\\naccording to the transition probability function t. A run of M is a sequence of steps that starts in\\nthe initial state s0 . The outcome of each step is probabilistic and depends on the actions chosen. For\\neach agent, a policy describes how to choose actions depending on observations made during the\\nrun of the process. A (history-dependent) policy ? chooses an action dependent on all observations\\nmade by the agent during the run of the process. This is described as a function ? : O? ? A, mapping\\neach finite sequence of observations to an action.\\nA trajectory ? of length |? | = m for M is a sequence of states ? = ?1 , ?2 , . . . , ?m (m ? 1, ?i ? S)\\nwhich starts with the initial state of M , i.e. ?1 = s0 . Given policies ?1 , . . . , ?k , each trajectory ?\\nhas a probability prob(? , ?1 , . . . , ?k ). We will use some abbreviations in the sequel. For ?1 , . . . , ?k\\nj\\nwe will write ?1k , and for ?1 (o(?1 , 1) ? ? ? o(? j , 1)), . . . , ?k (o(?1 , k) ? ? ? o(? j , k)) we will write ?1k (?1 )\\naccordingly. Then prob(? , ?1 , . . . , ?k ) is defined by\\nprob(? , ?1k ) =\\n\\n|? |?1\\n\\n? t(?i , ?1k (?1i ), ?i+1 ) .\\ni=1\\n\\nWe use Tl (s) to denote all length l trajectories which start in the initial state s0 and end in state s. The\\nexpected reward Ri (s, l, ?1k ) obtained by agent i in state s after exactly l steps under policies ?1k is\\nthe reward obtained in s by the actions according to ?1k weighted by the probability that s is reached\\nafter l steps,\\nRi (s, l, ?1k ) =\\nr(s, ?1k (?1l ), i) ? prob(? , ?1k ) .\\n?\\n? ?Tl (s),? =(?1 ,...,?l )\\n\\nA POSG may behave differently under different policies. The quality of a policy is determined by\\nits performance, i.e. by the sum of expected rewards received on it. We use |M | to denote the size\\nof the representation of M .1 The short-term performance for policies ?1k for agent i with POSG M\\nis the expected sum of rewards received by agent i during the next |M | steps by following the policy\\n?1k , i.e.\\nperf i (M , ?1k ) = ? Ri (s, |M |, ?1k ) .\\ns?S\\n\\nThe performance is also called the expected reward.\\nAgents may cooperate or compete in a stochastic game. We want to know whether a stochastic game\\ncan be won by some agents. This is formally expressed in the following decision problems.\\nThe cooperative agents problem for k agents:\\ninstance:\\nquery:\\n\\na POSG M for k agents\\nare there policies ?1 , . . . , ?k under which every agent has positive performance ?\\nV\\n(I.e. ??1 , . . . , ?k : ki=1 perf i (M , ?1k ) > 0 ?)\\n\\nThe competing agents problem for 2k agents:\\ninstance:\\nquery:\\n\\na POSG M for 2k agents\\nare there policies ?1 , . . . , ?k under which all agents 1, 2, . . . , k have positive performance independent of which policies agents k + 1, k + 2, . . . , 2k choose? (I.e.\\nV\\n??1 , . . . , ?k ??k+1 , . . . , ?2k : ki=1 perf i (M , ?12k ) > 0 ?)\\n\\nIt was shown by Bernstein et al. [1] that the cooperative agents problem for two or more agents is\\ncomplete for NEXP.\\n1 The size of the representation of M is the number of bits to encode the entire model, where the function\\nt, o, and r are encoded by tables. We do not consider smaller representations. In fact, smaller representations\\nmay increase the complexity.\\n\\n2\\n\\n\\x0c2.2 NEXPNP\\nA Turing machine M has exponential running time, if there is a polynomial p such that for every\\ninput x, the machine M on input x halts after at most 2 p(|x|) steps. NEXP is the class of sets that\\ncan be decided by a nondeterministic Turing machine within exponential time. NEXPNP is the class\\nof sets that can be decided by a nondeterministic oracle Turing machine within exponential time,\\nwhen a set in NP is used as an oracle. Similar as for the class NPNP , it turns out that a NEXPNP\\ncomputation can be performed by an NEXP oracle machine that asks exactly one query to a co NP\\noracle and accepts if and only if the oracle accepts.\\n2.3 Domino tilings\\nDomino tiling problems are useful for reductions between different kinds of computations. They\\nhave been proposed by Wang [3], and we will use it according to the following definition.\\nDefinition 2.1 We use [m] to denote the set {0, 1, 2, . . . , m ? 1}. A tile type T = (V, H) consists of\\ntwo finite sets V, H ? N ? N. A T -tiling of an m-square (m ? N) is a mapping ? : [m] ? [m] ? N that\\nsatisfies both the following conditions.\\n1. Every pair of two neighboured tiles in the same row is in H.\\nI.e. for all r ? [m] and c ? [m ? 1], (? (r, c), ? (r, c + 1)) ? H.\\n2. Every pair of two neighboured tiles in the same column is in V .\\nI.e. for all r ? [m ? 1] and c ? [m], (? (r, c), ? (r + 1, c)) ? V .\\nThe exponential square tiling problem is the set of all pairs (T, 1k ), where T is a tile type and 1k is\\na string consisting of k 1s (k ? N), such that there exists a T -tiling of the 2k -square.\\nIt was shown by Savelsbergh and van Emde Boas [4] that the exponential square tiling problem\\nis complete for NEXP. We will consider the following variant, which we call the exponential ?2\\nsquare tiling problem: given a pair (T, 1k ), does there exist a row w of tiles and a T -tiling of the\\n2k -square with final row w, such that there exists no T -tiling of the 2k -square with initial row w?\\nThe proof technique of Theorem 2.29 in [4], which translates Turing machine computations into\\ntilings, is very robust in the sense that simple variants of the square tiling problem can analogously\\nbe shown to be complete for different complexity classes. Together with the above characterization\\nof NEXPNP it can be used to prove the following.\\nTheorem 2.2 The exponential ?2 square tiling problem is complete for NEXPNP .\\n\\n3 Results\\nPOSGs can be seen as a generalization of partially-observable Markov decision processes (POMDPs) in that POMDPs have only one agent and POSGs allow for many agents. Papadimitriou\\nand Tsitsiklis [5] proved that it is PSPACE-complete to decide the cooperative agents problem for\\nPOMDPs. The result of Bernstein et al. [1] shows that in case of history-dependent policies, the\\ncomplexity of POSGs is greater than the complexity of POMDPs. We show that this difference\\ndoes not appear when stationary policies are considered instead of history-dependent policies. For\\nPOMDPs, the problem appears to be NP-complete [6]. A stationary policy is a mapping O ? A\\nfrom observations to actions. Whenever the same observation is made, the same action is chosen by\\na stationary policy.\\nTheorem 3.1 For any k ? 2, the cooperative agents problem for k agents for stationary policies is\\nNP-complete.\\nProof We start with proving NP-hardness. A POSG with only one agent is a POMDP. The problem\\nof deciding, for a given POMDP M , whether there exists a stationary policy such that the short-term\\nperformance of M is greater than 0, is NP-complete [6]. Hence, the cooperative agents problem for\\nstationary policies is NP-hard.\\n3\\n\\n\\x0cIt remains to show containment in NP. Let M = (I, S, s0 , A, O,t, o, r) be a POSG. We assume that t\\nis represented in a straightforward way as a table. Let ?1 , . . . , ?k be a sequence of stationary policies\\nfor the k agents. This sequence can be straightforwardly represented using not more space than\\nthe representation of t takes. Under a fixed sequence of policies, the performance of the POSG for\\nall of the agents can be calculated in polynomial time. Using a guess and check approach (guess\\nthe stationary policies and evaluate the POSG), this shows that the cooperative agents problem for\\nstationary policies is in NP.\\n2\\nIn the same way we can characterize the complexity of a problem that we will need in the proof of\\nLemma 3.3.\\nCorollary 3.2 The following problem is coNP-complete.\\ninstance:\\nquery:\\n\\na POSG M for k agents\\ndo all agents under every stationary policy have positive performance? (I.e.\\nV\\n?stationary ?1 . . . ?k : ki=1 perf i (M , ?1k ) > 0 ?)\\n\\nThe cooperative agents problem was shown to be NEXP-complete by Bernstein et al. [1]. Not\\nsurprisingly, if the agents compete, the problem becomes harder.\\nLemma 3.3 For every k ? 1, the competing agents problem for 2k agents is in NEXPNP .\\nProof The basic idea is as follows. We guess policies ?1 , ?2 , . . . , ?k for agents 1, 2, . . . , k, and\\nconstruct a POSG that ?implements? these policies and leaves open the actions chosen by agents\\nk + 1, . . ., 2k.\\nThis new POSG has states for all short-term trajectories through the origin POSG. Therefore, its\\nsize is exponential in the size of the origin POSG. Because the history is stored in every state, and\\nthe POSG is loop-free, it turns out that the new POSG can be taken as a POMDP for which a (joint)\\npolicy with positive reward is searched. This problem is known to be NP-complete.\\nLet M = (I, S, s0 , A, O,t, o, r) be a POSG with 2k agents, and let ?1 , . . . , ?k be short-term policies for\\nM . We define a k-agent POSG M ? = (I ? , S? , s?0 , A, O? ,t ? , o? , r? ) as follows2 . In M ? , we have as agents\\nthose of M , whose policies are not fixed, i.e. I ? = {k + 1, . . . , 2k}. The set of states of M ? is the\\ncross product of states from M and all trajectories up to length |M | over S, i.e. S? = S ? S?|M |+1.\\nThe meaning of state (s, u) ? S? is, that state s can be reached on a trajectory u (that ends with s)\\nthrough M with the fixed policies. The initial state s?0 is s?0 = (s0 , s0 ). The state (s0 , ? ) is taken as\\na special sink state. After |M | + 2 steps, the sink state is entered in M ? and it is not left thereafter.\\nAll rewards gained in the sink state are 0. Now for the transition probabilities. If s is reached on\\ntrajectory u in M and the actions a1 , . . . , ak are according to the fixed policies ?1 , . . . , ?k , then the\\nprobabiliy of reaching state s? on trajectory us? according to t in M is the same as to reach (s? , us? )\\nin M ? from (s, u). In the formal description, the sink state has to be considered, too.\\nt ? ((s, u), ak , . . . , a2k , (s,\\n? u))\\n?\\n=\\n?\\n?0,\\nt(s, ?1 (o(us, 1)), ? ? ? , ?k (o(us, k)), ak+1 , . . . , a2k , s),\\n?\\n?\\n1,\\n\\nif u 6= ? and us? 6= u?\\nif u? = us,\\n? |u|\\n? ? |M |, u 6= ?\\nif |u| = |M | + 1 or u = ? , and u? = ?\\n\\nThe observation in M ? is the sequence of observations made in the trajectory that is contained in\\neach state, i.e. o? ((s, w)) = o(w), where o(? ) is any element of O. Finally, the rewards. Essentially,\\nwe are interested in the rewards obtained by the agents 1, 2, . . . , k. The rewards obtained by the other\\nagents have no impact on this, only the actions the other agents choose. Therefore, agent i obtains the\\nrewards in M ? that are obtained by agent i ? k in M . In this way, the agents k + 1, . . . , 2k obtain in\\nM ? the same rewards that are obtained by agents 1, 2, . . . , k in M , and this is what we are interested\\nin. This results in r? ((s, u), ak , . . . , a2k , i) = r(s, ?1 (o(u, 1)), ? ? ? , ?k (o(u, k)), ak+1 , . . . , a2k , i ? k) for\\ni = k + 1, . . . , 2k.\\n2 S?|M |\\n\\ndenotes the set of sequences up to |M | elements from S. The empty sequence is denoted by\\n? . For w ? S?|M | we use o(w, i) to describe the sequence of observations made by agent i on trajectory w.\\nThe concatenation of sequences u and w is denoted uw. We do not distinguish between elements of sets and\\nsequences of one element.\\n\\n4\\n\\n\\x0cNotice that the size of M ? is exponential in the size of M . The sink state in M ? is the only state that\\nlies on a loop. This means, that on all trajectories through M ? , the sink state is the only state that\\nmay appear more than once. All states other than the sink state contain the full history of how they\\nare reached. Therefore, there is a one-to-one correspondence between history-dependent policies\\nfor M and stationary policies for M ? (with regard to horizon |M |). Moreover, the corresponding\\npolicies have the same performances.\\nClaim 1 Let ?1 , . . . , ?2k be short-term policies for M , and let ??k+1 , . . . , ??2k be their corresponding\\nstationary policies for M ? .\\n2k ).\\nFor |M | steps and i = 1, 2, . . . , k, perf i (M , ?12k ) = perf i+k (M ? , ??k+1\\n\\nThus, this yields an NEXPNP algorithm to decide the competitive agents problem. The input is a\\nPOSG M for 2k agents. In the first step, the policies for the agents 1, 2, . . . , k are guessed. This takes\\nnondeterministic exponential time. In the second step, the POSG M ? is constructed from the input\\nM and the guessed policies. This takes exponential time (in the length of the input M ). Finally, the\\noracle is queried whether M ? has positive performance for all agents under all stationary policies.\\nThis problem belongs to coNP (Corollary 3.2). Henceforth, the algorithm shows the competing\\nagents problem to be in NEXPNP .\\n2\\nLemma 3.4 For every k ? 2, the competing agents problem for 2k agents is hard for NEXPNP .\\nProof We give a reduction from the exponential ?2 square tiling problem to the competing agents\\nproblem.\\nLet T = (T, 1k ) be an instance of the exponential ?2 square tiling problem, where T = (V, H) is a\\ntile type. We will show how to construct a POSG M with 4 agents from it, such that T is a positive\\ninstance of the exponential ?2 square tiling problem if and only if (1) agents 1 and 2 have a tiling\\nfor the 2k square with final row w such that (2) agents 3 and 4 have no tiling for the 2k square with\\ninitial row w.\\nThe basic idea for checking of tilings with POSGs for two agents stems from Bernstein et al. [1],\\nbut we give a slight simplification of their proof technique, and in fact have to extend it for four\\nagents later on. The POSG is constructed so that on every trajectory each agent sees a position in\\nthe square. This position is chosen by the process. The only action of the agent that has impact\\non the process is putting a tile on the given position. In fact, the same position is observed by the\\nagents in different states of the POSG. From a global point of view, the process splits into two parts.\\nThe first part checks whether both agents know the same tiling, without checking that it is a correct\\ntiling. In the state where the agents are asked to put their tiles on the given position, a high negative\\nreward is obtained if the agents put different tiles on that position. ?High negative? means that,\\nif there is at least one trajectory on which such a reward is obtained, then the performance of the\\nwhole process will be negative. The second part checks whether the tiling is correct. The idea is to\\ngive both the agents neighboured positions in the square and to ask each which tile she puts on that\\nposition. Notice that the agents do not know in which part of the process they are. This means, that\\nthey do not know whether the other agent is asked for the same position, or for its upper or right\\nneighbour. This is why the agents cannot cheat the process. A high negative reward will be obtained\\nif the agents? tiles do not fit together.\\nFor the first part, we need to construct is a POSG Pk for two agents, that allows both agents to\\nmake the same sequence of observations consisting of 2k bits. This sequence is randomly chosen,\\nand encodes a position in a 2k ? 2k grid. At the end, state same is reached, at which no observation is\\nmade. At this state, it will be checked whether both agents put the same tile at this position (see later\\non). The task of Pk is to provide both agents with the same position. Figure 1 shows an example\\nfor a 24 ? 24 -square. The initial state is s4 . Dashed arrows indicate transitions with probability 21\\nindependent of the actions. The observation of agent 1 is written on the left hand side of the states,\\nand the observations of agent 2 at the right hand side. In s4 , the agents make no observation. In Pk\\nboth agents always make the same observations.\\nThe second part is more involved. The goal is to provide both agents with neighboured positions\\nin the square. Eventually, it is checked whether the tiles they put on the neighboured positions\\nare according to the tile type T . Because the positions are encoded in binary, we can make use\\n5\\n\\n\\x0cs4\\n\\ns\\n\\ns\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\nrow\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\ncolumn\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\nsame\\n\\nsame\\n\\nhori\\n\\nFigure 1: P4\\n\\nFigure 2: C3,4\\n\\ncheck tiles\\n\\nFigure 3: L3,4\\n\\nof the following fact of subsequent binary numbers. Let u = u1 . . . uk and w = w1 . . . wk be bitwise\\nrepresentation of strings. if nw = nu + 1, then for some index l it holds that (1) ui = wi for i =\\n1, 2, . . . , l ? 1, (2) wl = 1 and ul = 0, and (3) w j = 0 and u j = 1 for j = l + 1, . . ., k.\\nThe POSG Cl,k is intended to provide the agents with two neighboured positions in the same row,\\nwhere the index of the leftmost bit of the column encoding where both positions distinguish is l.\\n(The C stands for column.) Figure 2 shows an example for the 24 -square. The ?final state? of Cl,k\\nis the state hori, from which it is checked whether the agents put horizontically fitting tiles together.\\nIn the same way, a POSG Rl,k can be constructed (R stands for row), whose task is, to check\\nwhether two tiles in neighboured rows correspond to a correct tiling. This POSG has the final state\\nvert, from which on it is checked whether two tiles fit vertically.\\nFinally, we have to construct the last part of the POSG. It consists of the states same, hori, vert (as\\nmentioned above), good, bad, and sink. All transitions between these states are deterministic (i.e.\\nwith probability 1). From state same the state good is reached, if both agents take the same action\\n? otherwise bad is reached. From state hori the state good is reached, if action a1 by agent 1 and a2\\nby agent 2 make a pair (a1 , a2 ) in H, i.e. in the set of horizontically correct pairs of tiles ? otherwise\\nbad is reached. Similarly, from state vert the state good is reached, if action a1 by agent 1 and a2 by\\nagent 2 make a pair (a1 , a2 ) in V . All these transitions are with reward 0. From state good the state\\nsink is reached on every action with reward 1, and from state bad the state sink is reached on every\\naction with reward ?(22k+2 ). When the state sink is reached, the process stays there on any action,\\nand all agents obtain reward 0. All rewards are the same for both agents. (This part can be seen in\\nthe overall picture in Figure 4).\\nFrom these POSGs we construct a POSG T2,k that checks whether two agents know the same correct\\ntiling for a 2k ? 2k square, as described above. There are 2k + 1 parts of T2,k . The initial state of each\\npart can be reached with one step from the initial state s0 of T2,k . The parts of T2,k are as follows.\\n? P2k with initial state s (checks whether two agents have the same tiling)\\n? For each l = 1, 2, . . . , k, we take Cl,k . Let cl be the initial state of Cl,k .\\n6\\n\\n\\x0cs0\\n\\nsk\\n\\nc1\\n\\nck\\n\\nr1\\n\\nrk\\n\\nPk\\n\\nC1,k\\n\\nCk,k\\n\\nR1,k\\n\\nRk,k\\n\\nhori\\n\\nvert\\n\\nsame\\n\\ngood\\n\\nbad\\n\\nsink\\n\\nFigure 4: T2,k\\n? For each l = 1, 2, . . . , k, we take Rl,k . Let rl be the initial state of Rl,k .\\nThere are 22k + 2 ? ?kl=1 2k ? 2l?1 =: tr(k) trajectories with probability > 0 through T2,k . Notice\\nthat tr(k) < 22k+2 . From the initial state s0 of T2,k , each of the initial states of the parts is reachable\\nindependent on the action chosen by the agents. We will give transition probabilities to the transition\\nfrom s0 to each of the initial states of the parts in a way, that eventually each trajectory has the same\\nprobability.\\n\\n?\\n\\nt(s0 , a1 , a2 , s ) =\\n\\n(\\n\\n22k\\ntr(k) ,\\n2k+l?1\\ntr(k)\\n\\nif s? = s, i.e. the initial state of Pk\\nif s ? {rl , cl | l = 1, 2, . . . , k}\\n\\nIn the initial state s0 and in the initial states of all parts, the observation ? is made. When a state\\nsame, hori, vert is reached, each agent has made 2k + 3 observations, where the first and last are ?\\nand the remaining 2k are each in {0, 1}. Such a state is the only one where the actions of the agents\\nhave impact on the process. Because of the partial observability, they cannot know in which part\\nof T2,k they are. The agents can win, if they both know the same correct tiling and interpret the\\nsequence of observations as the position in the grid they are asked to put a tile on. On the other\\nhand, if both agents know different tilings or the tiling they share is not correct, then at least one\\ntrajectory will end in a bad state and has reward ?(22k+2 ). The structure of the POSG is given in\\nFigure 4.\\nClaim 2 Let (T, 1k ) be an instance of the exponential square tiling problem.\\n(1) There exists a polynomial time algorithm, that on input (T, 1k ) outputs T2,k .\\n(2) There exists a T -tiling of the 2k square if and only if there exist policies for the agents under\\nwhich T2,k has performance > 0.\\nPart (1) is straightforward. Part (2) is not much harder. If there exists a T -tiling of the 2k square,\\nboth agents use the same policy according to this tiling. Under these policies, state bad will not be\\nreached. This guarantess performance > 0 for both agents. For the other direction: if there exist\\npolicies for the agents under which T2,k has performance > 0, then state bad is not reached. Hence,\\nboth agents use the same policy. It can be shown inductively that this policy ?is? a T -tiling of the 2k\\nsquare.\\n7\\n\\n\\x0cThe POSG for the competing agents problem with 4 agents consists of three parts. The first part is\\na copy of T2,k . It is used to check whether the first square can be tiled correctly (by agents 1 and\\n2). In this part, the negative rewards are increased in a way that guarantees the performance of the\\nPOSG to be negative whenever agents 1 and 2 do not correctly tile their square. The second part\\nis a modified copy of T2,k . It is used to check whether the second square can be tiled correctly (by\\nagents 3 and 4). Whenever state bad is left in this copy, reward 0 is obtained, and whenever state\\ngood is left, reward ?1 is obtained. The third part checks whether agent 1 puts the same tiles into\\nthe last row of its square as agent 3 puts into the first row of its square. (See L3,4 in Figure 3 as an\\nexample.) If this succeeds, the performance of the third part equals 0, otherwise it has performance\\n1. These three parts run in parallel.\\nIf agents 1 and 2 have a tiling for the first square, the performance of the first part equals 1.\\n? If agents 3 and 4 are able to continue this tiling through their square, the performance\\nof the second part equals ?1 and the performance of the third part equals 0. At all, the\\nperformance of the POSG under these policies equals 0.\\n? If agents 3 and 4 are not able to continue this tiling through their square, then the performance of part 2 and part 3 is strictly greater ?1. At all, the performance of the POSG under\\nthese policies is > 0.\\n2\\nLemmas 3.3 and 3.4 together yield completeness of the competing agents problem.\\nTheorem 3.5 For every k ? 2, the competing agents problem for 2k agents is complete for NEXPNP .\\n\\n4 Conclusion\\nWe have shown that competition makes life?and computation?more complex. However, in order\\nto do so, we needed teamwork. It is not yet clear what the complexity is of determining the existence\\nof a good strategy for Player I in a 2-person POSG, or a 1-against-many POSG.\\nThere are other variations that can be shown to be complete for NEXPNP , a complexity class that,\\nshockingly, has not been well explored. We look forward to further results about the complexity of\\nPOSGs, and to additional NEXPNP -completeness results for familiar AI and ML problems.\\n\\nReferences\\n[1] Daniel S. Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. The complexity\\nof decentralized control of Markov decision processes. Math. Oper. Res., 27(4):819?840, 2002.\\n[2] E. Hansen, D. Bernstein, and S. Zilberstein. Dynamic programming for partially observable\\nstochastic games. In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI-04), pages 709?715, 2004.\\n[3] Hao Wang. Proving theorems by pattern recognition II. Bell Systems Technical Journal, 40:1?\\n42, 1961.\\n[4] M. Savelsbergh and P. van Emde Boas. Bounded tiling, an alternative to satisfiability. In Gerd\\nWechsung, editor, 2nd Frege Conference, volume 20 of Mathematische Forschung, pages 354?\\n363. Akademie Verlag, Berlin, 1984.\\n[5] C.H. Papadimitriou and J.N. Tsitsiklis. The complexity of Markov decision processes. Mathematics of Operations Research, 12(3):441?450, 1987.\\n[6] Martin Mundhenk, Judy Goldsmith, Christopher Lusena, and Eric Allender. Complexity results\\nfor finite-horizon Markov decision process problems. Journal of the ACM, 47(4):681?720, 2000.\\n\\n8\\n\\n\\x0c',\n",
              "       ...,\n",
              "       'On clustering network-valued data\\n\\nSoumendu Sundar Mukherjee\\nDepartment of Statistics\\nUniversity of California, Berkeley\\nBerkeley, California 94720, USA\\nsoumendu@berkeley.edu\\n\\nPurnamrita Sarkar\\nDepartment of Statistics and Data Sciences\\nUniversity of Texas, Austin\\nAustin, Texas 78712, USA\\npurna.sarkar@austin.utexas.edu\\n\\nLizhen Lin\\nDepartment of Applied and Computational Mathematics and Statistics\\nUniveristy of Notre Dame\\nNotre Dame, Indiana 46556, USA\\nlizhen.lin@nd.edu\\n\\nAbstract\\nCommunity detection, which focuses on clustering nodes or detecting communities in (mostly) a single network, is a problem of considerable practical\\ninterest and has received a great deal of attention in the research community. While being able to cluster within a network is important, there\\nare emerging needs to be able to cluster multiple networks. This is largely\\nmotivated by the routine collection of network data that are generated from\\npotentially different populations. These networks may or may not have node\\ncorrespondence. When node correspondence is present, we cluster networks\\nby summarizing a network by its graphon estimate, whereas when node\\ncorrespondence is not present, we propose a novel solution for clustering\\nsuch networks by associating a computationally feasible feature vector to\\neach network based on trace of powers of the adjacency matrix. We illustrate our methods using both simulated and real data sets, and theoretical\\njustifications are provided in terms of consistency.\\n\\n1\\n\\nIntroduction\\n\\nA network, which is used to model interactions or communications among a set of agents\\nor nodes, is arguably among one of the most common and important representations for\\nmodern complex data. Networks are ubiquitous in many scientific fields, ranging from\\ncomputer networks, brain networks and biological networks, to social networks, co-authorship\\nnetworks and many more. Over the past few decades, great advancement has been made\\nin developing models and methodologies for inference of networks. There are a range of\\nrigorous models for networks, starting from the relatively simple Erd?s-R?nyi model [12],\\nstochastic blockmodels and their extensions [15, 17, 6], to infinite dimensional graphons\\n[28, 13]. These models are often used for community detection, i.e. clustering the nodes in a\\nnetwork. Various community detection algorithms or methods have been proposed, including\\nmodularity-based methods [21], spectral methods [25], likelihood-based methods [8, 11, 7, 4],\\nand optimization-based approaches like those based on semidefinite programming [5], etc.\\nThe majority of the work in the community detection literature including the above mentioned\\nfocus on finding communities among the nodes in a single network. While this is still a very\\nimportant problem with many open questions, there is an emerging need to be able to detect\\nclusters among multiple network-valued objects, where a network itself is a fundamental unit\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\\x0cof data. This is largely motivated by the routine collection of populations or subpopulations\\nof network-valued data objects. Technological advancement and the explosion of complex\\ndata in many domains has made this a somewhat common practice.\\nThere has been some notable work on graph kernels in the Computer Science literature [27, 26].\\nIn these works the goal is to efficiently compute different types of kernel similarity matrices\\nor their approximations between networks. In contrast, we ask the following statistical\\nquestions. Can we cluster networks consistently from a mixture of graphons, when 1) there\\nis node correspondence and 2) when there isn?t. The first situation arises when one has a\\ndynamic network over time, or multiple instantiations of a network over time. If one thinks\\nof them as random samples from a mixture of graphons, then can we cluster them? Note that\\nthis is not answered by methods which featurize graphs using different statistics. Our work\\nproposes a simple and general framework for the first question - viewing the data as coming\\nfrom a mixture model on graphons. This is achieved by first obtaining a graphon estimate\\nof each of the networks, constructing a distance matrix based on the graphon estimates, and\\nthen performing spectral clustering on the resulting distance matrix. We call this algorithm\\nNetwork Clustering based on Graphon Estimates (NCGE).\\nThe second situation arises when one is interested in global properties of a network. This\\nsetting is closer to that of graph kernels. Say we have co-authorship networks from Computer\\nScience and High Energy Physics. Are these different types of networks? There has been a\\nlot of empirical and algorithmic work on featurizing networks or computing kernels between\\nnetworks. But most of these features require expensive computation. We propose a simple\\nfeature based on traces of powers of the adjacency matrix for this purpose which is very\\ncheap to compute as it involves only matrix multiplication. We cluster these features and\\ncall this method Network Clustering based on Log Moments (NCLM).\\nWe provide some theoretical guarantees for our algorithms in terms of consistency, in\\naddition to extensive simulations and real data examples. The simulation results show that\\nour algorithms clearly outperform the naive yet popular method of clustering (vectorized)\\nadjacency matrices in various settings. We also show that in absence of node correspondence,\\nour algorithm is consistently better and faster than methods which featurize networks with\\ndifferent global statistics and graphlet kernels. We also show our performance on a variety\\nof real world networks, like separating out co-authorship networks form different domains\\nand ego networks.\\nThe rest of the paper is organized as follows. In Section 2 we briefly describe graphonestimation methods and other related work. Next, in Section 3 we formally describe our\\nsetup and introduce our algorithms. Section 4.1 contains some theory for these algorithms.\\nIn Section 5 we provide simulations and real data examples. We conclude with a discussion\\nin Section 6.\\n\\n2\\n\\nRelated work\\n\\nThe focus of this paper is on 1) clustering networks which have node correspondence based on\\nestimating the underlying graphon and 2) clustering networks without node correspondence\\nbased on global properties of the networks. We present the related work in two parts: first\\nwe cite two such methods of obtaining graphon estimates, which we will use in our first\\nalgorithm. Second, we present existing results that summarize a network using different\\nstatistics and compare those to obtain a measure of similarity.\\nA prominent estimator of graphons is the so called Universal Singular Value Thresholding\\n(USVT) estimator proposed by [9]. The main idea behind USVT is to essentially approximate\\nthe rank of the population matrix by thresholding the singular values of the observed matrix\\nat an universal threshold, and then compute an approximation of the population using\\nthe top singular values and vectors. A recent work [29] proposes a novel, statistically\\nconsistent and computationally efficient approach for estimating the link probability matrix\\nby neighborhood smoothing. Typically for large networks USVT is a lot more scalable\\nthan the neighborhood-smoothing approach. There are several other methods for graphon\\nestimations, e.g., by fitting a stochastic blockmodel [24]. These methods can also be used in\\nour algorithm.\\n2\\n\\n\\x0cIn [10], a graph-based method for change-point detection is proposed, where an independent\\nsequence of observations are considered. These are generated i.i.d. under the null hypothesis,\\nwhereas under the alternative, after a change point, the underlying distribution changes. The\\ngoal is to find this change point. The observations can be high-dimensional vectors or even\\nnetworks, with the latter bearing some resemblance with our first framework. Essentially\\nthe authors of [10] present a statistically consistent clustering of the observations into ?past?\\nand ?future?. We remark here that our graphon-based clustering algorithm suggests an\\nalternative method for change point detection, namely by looking at the second eigenvector\\nof the distance matrix between estimated graphons. Another related work is due to [14]\\nwhich aims to extend the classical large sample theory to model network-valued objects.\\nFor comparing global properties of networks, there has been many interesting works which\\nfeaturize networks based on global features [3]. In the Computer Science literature, graph\\nkernels have gained much attention [27, 26]. In these works the goal is to efficiently compute\\ndifferent types of kernel similarity matrices or their approximations between networks.\\n\\n3\\n\\nA framework for clustering networks\\n\\nLet G be a binary random network or graph with n nodes. Denote by A its adjacency\\nmatrix, which is an n by n symmetric matrix with binary entries. That is, Aij = Aji ?\\n{0, 1}, 1 ? i < j ? n, where Aij = 1 if there is an observed edge between nodes i and j, and\\nAij = 0 otherwise. All the diagonal elements of A are structured to be zero (i.e. Aii = 0).\\nWe assume the following random Bernoulli model with:\\nAij | Pij ? Bernoulli(Pij ), i < j,\\n\\n(1)\\n\\nwhere Pij = P (Aij = 1) is the link probability between nodes i and j. We denote the\\nlink probability matrix as P = ((Pij )). The edge probabilities are often modeled using the\\nso-called graphons. A graphon f is a nonnegative bounded, measurable symmetric function\\nf : [0, 1]2 ? [0, 1]. Given such an f , one can use the model\\nPij = f (?i , ?j ),\\n\\n(2)\\n\\nwhere ?i , ?j are i.i.d. uniform random variables on (0, 1). In fact, any (infinite) exchangeable\\nnetwork arises in this way (by Aldous-Hoover representation [2, 16]).\\nIntuitively speaking, one wishes to model a discrete network G using some continuous object\\nf . Our current work focuses on the problem of clustering networks. Unlike in a traditional\\nsetup, where one observes a single network (with potentially growing number of nodes) and\\nthe goal is often to cluster the nodes, here we observe multiple networks and are interested\\nin clustering these networks viewed as fundamental data units.\\n3.1\\n\\nNode correspondence present\\n\\nA simple and natural model for this is what we call the graphon mixture model for obvious\\nreasons: there are only K (fixed) underlying graphons f1 , . . . , fK giving rise to link probability\\nmatrices ?1 , . . . , ?K and we observe T networks sampled i.i.d. from the mixture model\\nPmix (A) =\\n\\nK\\nX\\n\\nqi P?i (A),\\n\\n(3)\\n\\ni=1\\n\\nQ\\nAuv\\nwhere the qi ?s are the mixing proportions and PP (A) = u<v Puv\\n(1 ? Puv )1?Auv is the\\nprobability of observing the adjacency matrix A when the link probability matrix is given\\nby P . Consider n nodes, and T independent networks Ai , i ? [T ], which define edges\\nbetween these n nodes. We propose the algorithm the following simple and general algorithm\\n(Algorithm 3.1) for clustering them:\\n3\\n\\n\\x0cAlgorithm 1 Network Clustering based on Graphon Estimates (NCGE)\\n1: Graphon estimation. Given A1 , . . . , AT , estimate their corresponding link probability\\nmatrices P1 , . . . , PT using any one of the ?blackbox? algorithms such as USVT ([9]), the\\nneighborhood smoothing approach by [29] etc. Call these estimates P?1 , . . . , P?T .\\n? with D\\n? ij =\\n2: Forming a distance matrix. Compute the T by T distance matrix D\\n?\\n?\\n?\\nkPi ? Pj kF , where k?kF is the Frobenius norm. D is considered an estimate of D = ((Dij ))\\nwhere Dij = kPi ? Pj kF .\\n?\\n3: Clustering. Apply the spectral clustering algorithm to the distance matrix D.\\n\\nWe will from now on denote the above algorithm with the different graphon estimation\\n(?blackbox?) approaches as follows: the algorithm with USVT a blackbox will be denoted\\nby CL-USVT and the one with the neighborhood smoothing method will be denoted by\\nCL-NBS. We will compare these two algorithms with the CL-NAIVE method which does\\nnot estimate the underlying graphon, but uses the vectorized binary string representation of\\nthe adjacency matrices, and clusters those (in the spirit of [10]).\\n3.2\\n\\nNode correspondence absent\\n\\nWe will use certain graph statistics to construct a feature vector. The basic statistics we\\nchoose are the trace of powers of the adjacency matrix, suitably normalized and we call them\\ngraph moments:\\nmk (A) = trace(A/n)k .\\n\\n(4)\\n\\nThese statistics are related to various path/subgraph counts. For example, m2 (A) is the\\nnormalized count of the total number of edges, m3 (A) is the normalized triangle count of A.\\nHigher order moments are actually counts of closed walks (or directed circuits).\\nThe reason we use graph moments instead of subgraph counts is that the latter are quite\\ndifficult to compute and present day algorithms work only for subgraphs up to size 5. On\\nthe contrary, graph moments are easy to compute as they only involve matrix multiplication.\\nWhile it may seem that this is essentially the same as comparing the eigenspectrum, it is\\nnot clear how many eigenvalues one should use. Even if one could estimate the number of\\nlarge eigenvalues using an USVT type estimator, the length is different for different networks.\\nThe trace takes into account the relative magnitudes of ?i naturally. In fact, we tried (see\\nSection 5) using the top few eigenvalues as the sole features; but the results were not as\\nsatisfactory as using mk .\\nWe now present our second algorithm (Algorithm 2) Network Clustering with Log Moments (NCLM). In step 1, for some positive integer J ? 2, we compute gJ (A) :=\\n(log m2 (A), . . . , log mJ (A)) ? RJ . Our feature map here is g(A) = gJ (A). For step 2,\\n? ij = kgi ? gj k.\\nwe use the Euclidean norm, i.e. D\\nAlgorithm 2 Network Clustering based on Log Moments (NCLM)\\n1: Moment calculation. For a network Ai , i ? [T ] and a positive integer J, compute the\\nfeature vector gJ (A) := (log m1 (A), log m2 (A), . . . , log mJ (A)) (see Eq 4).\\n2: Forming a distance matrix. d(A1 , A2 ) := d(gJ (A1 ), gJ (A2 )).\\n?\\n3: Clustering. Apply the spectral clustering algorithm to the distance matrix D.\\n\\nNote: The rationale behind taking a logarithm of the graph moments is that, if we have\\ntwo graphs with the same degree density but different sizes, then the degree density will\\nnot play any role in the the distance (which is necessary because the degree density will\\nsubdue any other difference otherwise). The parameter J counts, in some sense, the effective\\nnumber of eigenvalues we are using.\\n4\\n\\n\\x0c4\\n\\nTheory\\n\\nWe will only mention our main results and discuss some of the consequences here. All the\\nproofs and further details can be found in the supplementary article [1].\\n4.1\\n\\nResults on NCGE\\n\\n? ij as estimating Dij = kPi ? Pj kF .\\nWe can think of D\\nTheorem 4.1. Suppose D = ((Dij )) has rank K. Let V (resp. V? ) be the T ? K matrix\\nwhose columns correspond to the leading K eigenvectors (corresponding to the K largest-in? Let ? = ?(K, n, T ) be the K-th smallest eigenvalue\\nmagnitude eigenvalues) of D (resp. D).\\n? such that\\nvalue of D in magnitude. Then there exists an orthogonal matrix O\\nX\\n? ? V k2F ? 64T\\nkV? O\\nkP?i ? Pi k2F .\\n2\\n?\\ni\\nCorollary 4.2. Assume for some absolute constants ?, ? > 0 the following holds for each\\ni = 1, . . . , T :\\nkP?i ? Pi k2F\\n? Ci n?? (log n)? ,\\n(5)\\nn2\\neither in expectation\\nP or with high probability (? 1 ? \\x0fi,n ). Then in expectation or with high\\nprobability (? 1 ? i \\x0fi,n ) we have that\\n? ? V k2F ?\\nkV? O\\n\\n64CT T 2 n2?? (log n)?\\n.\\n?2\\n\\n(6)\\n\\nwhere CT = maxi?i?T Ci . (If there are K (fixed, not growing with T ) underlying graphons,\\nthe constant CT does not depend on T .) Table 1 reports values of ?, ? for various graphon\\nestimation procedures (under assumptions on the underlying graphons, that are described in\\nthe supplementary article [1]).\\nTable 1: Values of ?, ? for various graphon estimation procedures.\\nProcedure\\n?\\n?\\n\\nUSVT\\n1/3\\n0\\n\\nNBS\\n1/2\\n1/2\\n\\nMinimax rate\\n1\\n1\\n\\nWhile it is hard to obtain an explicit bound on ? in general, let us consider a simple equal\\nweight mixture of two graphons to illustrate the relationship between ? and separation\\nbetween graphons. Let the distance between the population graphons be dn We have\\nD = ZDZ T , where the 2 ? 2 population matrix be D has D(i, j) = D(j, i) = dn. Here\\nZDZ T = dn(ET ? ZZ T ), where ET is the T ? T matrix of all ones, and the ith row of\\nthe binary matrix Z has a single one at position l if network Ai is sampled from ?l . The\\neigenvalues of this matrix are ?T nd/2 and ?T nd/2. Thus in this case ? = T nd/2. As a\\nresult (6) becomes\\n??\\n?\\n? ? V k2F ? 256CT n (log n) .\\nkV? O\\n(7)\\n2\\nd\\nLet us look at a more specific case of blockmodels with the same number (= m) of clusters\\nof equal sizes (= n/m) to gain some insight into d. Let C be a n ? m binary matrix\\nof memberships such that Cib = 1 if node i within a blockmodel comes from cluster b.\\nConsider two blockmodels ?1 = CB1 C T with B1 = (p ? q)Im + qEm and ?2 = CB2 C T\\nwith B2 = (p0 ? q 0 )Im + q 0 Em , where Im is the identity matrix of order k (here the only\\ndifference between the models come from link formation probabilities within/between blocks,\\nthe blocks remaining the same). In this case\\n\\x12\\n\\x13\\nk?1 ? ?2 k2F\\n1\\n1\\n0 2\\nd2 =\\n=\\n(p\\n?\\np\\n)\\n+\\n1\\n?\\n(q ? q 0 )2 .\\nn2\\nm\\nm\\n5\\n\\n\\x0cThe bound (6) can be turned into a bound on the proportion of ?misclustered? networks,\\ndefined appropriately. There are several ways to define misclustered nodes in the context\\nof community detection in stochastic blockmodels that are easy to analyze with spectral\\nclustering (see, e.g., [25, 18]). These definitions work in our context too. For example, if we\\nuse Definition 4 of [25] and denote by M the set of misclustered networks, then from the\\nproof of their Theorem 1, we have\\n? ? V k2F ,\\n|M| ? 8mT kV? O\\nwhere mT = maxj=1,...,K (Z T Z)jj is the maximum number of networks coming from any of\\nthe graphons.\\n4.2\\n\\nResults on NCLM\\n\\nWe first establish concentration of trace(Ak ). The proof uses Talagrand?s concentration\\ninequality, which requires additional results on Lipschitz continuity and convexity. This\\nis obtained via decomposing A 7? trace(Ak ) into a linear combination of convex-Lipschitz\\nfunctions.\\nTheorem 4.3 (Concentration of moments). Let A be the adjacency matrix of an inhomogen\\nneous random graph with link-probability matrix P . Then for any k. Let ?k (A) := k?\\nm (A).\\n2 k\\nThen\\n?\\nP(|?k (A) ? E?k (A)| > t) ? 4 exp(?(t ? 4 2)2 /16).\\nAs a consequence of this, we can show that gJ (A) concentrates around g?J (A) :=\\n(log Em2 (A), . . . , log EmJ (A)).\\nTheorem 4.4 (Concentration of gJ (A)). Let EA = ?S, where ? ? (0, 1), mini,j Sij = ?(1),\\nP\\nand i,j Sij = n2 . Then k?\\ngJ (A)k = ?(J 3/2 log(1/?)) and for any 0 < ? < 1 satisfying\\n?J log(1/?) = ?(1), we have\\nP(kgJ (A) ? g?J (A)k ? ?J 3/2 log(1/?)) ? JC1 e?C2 n\\n\\n2 2J\\n\\n?\\n\\n.\\n\\nWe expect that g?J will be a good population level summary for many models. In general,\\nit is hard to show an explicit separation result for g?J . However, in simple models, we can\\ndo explicit computations to show separation. For example, in a two parameter blockmodel\\nB = (p?q)Im +qEm , with equal block sizes, we have Em2 (A) = (p/m+(m?1)q/m)(1+o(1)),\\nEm3 (A) = (p3 /m2 + (m ? 1)pq 2 /m2 + (m ? 1)(m ? 2)q 3 /6m2 )(1 + o(1)) and so on. Thus\\nwe see that if m = 2, then g?2 should be able to distinguish between such blockmodels (i.e.\\ndifferent p, q).\\nNote: After this paper was submitted, we came to know of a concurrent work [20] that\\nprovides a topological/combinatorial perspective on the expected graph moments Emk (A).\\nTheorem 1 in [20] shows that under some mild assumptions on the model (satisfied, for\\nexample, by generalized random graphs with bounded kernels as long as the average degree\\ngrows to infinity), Etrace(Ak ) = E(# of closed k-walks) will be asymptotic to E(# of\\nclosed k-walks that trace out a k-cycle) plus 1{k even} E(# of closed k-walks that trace out a\\n(k/2+1)-tree). For even k, if the degree grows fast enough k-cycles tend to dominate, whereas\\nfor sparser graphs trees tend to dominate. From this and our concentration results, we can\\nexpect NCLM to be able to tell apart graphs which are different in terms the counts of these\\nsimpler closed k-walks. Incidentally, the authors of [20] also show that the expected count of\\nclosed non-backtracking walks of length k is dominated by walks tracing out k-cycles. Thus\\nif one uses counts of closed non-backtracking k-walks (i.e. moments of the non-backtracking\\nmatrix) instead of just closed k-walks as features, one would expect similar performance on\\ndenser networks, but in sparser settings it may lead to improvements because of the absence\\nof the non-informative trees in lower order even moments.\\n\\n5\\n\\nSimulation study and data analysis\\n\\nIn this section, we describe the results of our experiments with simulated and real data\\nto evaluate the performance of NCGE and NCLM. We measure performance in terms of\\n6\\n\\n\\x0cclustering error which is the minimum normalized hamming distance between the estimated\\nlabel vector and all K! permutations of the true label assignment. Clustering accuracy is\\none minus clustering error.\\nNode correspondence present: We provide two simulated data experiments1 for clustering networks with node correspondence. In each experiment twenty 150-node networks\\nwere generated from a mixture of two graphons, 13 networks from the first and the other\\n7 from the second. We also used a scalar multiplier with the graphons to ensure that the\\nnetworks are not too dense. The average degree for all these experiments were around 20-25.\\nWe report the average error bars from a few random runs.\\nFirst we generate a mixture of graphons from two blockmodels, with probability matrices\\n(pi ? qi )Im + qi Em with i ? {1, 2}. We use p2 = p1 (1 + \\x0f) and q2 = q1 (1 + \\x0f) and measure\\nclustering accuracy as the multiplicative error \\x0f is increased from 0.05 to 0.15. We compare\\nCL-USVT, CL-NBS and CL-NAIVE and the results are summarized in Figure 1(A). We\\nhave observed two things. First, CL-USVT and CL-NBS start distinguishing the graphons\\nbetter as \\x0f increases (as the theory suggests). Second, the naive approach does not do a\\ngood job even when \\x0f increases.\\nFigure 1: We show the behavior of the three algorithms when \\x0f increases, when the underlying\\nnetwork is generated from (A) a blockmodel and (B) a smooth graphon.\\n\\n(A)\\n\\n(B)\\n\\nIn the second simulation, we generate the networks from two smooth graphons ?1 and\\n?2 , where ?2 = ?1 (1 + \\x0f) (here ?1 corresponds to the graphon 3 appearing in Table 1 of\\n[29]). As is seen from Figure 1(B), here also CL-USVT and CL-NBS outperform the naive\\nalgorithm by a huge margin. Also, CL-NBS is consistently better than CL-USVT. This may\\nhave happened because we did our experiments on somewhat sparse networks, where USVT\\nis known to struggle.\\nNode correspondence absent: We show the efficacy of our approach via two sets of\\nexperiments. We compare our log-moment based method NCLM with three other methods.\\nThe first is Graphlet Kernels [26] with 3, 4 and 5 graphlets, denoted by GK3, GK4 and\\nGK5 respectively. In the second method, we use six different network-based statistics to\\nsummarize each graph; these statistics are the algebraic connectivity, the local and global\\nclustering coefficients [23], the distance distribution [19] for 3 hops, the Pearson correlation\\ncoefficient [22] and the rich-club metric [30]. We also compare against graphs summarized by\\nthe top J eigenvalues of A/n (TopEig). These are detailed in the supplementary article [1].\\n? we compute with NCLM, GraphStats and TopEig, we calculate a\\nFor each distance matrix D\\n? where t is learned by picking the value within a range which\\nsimilarity matrix K = exp(?tD)\\nmaximizes the relative eigengap (?K (K) ? ?K+1 (K))/?K+1 (K). It would be interesting to\\nhave a data dependent range for t. We are currently working on cross-validating the range\\nusing the link prediction accuracy on held out edges.\\n1\\n\\nCode used in this paper is publicly available at https://github.com/soumendu041/\\nclustering-network-valued-data.\\n\\n7\\n\\n\\x0cFor each matrix K we calculate the top T eigenvectors, and do K-means on them to get the\\nfinal clustering. We use T = K; however, as we will see later in this subsection, for GK3,\\nGK4, and GK5 we had to use a smaller T which boosted their clustering accuracy.\\nFirst we construct four sets of parameters for the two parameter blockmodel (also known as\\nthe planted partition model): ?1 = {p = 0.1, q = 0.05, K = 2, ? = 0.6}, ?2 = {p = 0.1, q =\\n0.05, K = 2, ? = 1}, ?3 = {p = 0.1, q = 0.05, K = 8, ? = 0.6}, and ?4 = {p = 0.2, q =\\n0.1, K = 8, ? = 0.6}. Note that the first two settings differ only in the density parameter ?.\\nThe second two settings differ in the within and across cluster probabilities. The first two\\nand second two differ in K. For each parameter setting, we generate two sets of 20 graphs,\\none with n = 500 and the other with n = 1000.\\nFor choosing J, we calculate the moments for a large J; compute a kernel similarity matrix\\nfor each choice of J and report the one with largest relative eigengap between the K th and\\n(K + 1)th eigenvalue. We show these plots in the supplementary article [1]. We see that the\\neigengap increases and levels off after a point. However, as J increases, the computation\\ntime increases. We report the accuracy of J = 5, whereas J = 8 also returns the same in 48\\nseconds.\\nTable 2: Error of 6 different methods on the simulated networks.\\nError\\nTime (s)\\n\\nNCLM (J = 5)\\n0\\n25\\n\\nGK3\\n0.5\\n14\\n\\nGK4\\n0.36\\n16\\n\\nGK5\\n0.26\\n38\\n\\nGraphStats (J = 6)\\n0.37\\n94\\n\\nTopEig (J = 5)\\n0.18\\n8\\n\\nWe see that NCLM performs the best. For GK3, GK4 and GK5, if one uses the top two\\neigenvectors , and clusters those into 4 clusters (since there are four parameter settings),\\nthe errors are respectively 0.08, 0.025 and 0.03. This means that for clustering one needs\\nto estimate the effective rank of the graphlet kernels as well. TopEig performs better than\\nGraphStats, which has trouble separating out ?2 and ?4 .\\nNote: Intuitively one would expect that, if there is node correspondence between the graphs,\\nclustering based on graphon estimates would work better, because it aims to estimate the\\nunderlying probabilistic model for comparison. However, in our experiments we found that\\na properly tuned NCLM matched the performance of NCGE. This is probably because a\\nproperly tuned NCLM captures the global features that distinguish two graphons. We leave\\nit for future work to compare their performance theoretically.\\nReal Networks: We cluster about fifty real world networks. We use 11 co-authorship\\nnetworks between 15,000 researchers from the High Energy Physics corpus of the arXiv, 11\\nco-authorship networks with 21,000 nodes from Citeseer (which had Machine Learning in\\ntheir abstracts), 17 co-authorship networks (each with about 3000 nodes) from the NIPS\\nconference and finally 10 Facebook ego networks2 . The average degrees vary between 0.2 to\\n0.4 for co-authorship networks and are around 10 for the ego networks. Each co-authorship\\nnetwork is dynamic, i.e. a node corresponds to an author in that corpus and this node index\\nis preserved in the different networks over time. The ego networks are different in that sense,\\nsince each network is the subgraph of Facebook induced by the neighbors of a given central\\nor ?ego? node. The sizes of these networks vary between 350 to 4000.\\nTable 3: Clustering error of 6 different methods on a collection of real world networks\\nconsisting of co-authorship networks from Citeseer, High Energy Physics (HEP-Th) corpus\\nof arXiv, NIPS and ego networks from Facebook.\\nError\\nTime (s)\\n\\nNCLM (J = 8)\\n0.1\\n2.7\\n\\nGK3\\n0.6\\n45\\n\\nGK4\\n0.6\\n50\\n\\nGK5\\n0.6\\n60\\n\\nGraphStats (J = 8)\\n0.16\\n765\\n\\nTopEig (J = 30)\\n0.32\\n14\\n\\nTable 3 summarizes the performance of different algorithms and their running time to\\ncompute distance between the graphs. We use the different sources of networks as labels, i.e.\\nHEP-Th will be one cluster, etc. We explore different choices of J, and see that the best\\n2\\n\\nhttps://snap.stanford.edu/data/egonets-Facebook.html\\n\\n8\\n\\n\\x0cperformance is from NCLM, with J = 8, followed closely by GraphStats. TopEig (J in this\\ncase is where the eigenspectra of the larger networks have a knee) and the graph kernels do\\nnot perform very well. GraphStats take 765 seconds to complete, whereas NCLM finishes in\\n2.7 seconds. This is because the networks are large but extremely sparse, and so calculation\\nof matrix powers is comparatively cheap.\\nFigure 2: Kernel matrix for NCLM on 49 real networks.\\n\\nIn Figure 2 we plot the kernel similarity matrix obtained using NCLM on the real networks\\n(higher the value, more similar the points are). The first 11 networks are from HEP-Th,\\nwhereas the next 11 are from Citeseer. The next 16 are from NIPS and the remaining\\nones are the ego networks from Facebook. First note that {HEP-Th, Citeseer}, NIPS and\\nFacebook are well separated. However, HEP-Th and Citeseer are hard to separate out. This\\nis also verified by the bad performance of TopEig in separating out the first two (shown in\\nSection 5). However, in Figure 2, we can see that the Citeseer networks are different from\\nHEP-Th in the sense that they are not as strongly connected inside as HEP-Th.\\n\\n6\\n\\nDiscussion\\n\\nWe consider the problem of clustering network-valued data for two settings, both of which\\nare prevalent in practice. In the first setting, different network objects have node correspondence. This includes clustering brain networks obtained from FMRI data where each node\\ncorresponds to a specific region in the brain, or co-authorship networks between a set of\\nauthors where the connections vary from one year to another. In the second setting, node\\ncorrespondence is not present, e.g., when one wishes to compare different types of networks:\\nco-authorship networks, Facebook ego networks, etc. One may be interested in seeing if\\nco-authorship networks are more ?similar? to each other than ego or friendship networks.\\nWe present two algorithms for these two settings based on a simple general theme: summarize\\na network into a possibly high dimensional feature vector and then cluster these feature\\nvectors. In the first setting, we propose NCGE, where each network is represented using its\\ngraphon-estimate. We can use a variety of graphon estimation algorithms for this purpose.\\nWe show that if the graphon estimation is consistent, then NCGE can cluster networks\\ngenerated from a finite mixture of graphons in a consistent way, if those graphons are\\nsufficiently different. In the second setting, we propose to represent a network using an\\neasy-to-compute summary statistic, namely the vector of the log-traces of the first few\\npowers of a suitably normalized version of the adjacency matrix. We call this method\\nNCLM and show that the summary statistic concentrates around its expectation, and\\nargue that this expectation should be able to separate networks generated from different\\nmodels. Using simulated and real data experiments we show that NCGE is vastly superior\\nto the naive but often-used method of comparing adjacency matrices directly, and NCLM\\noutperforms most computationally expensive alternatives for differentiating networks without\\nnode correspondence. In conclusion, we believe that these methods will provide practitioners\\nwith a powerful and computationally tractable tool for comparing network-structured data\\nin a range of disciplines.\\n9\\n\\n\\x0cAcknowledgments\\nWe thank Professor Peter J. Bickel for helpful discussions. SSM was partially supported by\\nNSF-FRG grant DMS-1160319 and a Lo?ve Fellowship. PS was partially supported by NSF\\ngrant DMS 1713082. LL was partially supported by NSF grants IIS 1663870, DMS 1654579\\nand a DARPA grant N-66001-17-1-4041.\\n\\nReferences\\n[1] Supplement to ?On clustering network-valued data?. 2017.\\n[2] David J. Aldous. Representations for partially exchangeable arrays of random variables.\\nJournal of Multivariate Analysis, 11(4):581 ? 598, 1981.\\n[3] S. et al Aliakbary. Learning an integrated distance metric for comparing structure of\\ncomplex networks. Chaos, 25(2):177?214, 2015.\\n[4] Arash A Amini, Aiyou Chen, Peter J Bickel, Elizaveta Levina, et al. Pseudo-likelihood\\nmethods for community detection in large sparse networks. The Annals of Statistics,\\n41(4):2097?2122, 2013.\\n[5] Arash A Amini and Elizaveta Levina. On semidefinite relaxations for the block model.\\narXiv preprint arXiv:1406.5647, 2014.\\n[6] Brian Ball, Brian Karrer, and MEJ Newman. Efficient and principled method for\\ndetecting communities in networks. Physical Review E, 84(3):036103, 2011.\\n[7] Peter Bickel, David Choi, Xiangyu Chang, Hai Zhang, et al. Asymptotic normality of\\nmaximum likelihood and its variational approximation for stochastic blockmodels. The\\nAnnals of Statistics, 41(4):1922?1943, 2013.\\n[8] Peter J. Bickel and Aiyou Chen. A nonparametric view of network models and newman\\ngirvan and other modularities. Proceedings of the National Academy of Sciences of the\\nUnites States of America, 106(50):21068?21073, 2009.\\n[9] Sourav Chatterjee. Matrix estimation by universal singular value thresholding. Ann.\\nStatist., 43(1):177?214, 02 2015.\\n[10] Hao Chen and Nancy Zhang. Graph-based change-point detection. Ann. Statist.,\\n43(1):139?176, 02 2015.\\n[11] David S Choi, Patrick J Wolfe, and Edoardo M Airoldi. Stochastic blockmodels with a\\ngrowing number of classes. Biometrika, page asr053, 2012.\\n[12] Paul Erd?s and Alfr?d R?nyi. On random graphs i. Publicationes Mathematicae\\n(Debrecen), 6:290?297, 1959 1959.\\n[13] Chao Gao, Yu Lu, and Harrison H. Zhou. Rate-optimal graphon estimation. Ann.\\nStatist., 43(6):2624?2652, 12 2015.\\n[14] C. E. Ginestet, P. Balanchandran, S. Rosenberg, and E. D. Kolaczyk. Hypothesis\\nTesting For Network Data in Functional Neuroimaging. ArXiv e-prints, July 2014.\\n[15] Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic\\nblockmodels: First steps. Social networks, 5(2):109?137, 1983.\\n[16] D.N. Hoover. Relations on probability spaces and arrays of random variables. Technical\\nreport, Institute of Advanced Study, Princeton., 1979.\\n[17] Brian Karrer and M. E. J. Newman. Stochastic blockmodels and community structure\\nin networks. Phys. Rev. E, 83:016107, Jan 2011.\\n[18] Jing Lei, Alessandro Rinaldo, et al. Consistency of spectral clustering in stochastic\\nblock models. The Annals of Statistics, 43(1):215?237, 2015.\\n10\\n\\n\\x0c[19] Priya Mahadevan, Dmitri Krioukov, Kevin Fall, and Amin Vahdat. Systematic topology\\nanalysis and generation using degree correlations. SIGCOMM Comput. Commun. Rev.,\\n36(4):135?146, August 2006.\\n[20] Pierre-Andr? G Maugis, Sofia C Olhede, and Patrick J Wolfe. Topology reveals universal\\nfeatures for network comparison. arXiv preprint arXiv:1705.05677, 2017.\\n[21] M. E. J. Newman. Modularity and community structure in networks. Proceedings of\\nthe National Academy of Sciences, 103(23):8577?8582, 2006.\\n[22] Mark E. Newman. Assortative mixing in networks. Phys. Rev. Lett., 89(20):208701,\\n2002.\\n[23] M.E.J. Newman. The structure and function of complex networks. SIAM review,\\n45(2):167?256, 2003.\\n[24] Sofia C. Olhede and Patrick J. Wolfe. Network histograms and universality of blockmodel\\napproximation. Proceedings of the National Academy of Sciences of the Unites States\\nof America, 111(41):14722?14727, 2014.\\n[25] Karl Rohe, Sourav Chatterjee, and Bin Yu. Spectral clustering and the high-dimensional\\nstochastic block model. Annals of Statistics, 39:1878?1915, 2011.\\n[26] N. Shervashidze, SVN. Vishwanathan, TH. Petri, K. Mehlhorn, and KM. Borgwardt.\\nEfficient graphlet kernels for large graph comparison. In JMLR Workshop and Conference\\nProceedings Volume 5: AISTATS 2009, pages 488?495, Cambridge, MA, USA, April\\n2009. Max-Planck-Gesellschaft, MIT Press.\\n[27] S. V. N. Vishwanathan, Nicol N. Schraudolph, Risi Kondor, and Karsten M. Borgwardt.\\nGraph kernels. J. Mach. Learn. Res., 11:1201?1242, August 2010.\\n[28] P. J. Wolfe and S. C. Olhede. Nonparametric graphon estimation. ArXiv e-prints,\\nSeptember 2013.\\n[29] Y. Zhang, E. Levina, and J. Zhu. Estimating network edge probabilities by neighborhood\\nsmoothing. ArXiv e-prints, September 2015.\\n[30] Shi Zhou and Raul J. Mondrag?n. The rich club phenomenon in the internet topology.\\nIEEE Communications Letters, 8(3):180?182, 2004.\\n\\n11\\n\\n\\x0c',\n",
              "       'A General Framework for Robust Interactive\\nLearning?\\n\\nEhsan Emamjomeh-Zadeh?\\n\\nDavid Kempe?\\n\\nAbstract\\nWe propose a general framework for interactively learning models, such as (binary\\nor non-binary) classifiers, orderings/rankings of items, or clusterings of data points.\\nOur framework is based on a generalization of Angluin?s equivalence query model\\nand Littlestone?s online learning model: in each iteration, the algorithm proposes a\\nmodel, and the user either accepts it or reveals a specific mistake in the proposal.\\nThe feedback is correct only with probability p > 21 (and adversarially incorrect\\nwith probability 1 ? p), i.e., the algorithm must be able to learn in the presence of\\narbitrary noise. The algorithm?s goal is to learn the ground truth model using few\\niterations.\\nOur general framework is based on a graph representation of the models and user\\nfeedback. To be able to learn efficiently, it is sufficient that there be a graph G\\nwhose nodes are the models, and (weighted) edges capture the user feedback, with\\nthe property that if s, s? are the proposed and target models, respectively, then any\\n(correct) user feedback s0 must lie on a shortest s-s? path in G. Under this one\\nassumption, there is a natural algorithm, reminiscent of the Multiplicative Weights\\nUpdate algorithm, which will efficiently learn s? even in the presence of noise in\\nthe user?s feedback.\\nFrom this general result, we rederive with barely any extra effort classic results on\\nlearning of classifiers and a recent result on interactive clustering; in addition, we\\neasily obtain new interactive learning algorithms for ordering/ranking.\\n\\n1\\n\\nIntroduction\\n\\nWith the pervasive reliance on machine learning systems across myriad application domains in the real\\nworld, these systems frequently need to be deployed before they are fully trained. This is particularly\\ntrue when the systems are supposed to learn a specific user?s (or a small group of users?) personal\\nand idiosyncratic preferences. As a result, we are seeing an increased practical interest in online and\\ninteractive learning across a variety of domains.\\nA second feature of the deployment of such systems ?in the wild? is that the feedback the system\\nreceives is likely to be noisy. Not only may individual users give incorrect feedback, but even if they\\ndo not, the preferences ? and hence feedback ? across different users may vary. Thus, interactive\\nlearning algorithms deployed in real-world systems must be resilient to noisy feedback.\\nSince the seminal work of Angluin [2] and Littlestone [14], the paradigmatic application of (noisy)\\ninteractive learning has been online learning of a binary classifier when the algorithm is provided\\nwith feedback on samples it had previously classified incorrectly. However, beyond (binary or other)\\nclassifiers, there are many other models that must be frequently learned in an interactive manner. Two\\n?\\n\\nA full version is available on the arXiv at https://arxiv.org/abs/1710.05422. The present version\\nomits all proofs and several other details and discussions.\\n?\\nDepartment of Computer Science, University of Southern California, emamjome@usc.edu\\n?\\nDepartment of Computer Science, University of Southern California, dkempe@usc.edu\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\\x0cparticularly relevant examples are the following: (1) Learning an ordering/ranking of items is a key\\npart of personalized Web search or other information-retrieval systems (e.g., [12, 18]). The user is\\ntypically presented with an ordering of items, and from her clicks or lack thereof, an algorithm can\\ninfer items that are in the wrong order. (2) Interactively learning a clustering [6, 5, 4] is important\\nin many application domains, such as interactively identifying communities in social networks or\\npartitioning an image into distinct objects. The user will be shown a candidate clustering, and can\\nexpress that two clusters should be merged, or a cluster should be split into two.\\nIn all three examples ? classification, ranking, and clustering ? the interactive algorithm proposes\\na model4 (a classifier, ranking, or clustering) as a solution. The user then provides ? explicitly or\\nimplicitly ? feedback on whether the model is correct or needs to be fixed/improved. This feedback\\nmay be incorrect with some probability. Based on the feedback, the algorithm proposes a new\\nand possibly very different model, and the process repeats. This type of interaction is the natural\\ngeneralization of Angluin?s equivalence query model [2, 3]. It is worth noting that in contrast to\\nactive learning, in interactive learning (which is the focus of this work), the algorithm cannot ?ask?\\ndirect questions; it can only propose a model and receive feedback in return. The algorithm should\\nminimize the number of user interactions, i.e., the number of times that the user needs to propose\\nfixes. A secondary goal is to make the algorithm?s internal computations efficient as well.\\nThe main contribution of this article is a general framework for efficient interactive learning of models\\n(even with noisy feedback), presented in detail in Section 2. We consider the set of all N models as\\nnodes of a positively weighted undirected or directed graph G. The one key property that G must\\nsatisfy is the following: (*) If s is a proposed model, and the user (correctly) suggests changing it to\\ns0 , then the graph must contain the edge (s, s0 ); furthermore, (s, s0 ) must lie on a shortest path from\\ns to the target model s? (which is unknown to the algorithm).\\nWe show that this single property is enough to learn the target model s? using at most log N queries5\\nto the user, in the absence of noise. When the feedback is correct with probability p > 12 , the required\\nnumber of queries gracefully deteriorates to O(log N ); the constant depends on p. We emphasize\\nthat the assumption (*) is not an assumption on the user. We do not assume that the user somehow\\n?knows? the graph G and computes shortest paths in order to find a response. Rather, (*) states that\\nG was correctly chosen to model the underlying domain, so that correct answers by the user must\\nin fact have the property (*). To illustrate the generality of our framework, we apply it to ordering,\\nclustering, and classification:\\n1. For ordering/ranking, each permutation is a node in G; one permutation is the unknown\\ntarget. If the user can point out only adjacent elements that are out of order, then G is an\\nadjacent transposition ?B UBBLE S ORT? graph, which naturally has the property (*). If the\\nuser can pick any element and suggest that it should precede an entire block of elements\\nit currently follows, then we can instead use an ?I NSERSION S ORT? graph; interestingly,\\nto ensure the property (*), this graph must be weighted. On the other hand, as we show in\\nSection 3, if the user can propose two arbitrary elements that should be swapped, there is no\\ngraph G with the property (*).\\nOur framework directly leads to an interactive algorithm that will learn the correct ordering\\nof n items in O(log(n!)) = O(n log n) queries; we show that this bound is optimal under\\nthe equivalence query model.\\n2. For learning a clustering of n items, the user can either propose merging two clusters, or\\nsplitting one cluster. In the interactive clustering model of [6, 5, 4], the user can specify\\nthat a particular cluster C should be split, but does not give a specific split. We show in\\nSection 4 that there is a weighted directed graph with the property (*); then, if each cluster\\nis from a ?small? concept class of size at most M (such as having low VC-dimension), there\\nis an algorithm finding the true clustering in O(k log M ) queries, where k is number of the\\nclusters (known ahead of time).\\n3. For binary classification, G is simply an n-dimensional hypercube (where n is the number\\nof sample points that are to be classified). As shown in Section 5, one immediately recovers\\na close variant of standard online learning algorithms within this framework. An extension\\nto classification with more than two classes is very straightforward.\\n4\\n\\nWe avoid the use of the term ?concept,? as it typically refers to a binary function, and is thus associated\\nspecifically with a classifier.\\n5\\nUnless specified otherwise, all logarithms are base 2.\\n\\n2\\n\\n\\x0cDue to space limits, all proofs and several other details and discussions are omitted. A full version is\\navailable on the arXiv at https://arxiv.org/abs/1710.05422.\\n\\n2\\n\\nLearning Framework\\n\\nWe define a framework for query-efficient interactive learning of different types of models. Some\\nprototypical examples of models to be learned are rankings/orderings of items, (unlabeled) clusterings\\nof graphs or data points, and (binary or non-binary) classifiers. We denote the set of all candidate\\nmodels (permutations, partitions, or functions from the hypercube to {0, 1}) by ?, and individual\\nmodels6 by s, s0 , s? , etc. We write N = |?| for the number of candidate models.\\nWe study interactive learning of such models in a natural generalization of the equivalence query\\nmodel of Angluin [2, 3]. This model is equivalent to the more widely known online learning model of\\nLittlestone [14], but more naturally fits the description of user interactions we follow here. It has also\\nserved as the foundation for the interactive clustering model of Balcan and Blum [6] and Awasthi et\\nal. [5, 4].\\nIn the interactive learning framework, there is an unknown ground truth model s? to be learned. In\\neach round, the learning algorithm proposes a model s to the user. In response, with probability\\np > 12 , the user provides correct feedback. In the remaining case (i.e., with probability 1 ? p), the\\nfeedback is arbitrary; in particular, it could be arbitrarily and deliberately misleading.\\nCorrect feedback is of the following form: if s = s? , then the algorithm is told this fact in the form of\\na user response of s. Otherwise, the user reveals a model s0 6= s that is ?more similar? to s? than\\ns was. The exact nature of ?more similar,? as well as the possibly restricted set of suggestions s0\\nthat the user can propose, depend on the application domain. Indeed, the strength of our proposed\\nframework is that it provides strong query complexity guarantees under minimal assumptions about\\nthe nature of the feedback; to employ the framework, one merely has to verify that the the following\\nassumption holds.\\nDefinition 2.1 (Graph Model for Feedback) Define a weighted graph G (directed or undirected)\\nthat contains one node for each model s ? ?, and an edge (s, s0 ) with arbitrary positive edge length\\n?(s,s0 ) > 0 if the user is allowed to propose s0 in response to s. (Choosing the lengths of edges is an\\nimportant part of using the framework.) G may contain additional edges not corresponding to any\\nuser feedback. The key property that G must satisfy is the following: (*) If the algorithm proposes\\ns and the ground truth is s? 6= s, then every correct user feedback s0 lies on a shortest path from s\\nto s? in G with respect to the lengths ?e . If there are multiple candidate nodes s0 , then there is no\\nguarantee on which one the algorithm will be given by the user.\\n2.1\\n\\nAlgorithm and Guarantees\\n\\nOur algorithms are direct reformulations and slight generalizations of algorithms recently proposed\\nby Emamjomeh-Zadeh et al. [10], which itself was a significant generalization of the natural ?Halving\\nAlgorithm? for learning a classifier (e.g., [14]). They studied the search problem as an abstract\\nproblem they termed ?Binary Search in Graphs,? without discussing any applications. Our main\\ncontribution here is the application of the abstract search problem to a large variety of interactive\\nlearning problems, and a framework that makes such applications easy. We begin with the simplest\\ncase p = 1, i.e., when the algorithm only receives correct feedback.\\nAlgorithm 1 gives essentially best-possible general guarantees [10]. To state the algorithm and its\\nguarantees, we need the notion of an approximate median node of the graph G. First, we denote by\\n\\x1a\\n{s}\\nif s0 = s\\nN (s, s0 ) :=\\n0\\n{?\\ns | s lies on a shortest path from s to s?} if s0 6= s\\nthe set of all models s? that are consistent with a user feedback of s0 to a model s. In anticipation\\nof the noisy case, we allow models to be weighted7 , and denote the node weights or likelihoods by\\n6\\nWhen considering specific applications, we will switch to notation more in line with that used for the\\nspecific application.\\n7\\nEdge lengths are part of the definition of the graph, but node weights will be assigned by our algorithm;\\nthey basically correspond to likelihoods.\\n\\n3\\n\\n\\x0c?(s) ? 0. If feedback is not noisy (i.e.,\\nPp = 1), all the non-zero node weights are equal. For every\\nsubset of models S, we write ?(S) := s?S ?(s) for the total node weight of the models in S. Now,\\nfor every model s, define\\n1\\n?? (s) :=\\n?\\nmax\\n?(N (s, s0 ))\\n?(?) s0 6=s,(s,s0 )?G\\nto be the largest fraction (with respect to node weights) of models that could still be consistent with a\\nworst-case response s0 to a proposed model of s. For every subset of models S, we denote by ?S\\nthe likelihood function that assigns weight 1 to every node s ? S and 0 elsewhere. For simplicity of\\nnotation, we use ?S (s) when the node weights are ?S .\\nThe simple key insight of [10] can be summarized and reformulated as the following proposition:\\nProposition 2.1 ([10], Proofs of Theorems 3 and 14) Let G be a (weighted) directed graph in\\nwhich each edge e with length ?e is part of a cycle of total edge length at most c ? ?e . Then,\\nfor every node weight function ?, there exists a model s such that ?? (s) ? c?1\\nc .\\nWhen G is undirected (and hence c = 2), for every node weight function ?, there exists an s such that\\n?? (s) ? 12 .\\nIn Algorithm 1, we always have uniform node weight for all the models which are consistent with\\nall the feedback received so far, and node weight 0 for models that are inconsistent with at least one\\nresponse. Prior knowledge about candidates for s? can be incorporated by providing the algorithm\\nwith the input Sinit 3 s? to focus its search on; in the absence of prior knowledge, the algorithm can\\nbe given Sinit = ?.\\nAlgorithm 1 L EARNING A MODEL WITHOUT F EEDBACK E RRORS (Sinit )\\n1: S ? Sinit .\\n2: while |S| > 1 do\\n3:\\nLet s be a model with a ?small? value of ?S (s).\\n4:\\nLet s0 be the user?s feedback model.\\n5:\\nSet S ? S ? N (s, s0 ).\\n6: return the only remaining model in S.\\n\\nLine 3 is underspecified as ?small.? Typically, an algorithm would choose the s with smallest ?S (s).\\nBut computational efficiency constraints or other restrictions (see Sections 2.2 and 5) may preclude\\nthis choice and force the algorithm to choose a suboptimal s. The guarantee of Algorithm 1 is\\nsummarized by the following Theorem 2.2. It is a straightforward generalization of Theorems 3 and\\n14 from [10]\\nTheorem 2.2 Let N0 = |Sinit | be the number of initial candidate models. If each model s chosen in\\nLine 3 of Algorithm 1 has ?S (s) ? ?, then Algorithm 1 finds s? using at most log1/? N0 queries.\\nCorollary 2.3 When G is undirected and the optimal s is used in each iteration, ? =\\nAlgorithm 1 finds s? using at most log2 N0 queries.\\n\\n1\\n2\\n\\nand\\n\\nIn the presence of noise, the algorithm is more complicated. The algorithm and its analysis are given\\nin the full version. The performance of the robust algorithm is summarized in Theorem 2.4.\\nTheorem 2.4 Let ? ? [ 12 , 1), define ? = ?p + (1 ? ?)(1 ? p), and let N0 = |Sinit |. Assume that\\nlog(1/? ) > H(p) where H(p) = ?p log p ? (1 ? p) log(1 ? p) denotes the entropy. (When ? = 21 ,\\nthis holds for every p > 12 .)\\nIf in each iteration, the algorithm can find a model s with ?? (s) ? ?, then with probability at least\\n(1??)\\n2\\n1 ? ?, the robust algorithm finds s? using at most log(1/?\\n)?H(p) log N0 + o(log N0 ) + O(log (1/?))\\nqueries in expectation.\\nCorollary 2.5 When the graph G is undirected and the optimal s is used in each iteration, then with\\n(1??)\\nprobability at least 1 ? ?, the robust algorithm finds s? using at most 1?H(p)\\nlog2 N0 + o(log N0 ) +\\nO(log2 (1/?)) queries in expectation.\\n4\\n\\n\\x0c2.2\\n\\nComputational Considerations and Sampling\\n\\nCorollaries 2.3 and 2.5 require the algorithm to find a model s with small ?? (s) in each iteration. In\\nmost learning applications, the number N of candidate models is exponential in a natural problem\\nparameter n, such as the number of sample points (classification), or the number of items to rank or\\ncluster. If computational efficiency is a concern, this precludes explicitly keeping track of the set S or\\nthe weights ?(s). It also rules out determining the model s to query by exhaustive search over all\\nmodels that have not yet been eliminated.\\nIn some cases, these difficulties can be circumvented by exploiting problem-specific structure. A\\nmore general approach relies on Monte Carlo techniques. We show that the ability to sample models\\ns with probability (approximately) proportional to ?(s) (or approximately uniformly from S in the\\ncase of Algorithm 1) is sufficient to essentially achieve the results of Corollaries 2.3 and 2.5 with a\\ncomputationally efficient algorithm. Notice that both in Algorithm 1 and the robust algorithm with\\nnoisy feedback (omitted from this version), the node weights ?(s) are completely determined by all\\nthe query responses the algorithm has seen so far and the probability p.\\nTheorem 2.6 Let n be a natural measure of the input size and assume that log N is polynomial in n.\\nAssume that G = (V, E) is undirected8 , all edge lengths are integers, and the maximum degree and\\ndiameter (both with respect to the edge lengths) are bounded by poly(n). Also assume w.l.o.g. that ?\\nis normalized to be a distribution over the nodes9 (i.e., ?(?) = 1).\\nLet 0 ? ? < 14 be a constant, and assume that there is an oracle that ? given a set of query\\nresponses ? runs in polynomial time in n and returns a model s drawn from a distribution ?0 with\\ndTV (?, ?0 ) ? ?. Also assume that there is a polynomial-time algorithm that, given a model s,\\ndecides whether or not s is consistent with every given query response or not.\\nThen, for every \\x0f > 0, in time poly(n, 1\\x0f ), an algorithm can find a model s with ?? (s) ?\\nwith high probability.\\n\\n3\\n\\n1\\n2\\n\\n+ 2? + \\x0f,\\n\\nApplication I: Learning a Ranking\\n\\nAs a first application, we consider the task of learning the correct order of n elements with supervision\\nin the form of equivalence queries. This task is motivated by learning a user?s preference over web\\nsearch results (e.g., [12, 18]), restaurant or movie orders (e.g., [9]), or many other types of entities.\\nUsing pairwise active queries (?Do you think that A should be ranked ahead of B??), a learning\\nalgorithm could of course simulate standard O(n log n) sorting algorithms; this number of queries is\\nnecessary and sufficient. However, when using equivalence queries, the user must be presented with\\na complete ordering (i.e., a permutation ? of the n elements), and the feedback will be a mistake in\\nthe proposed permutation. Here, we propose interactive algorithms for learning the correct ranking\\nwithout additional information or assumptions.10 We first describe results for a setting with simple\\nfeedback in the form of adjacent transpositions; we then show a generalization to more realistic\\nfeedback as one is wont to receive in applications such as search engines.\\n3.1\\n\\nAdjacent Transpositions\\n\\nWe first consider ?B UBBLE S ORT? feedback of the following form: the user specifies that elements\\ni and i + 1 in the proposed permutation ? are in the wrong relative order. An obvious correction\\nfor an algorithm would be to swap the two elements, and leave the rest of ? intact. This algorithm\\nwould exactly implement B UBBLE S ORT, and thus require ?(n2 ) equivalence queries. Our general\\nframework allows us to easily obtain an algorithm with O(n log n) equivalence queries instead. We\\ndefine the undirected and unweighted graph GBS as follows:\\n? GBS contains N = n! nodes, one for each permutation ? of the n elements;\\n? it contains an edge between ? and ? 0 if and only if ? 0 can be obtained from ? by swapping\\ntwo adjacent elements.\\n8\\nIt is actually sufficient that for every node weight function ? : V ? R+ , there exists a model s with\\n?? (s) ? 12 .\\n9\\nFor Algorithm 1, ? is uniform over all models consistent with all feedback up to that point.\\n10\\nFor example, [12, 18, 9] map items to feature vectors and assume linearity of the target function(s).\\n\\n5\\n\\n\\x0cLemma 3.1 GBS satisfies Definition 2.1 with respect to B UBBLE S ORT feedback.\\nHence, applying Corollary 2.3 and Theorem 2.4, we immediately obtain the existence of learning\\nalgorithms with the following properties:\\nCorollary 3.2 Assume that in response to each equivalence query on a permutation ?, the user\\nresponds with an adjacent transposition (or states that the proposed permutation ? is correct).\\n1. If all query responses are correct, then the target ordering can be learned by an interactive\\nalgorithm using at most log N = log n! ? n log n equivalence queries.\\n2. If query responses are correct with probability p > 12 , the target ordering can be learned\\n(1??)\\nby an interactive algorithm with probability at least 1 ? ? using at most 1?H(p)\\nn log n +\\no(n log n) + O(log2 (1/?)) equivalence queries in expectation.\\nUp to constants, the bound of Corollary 3.2 is optimal: Theorem 3.3 shows that ?(n log n) equivalence queries are necessary in the worst case. Notice that Theorem 3.3 does not immediately follow\\nfrom the classical lower bound for sorting with pairwise comparisons: while the result of a pairwise\\ncomparison always reveals one bit, there are n ? 1 different possible responses to an equivalence\\nquery, so up to O(log n) bits might be revealed. For this reason, the proof of Theorem 3.3 explicitly\\nconstructs an adaptive adversary, and does not rely on a simple counting argument.\\nTheorem 3.3 With adversarial responses, any interactive ranking algorithm can be forced to ask\\n?(n log n) equivalence queries. This is true even if the true ordering is chosen uniformly at random,\\nand only the query responses are adversarial.\\n3.2\\n\\nImplicit Feedback from Clicks\\n\\nIn the context of search engines, it has been argued (e.g., by [12, 18, 1]) that a user?s clicking behavior\\nprovides implicit feedback of a specific form on the ranking. Specifically, since users will typically\\nread the search results from first to last, when a user skips some links that appear earlier in the\\nranking, and instead clicks on a link that appears later, her action suggests that the later link was more\\ninformative or relevant.\\nFormally, when a user clicks on the element at index i, but did not previously click on any elements at\\nindices j, j + 1, . . . , i ? 1, this is interpreted as feedback that element i should precede all of elements\\nj, j + 1, . . . , i ? 1. Thus, the feedback is akin to an ?I NSERSION S ORT? move. (The B UBBLE S ORT\\nfeedback model is the special case in which j = i ? 1 always.)\\nTo model this more informative feedback, the new graph GIS has more edges, and the edge lengths\\nare non-uniform. It contains the same N nodes (one for each permutation). For a permutation ? and\\nindices 1 ? j < i ? n, ?j?i denotes the permutation that is obtained by moving the ith element in\\n? before the j th element (and thus shifting elements j, j + 1, . . . , i ? 1 one position to the right). In\\nGIS , for every permutation ? and every 1 ? j < i ? n, there is an undirected edge from ? to ?j?i\\nwith length i ? j. Notice that for i > j + 1, there is actually no user feedback corresponding to the\\nedge from ?j?i to ?; however, additional edges are permitted, and Lemma 3.4 establishes that GIS\\ndoes in fact satisfy the ?shortest paths? property.\\nLemma 3.4 GIS satisfies Definition 2.1 with respect to I NSERSION S ORT feedback.\\nAs in the case of GBS , by applying Corollary 2.3 and Theorem 2.4, we immediately obtain the\\nexistence of interactive learning algorithms with the same guarantees as those of Corollary 3.2.\\nCorollary 3.5 Assume that in response to each equivalence query, the user responds with a pair of\\nindices j < i such that element i should precede all elements j, j + 1, . . . , i ? 1.\\n1. If all query responses are correct, then the target ordering can be learned by an interactive\\nalgorithm using at most log N = log n! ? n log n equivalence queries.\\n2. If query responses are correct with probability p > 12 , the target ordering can be learned\\n(1??)\\nby an interactive algorithm with probability at least 1 ? ? using at most 1?H(p)\\nn log n +\\no(n log n) + O(log2 (1/?)) equivalence queries in expectation.\\n6\\n\\n\\x0c3.3\\n\\nComputational Considerations\\n\\nWhile Corollaries 3.2 and 3.5 imply interactive algorithms using O(n log n) equivalence queries,\\nthey do not guarantee that the internal computations of the algorithms are efficient. The na??ve\\nimplementation requires keeping track of and comparing likelihoods on all N = n! nodes.\\nWhen p = 1, i.e., the algorithm only receives correct feedback, it can be made computationally\\nefficient using Theorem 2.6. To apply Theorem 2.6, it suffices to show that one can efficiently sample\\na (nearly) uniformly random permutation ? consistent with all feedback received so far. Since the\\nfeedback is assumed to be correct, the set of all pairs (i, j) such that the user implied that element i\\nmust precede element j must be acyclic, and thus must form a partial order. The sampling problem is\\nthus exactly the problem of sampling a linear extension of a given partial order.\\nThis is a well-known problem, and a beautiful result of Bubley and Dyer [8, 7] shows that the\\nKarzanov-Khachiyan Markov Chain [13] mixes rapidly. Huber [11] shows how to modify the Markov\\nChain sampling technique to obtain an exactly (instead of approximately) uniformly random linear\\nextension of the given partial order. For the purpose of our interactive learning algorithm, the sampling\\nresults can be summarized as follows:\\nTheorem 3.6 (Huber [11]) Given a partial order over n elements, let L be the set of all linear\\nextensions, i.e., the set of all permutations consistent with the partial order. There is an algorithm\\nthat runs in expected time O(n3 log n) and returns a uniformly random sample from L.\\nThe maximum node degree in GBS is n ? 1, while the maximum node degree in GIS is O(n2 ). The\\ndiameter of both GBS and GIS is O(n2 ). Substituting these bounds and the bound from Theorem 3.6\\ninto Theorem 2.6, we obtain the following corollary:\\nCorollary 3.7 Both under B UBBLE S ORT feedback and I NSERSION S ORT feedback, if all feedback is\\ncorrect, there is an efficient interactive learning algorithm using at most log n! ? n log n equivalence\\nqueries to find the target ordering.\\nThe situation is significantly more challenging when feedback could be incorrect, i.e., when p < 1.\\nIn this case, the user?s feedback is not always consistent and may not form a partial order. In fact, we\\nprove the following hardness result.\\nTheorem 3.8 There exists a p (depending on n) for which the following holds. GivenPa set of user\\nresponses, let ?(?) be the likelihood of ? given the responses, and normalized so that ? ?(?) = 1.\\nLet 0 < ? < 1 be any constant. There is no polynomial-time algorithm to draw a sample from a\\ndistribution ?0 with dTV (?, ?0 ) ? 1 ? ? unless RP = NP.\\nIt should be noted that the value of p in the reduction is exponentially close to 1. In this range,\\nincorrect feedback is so unlikely that with high probability, the algorithm will always see a partial\\norder. It might then still be able to sample efficiently. On the other hand, for smaller values of p\\n(e.g., constant p), sampling approximately from the likelihood distribution might be possible via a\\nmetropolized Karzanov-Khachiyan chain or a different approach. This problem is still open.\\n\\n4\\n\\nApplication II: Learning a Clustering\\n\\nMany traditional approaches for clustering optimize an (explicit) objective function or rely on\\nassumptions about the data generation process. In interactive clustering, the algorithm repeatedly\\nproposes a clustering, and obtains feedback that two proposed clusters should be merged, or a\\nproposed cluster should be split into two. There are n items, and a clustering C is a partition of the\\nitems into disjoint sets (clusters) C1 , C2 , . . .. It is known that the target clustering has k clusters, but\\nin order to learn it, the algorithm can query clusterings with more or fewer clusters as well. The user\\nfeedback has the following semantics, as proposed by Balcan and Blum [6] and Awasthi et al. [5, 4].\\n1. M ERGE(Ci , Cj ): Specifies that all items in Ci and Cj belong to the same cluster.\\n2. S PLIT(Ci ): Specifies that cluster Ci needs to be split, but not into which subclusters.\\n7\\n\\n\\x0cNotice that feedback that two clusters be merged, or that a cluster be split (when the split is known),\\ncan be considered as adding constraints on the clustering (see, e.g., [21]); depending on whether\\nfeedback may be incorrect, these constraints are hard or soft.\\nWe define a weighted and directed graph GUC on all clusterings C. Thus, N = Bn ? nn is the nth\\nBell number. When C 0 is obtained by a M ERGE of two clusters in C, GUC contains a directed edge\\n(C, C 0 ) of length 2. If C = {C1 , C2 , . . .} is a clustering, then for each Ci ? C, the graph GUC contains\\na directed edge of length 1 from C to C \\\\ {Ci } ? {{v} | v ? Ci }. That is, GUC contains an edge from\\nC to the clustering obtained from breaking Ci into singleton clusters of all its elements. While this\\nmay not be the ?intended? split of the user, we can still associate this edge with the feedback.\\nLemma 4.1 GUC satisfies Definition 2.1 with respect to M ERGE and S PLIT feedback.\\n1\\nGUC is directed, and every edge makes up at least a 3n\\nfraction of the total length of at least one cycle\\nit participates in. Hence, Proposition 2.1 gives an upper bound of 3n?1\\n3n on the value of ? in each\\niteration. A more careful analysis exploiting the specific structure of GUC gives us the following:\\n\\nLemma 4.2 In GUC , for every non-negative node weight function ?, there exists a clustering C with\\n?? (C) ? 21 .\\nIn the absence of noise in the feedback, Lemmas 4.1 and 4.2 and Theorem 2.2 imply an algorithm\\nthat finds the true clustering using log N = log B(n) = ?(n log n) queries. Notice that this is worse\\nthan the ?trivial? algorithm, which starts with each node as a singleton cluster and always executes\\nthe merge proposed by the user, until it has found the correct clustering; hence, this bound is itself\\nrather trivial.\\nNon-trivial bounds can be obtained when clusters belong to a restricted set, an approach also followed\\nby Awasthi and Zadeh [5]. If there are at most M candidate clusters, then the number of clusterings is\\nN0 ? M k . For example, if there is a set system F of VC dimension at most d such that each cluster\\nis in the range space of F, then M = O(nd ) by the Sauer-Shelah Lemma [19, 20]. Combining\\nLemmas 4.1 and 4.2 with Theorems 2.2 and 2.4, we obtain the existence of learning algorithms with\\nthe following properties:\\nCorollary 4.3 Assume that in response to each equivalence query, the user responds with M ERGE\\nor S PLIT. Also, assume that there are at most M different candidate clusters, and the clustering has\\n(at most) k clusters.\\n1. If all query responses are correct, then the target clustering can be learned by an interactive\\nalgorithm using at most log N = O(k log M ) equivalence queries. Specifically when\\nM = O(nd ), this bound is O(kd log n). This result recovers the main result of [5].11\\n2. If query responses are correct with probability p > 21 , the target clustering can be learned\\nlog M\\nwith probability at least 1 ? ? using at most (1??)k\\n+ o(k log M ) + O(log2 (1/?))\\n1?H(p)\\nequivalence queries in expectation. Our framework provides the noise tolerance ?for free;?\\n[5] instead obtain results for a different type of noise in the feedback.\\n\\n5\\n\\nApplication III: Learning a Classifier\\n\\nLearning a binary classifier is the original and prototypical application of the equivalence query\\nmodel of Angluin [2], which has seen a large amount of follow-up work since (see, e.g., [16, 17]).\\nNaturally, if no assumptions are made on the classifier, then n queries are necessary in the worst case.\\nIn general, applications therefore restrict the concept classes to smaller sets, such as assuming that\\nthey have bounded VC dimension. We use F to denote the set of all possible concepts, and write\\nM = |F|; when F has VC dimension d, the Sauer-Shelah Lemma [19, 20] implies that M = O(nd ).\\nLearning a binary classifier for n points is an almost trivial application of our framework12 . When\\nthe algorithm proposes a candidate classifier, the feedback it receives is a point with a corrected label\\n(or the fact that the classifier was correct on all points).\\n11\\n12\\n\\nIn fact, the algorithm in [5] is implicitly computing and querying a node with small ? in GUC\\nThe results extend readily to learning a classifier with k ? 2 labels.\\n\\n8\\n\\n\\x0cWe define the graph GCL to be the n-dimensional hypercube13 with unweighted and undirected edges\\nbetween every pair of nodes at Hamming distance 1. Because the distance between two classifiers C,\\nC 0 is exactly the number of points on which they disagree, GCL satisfies Definition 2.1. Hence, we\\ncan apply Corollary 2.3 and Theorem 2.4 with Sinit equal to the set of all M candidate classifiers,\\nrecovering the classic result on learning a classifier in the equivalence query model when feedback is\\nperfect, and extending it to the noisy setting.\\nCorollary 5.1\\n\\n1. With perfect feedback, the target classifier is learned using log M queries14 .\\n\\n2. When each query response is correct with probability p > 12 , there is an algorithm learning\\nlog M\\nthe true binary classifier with probability at least 1?? using at most (1??)\\n1?H(p) +o(log M )+\\nO(log2 (1/?)) queries in expectation.\\n\\n6\\n\\nDiscussion and Conclusions\\n\\nWe defined a general framework for interactive learning from imperfect responses to equivalence\\nqueries, and presented a general algorithm that achieves a small number of queries. We then showed\\nhow query-efficient interactive learning algorithms in several domains can be derived with practically\\nno effort as special cases; these include some previously known results (classification and clustering)\\nas well as new results on ranking/ordering.\\nOur work raises several natural directions for future work. Perhaps most importantly, for which\\ndomains can the algorithms be made computationally efficient (in addition to query-efficient)? We\\nprovided a positive answer for ordering with perfect query responses, but the question is open\\nfor ordering when feedback is imperfect. For classification, when the possible clusters have VC\\ndimension d, the time is O(nd ), which is unfortunately still impractical for real-world values of d.\\nMaass and Tur?an [15] show how to obtain better bounds specifically when the sample points form a\\nd-dimensional grid; to the best of our knowledge, the question is open when the sample points are\\narbitrary. The Monte Carlo approach of Theorem 2.6 reduces the question to the question of sampling\\na uniformly random hyperplane, when the uniformity is over the partition induced by the hyperplane\\n(rather than some geometric representation). For clustering, even less appears to be known.\\nIt should be noted that our algorithms may incorporate ?improper? learning steps: for instance, when\\ntrying to learn a hyperplane classifier, the algorithm in Section 5 may propose intermediate classifiers\\nthat are not themselves hyperplanes (though the final output is of course a hyperplane classifier). At\\nan increase of a factor O(log d) in the number of queries, we can ensure that all steps are proper for\\nhyperplane learning. An interesting question is whether similar bounds can be obtained for other\\nconcept classes, and for other problems (such as clustering).\\nFinally, our noise model is uniform. An alternative would be that the probability of an incorrect\\nresponse depends on the type of response. In particular, false positives could be extremely likely, for\\ninstance, because the user did not try to classify a particular incorrectly labeled data point, or did not\\nsee an incorrect ordering of items far down in the ranking. Similarly, some wrong responses may be\\nmore likely than others; for example, a user proposing a merge of two clusters (or split of one) might\\nbe ?roughly? correct, but miss out on a few points (the setting that [5, 4] studied). We believe that\\nseveral of these extensions should be fairly straightforward to incorporate into the framework, and\\nwould mostly lead to additional complexity in notation and in the definition of various parameters.\\nBut a complete and principled treatment would be an interesting direction for future work.\\nAcknowledgments\\nResearch supported in part by NSF grant 1619458. We would like to thank Sanjoy Dasgupta, Ilias\\nDiakonikolas, Shaddin Dughmi, Haipeng Luo, Shanghua Teng, and anonymous reviewers for useful\\nfeedback and suggestions.\\n13\\n14\\n\\nWhen there are k labels, GCL is a graph with kn nodes.\\nWith k labels, this bound becomes (k ? 1) log M .\\n\\n9\\n\\n\\x0cReferences\\n[1] E. Agichtein, E. Brill, S. Dumais, and R. Ragno. Learning user interaction models for predicting\\nweb search result preferences. In Proc. 29th Intl. Conf. on Research and Development in\\nInformation Retrieval (SIGIR), pages 3?10, 2006.\\n[2] D. Angluin. Queries and concept learning. Machine Learning, 2:319?342, 1988.\\n[3] D. Angluin. Computational learning theory: Survey and selected bibliography. In Proc. 24th\\nACM Symp. on Theory of Computing, pages 351?369, 1992.\\n[4] P. Awasthi, M.-F. Balcan, and K. Voevodski. Local algorithms for interactive clustering. Journal\\nof Machine Learning Research, 18:1?35, 2017.\\n[5] P. Awasthi and R. B. Zadeh. Supervised clustering. In Proc. 24th Advances in Neural Information\\nProcessing Systems, pages 91?99. 2010.\\n[6] M.-F. Balcan and A. Blum. Clustering with interactive feedback. In Proc. 19th Intl. Conf. on\\nAlgorithmic Learning Theory, pages 316?328, 2008.\\n[7] R. Bubley. Randomized Algorithms: Approximation, Generation, and Counting. Springer, 2001.\\n[8] R. Bubley and M. Dyer. Faster random generation of linear extensions. Discrete Mathematics,\\n201(1):81?88, 1999.\\n[9] K. Crammer and Y. Singer. Pranking with ranking. In Proc. 16th Advances in Neural Information\\nProcessing Systems, pages 641?647, 2002.\\n[10] E. Emamjomeh-Zadeh, D. Kempe, and V. Singhal. Deterministic and probabilistic binary search\\nin graphs. In Proc. 48th ACM Symp. on Theory of Computing, pages 519?532, 2016.\\n[11] M. Huber. Fast perfect sampling from linear extensions. Discrete Mathematics, 306(4):420?428,\\n2006.\\n[12] T. Joachims. Optimizing search engines using clickthrough data. In Proc. 8th Intl. Conf. on\\nKnowledge Discovery and Data Mining, pages 133?142, 2002.\\n[13] A. Karzanov and L. Khachiyan. On the conductance of order Markov chains. Order, 8(1):7?15,\\n1991.\\n[14] N. Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold\\nalgorithm. Machine Learning, 2:285?318, 1988.\\n[15] W. Maass and G. Tur?an. On the complexity of learning from counterexamples and membership\\nqueries. In Proc. 31st IEEE Symp. on Foundations of Computer Science, pages 203?210, 1990.\\n[16] W. Maass and G. Tur?an. Lower bound methods and separation results for on-line learning\\nmodels. Machine Learning, 9(2):107?145, 1992.\\n[17] W. Maass and G. Tur?an. Algorithms and lower bounds for on-line learning of geometrical\\nconcepts. Machine Learning, 14(3):251?269, 1994.\\n[18] F. Radlinski and T. Joachims. Query chains: Learning to rank from implicit feedback. In Proc.\\n11th Intl. Conf. on Knowledge Discovery and Data Mining, pages 239?248, 2005.\\n[19] N. Sauer. On the density of families of sets. Journal of Combinatorial Theory, Series A,\\n13(1):145?147, 1972.\\n[20] S. Shelah. A combinatorial problem; stability and order for models and theories in infinitary\\nlanguages. Pacific Journal of Mathematics, 41(1):247?261, 1972.\\n[21] K. L. Wagstaff. Intelligent Clustering with Instance-Level Constraints. PhD thesis, Cornell\\nUniversity, 2002.\\n\\n10\\n\\n\\x0c',\n",
              "       'Multi-view Matrix Factorization for Linear\\nDynamical System Estimation\\n\\nMahdi Karami, Martha White, Dale Schuurmans, Csaba Szepesv?ri\\nDepartment of Computer Science\\nUniversity of Alberta\\nEdmonton, AB, Canada\\n{karami1, whitem, daes, szepesva}@ualberta.ca\\n\\nAbstract\\nWe consider maximum likelihood estimation of linear dynamical systems with\\ngeneralized-linear observation models. Maximum likelihood is typically considered\\nto be hard in this setting since latent states and transition parameters must be\\ninferred jointly. Given that expectation-maximization does not scale and is prone\\nto local minima, moment-matching approaches from the subspace identification\\nliterature have become standard, despite known statistical efficiency issues. In this\\npaper, we instead reconsider likelihood maximization and develop an optimization\\nbased strategy for recovering the latent states and transition parameters. Key to\\nthe approach is a two-view reformulation of maximum likelihood estimation for\\nlinear dynamical systems that enables the use of global optimization algorithms for\\nmatrix factorization. We show that the proposed estimation strategy outperforms\\nwidely-used identification algorithms such as subspace identification methods, both\\nin terms of accuracy and runtime.\\n\\n1\\n\\nIntroduction\\n\\nLinear dynamical systems (LDS) provide a fundamental model for estimation and forecasting in\\ndiscrete-time multi-variate time series. In an LDS, each observation is associated with a latent state;\\nthese unobserved states evolve as a Gauss-Markov process where each state is a linear function of the\\nprevious state plus noise. Such a model of a partially observed dynamical system has been widely\\nadopted, particularly due to its efficiency for prediction of future observations using Kalman filtering.\\nEstimating the parameters of an LDS?sometimes referred to as system identification?is a difficult\\nproblem, particularly if the goal is to obtain the maximum likelihood estimate of parameters. Consequently, spectral methods from the subspace identification literature, based on moment-matching\\nrather than maximum likelihood, have become popular. These methods provide closed form solutions,\\noften involving a singular value decomposition of a matrix constructed from the empirical moments\\nof observations (Moonen and Ramos, 1993; Van Overschee and De Moor, 1994; Viberg, 1995;\\nKatayama, 2006; Song et al., 2010; Boots and Gordon, 2012). The most widely used such algorithms\\nfor parameter estimation in LDSs are the family of N4SID algorithms (Van Overschee and De Moor,\\n1994), which are computationally efficient and asymptotically consistent (Andersson, 2009; Hsu\\net al., 2012). Recent evidence, however, suggests that these moment-matching approaches may suffer\\nfrom weak statistical efficiency, performing particularly poorly with small sample sizes (Foster et al.,\\n2012; Zhao and Poupart, 2014).\\nMaximum likelihood for LDS estimation, on the other hand, has several advantages. For example, it\\nis asymptotically efficient under general conditions (Cram?r, 1946, Ch.33), and this property often\\ntranslates to near-minimax finite-sample performance. Further, maximum likelihood is amenable\\nto coping with missing data. Another benefit is that, since the likelihood for exponential families\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\\x0cand corresponding convex losses (Bregman divergences) are well understood (Banerjee et al., 2005),\\nmaximum likelihood approaches can generalize to a broad range of distributions over the observations.\\nSimilarly, other common machine learning techniques, such as regularization, can be naturally\\nincorporated in a maximum likelihood framework, interpretable as maximum a posteriori estimation.\\nUnfortunately, unlike spectral methods, there is no known efficient algorithm for recovering parameters that maximize the marginal likelihood of observed data in an LDS. Standard iterative\\napproaches are based on EM (Ghahramani and Hinton, 1996; Roweis and Ghahramani, 1999), which\\nare computationally expensive and have been observed to produce locally optimal solutions that yield\\npoor results (Katayama, 2006). A classical system identification method, called the prediction error\\nmethod (PEM), is based on minimization of prediction error and can be interpreted as maximum\\nlikelihood estimation under certain distributional assumptions (e.g., Ch. 7.4 of Ljung 1999, ?str?m\\n1980). PEM, however, is prone to local minima and requires selection of a canonical parameterization,\\nwhich can be difficult in practice and can result in ill-conditioned problems (Katayama, 2006).\\nIn this paper, we propose an alternative approach to LDS parameter estimation under exponential\\nfamily observation noise. In particular, we reformulate the LDS as a two-view generative model,\\nwhich allows us to approximate the estimation task as a form of matrix factorization, and apply recent\\nglobal optimization techniques for such models (Zhang et al., 2012; Yu et al., 2014). To extend these\\nprevious algorithms to this setting, we provide a novel proximal update for the two-view approach\\nthat significantly simplifies the algorithm. Finally, for forecasting on synthetic and real data, we\\ndemonstrate that the proposed algorithm matches or outperforms N4SID, while scaling better with\\nincreasing sample size and data dimension.\\n\\n2\\n\\nLinear dynamical systems\\n\\nWe address discrete-time, time-invariant linear dynamical systems, specified as\\n?t+1 = A?t + ?t\\nxt = C?t + \\x0ft\\n\\n(1)\\n\\nwhere ?t ? Rk is the hidden state at time t; xt ? Rd is the observation vector at time t; A ? Rk?k is\\nthe dynamics matrix; C ? Rd?k is the observation matrix; ? is the state evolution noise; and \\x0f is the\\nobservation noise. The noise terms are assumed to be independent. As is common, we assume that the\\nstate evolution noise is Gaussian: ? ? N (0, ?? ). We additionally allow for general observation noise\\nto be generated from an exponential family distribution (e.g., Poisson). The graphical representation\\nfor this LDS is shown in Figure 1.\\nAn LDS encodes the intuition that a latent state is driving the dynamics, which can significantly\\nsimplify estimation and forecasting. The observations typically contain only partial information\\nabout the environment (such as in the form of limited sensors), and further may contain noisy or\\neven irrelevant observations. Learning transition models for such observations can be complex,\\nparticularly if the observations are high-dimensional. For example, in spatiotemporal processes, the\\ndata is typically extremely high-dimensional, composed of structured grid data; however, it is possible\\nto extract a low-rank state-space that significantly simplifies analysis (Gelfand et al., 2010, Chapter\\n8). Further, for forecasting, iterating transitions for such a low-rank state-space can provide longer\\nrange predictions with less error accumulation than iterating with the observations themselves.\\nThe estimation problem for an LDS involves extracting the unknown parameters, given a time series\\nof observations x1 , . . . , xT . Unfortunately, jointly estimating the parameters A, C and ?t is difficult\\nbecause the multiplication of these variables typically results in a nonconvex optimization. Given the\\nlatent states ?t , estimation of A and C is more straightforward, though there are still some issues\\nwith maintaining stability (Siddiqi et al., 2007). There are some recent advances improving estimation\\nin time series models using matrix factorization. White et al. (2015) provide a convex formulation for\\nauto-regressive moving average models?although related to state-space models, these do not permit\\na straightforward conversion between the parameters of one to the other. Yu et al. (2015) factorize the\\nobservation into a hidden state and dictionary, using a temporal regularizer on the extracted hidden\\nstate?the resulting algorithm, however, is not guaranteed to provide an optimal solution.\\n2\\n\\n\\x0c?1\\n\\nA\\n\\n?2\\n\\n?3\\n...\\n\\nE\\n\\nC\\n\\n...\\nx1\\n3\\n\\nx2\\n\\nFigure 1: Graphical representation for the standard\\nLDS formulation and the corresponding two-view\\nmodel. The two-view formulation is obtained by a linear transformation of the LDS model. The LDS model\\nincludes only parameters C and A and the two-view\\nmodel includes parameters C and E = CA, where A\\ncan be extracted from E after C and E are estimated.\\n\\nx3\\n\\nTwo-view Formulation of LDS\\n\\nIn this section, we reformulate the LDS as a generative two-view model with a shared latent factor. In\\nthe following section, we demonstrate how to estimate the parameters of this reformulation optimally,\\nfrom which parameter estimates of the original LDS can be recovered.\\nTo obtain a two-view formulation, we re-express the two equations for the LDS as two equations for\\npairs of sequential observations. To do so, we multiply the state evolution equation in (1) by C and\\nadd \\x0ft+1 to obtain C?t+1 + \\x0ft+1 = CA?t + C?t + \\x0ft+1 ; representing the LDS model as\\nxt+1 = E?t + \\x0f0t+1\\nxt = C?t + \\x0ft\\n\\n(2)\\n\\nwhere we refer to E := CA as the factor loading matrix and \\x0f0t+1 := C?t + \\x0ft+1 as the noise of\\nthe second view. We then have a two-view problem where we need to estimate parameters E and C.\\nSince the noise components \\x0ft and \\x0f0t are independent, the two views xt and xt+1 are conditionally\\nindependent given the shared latent state ?t . The maximum log likelihood problem for the two-view\\nformulation then becomes\\nmax log p(x1 , . . . , xT |?0 , ?1 , . . . , ?T , C, E)\\n\\n= max\\n\\nC,E,?\\n\\nC,E,?\\n\\nT\\nX\\n\\nlog p(xt |?t?1 , ?t , C, E)\\n\\n(3)\\n\\nt=1\\n\\nwhere, given the hidden states, the observations are conditionally independent. The log-likelihood (3)\\nis equivalent to the original LDS, but is expressed in terms of the distribution p(xt |?t?1 , ?t , C, E),\\nwhere the probability of an observation increases if it has high probability under both ?t?1 and ?t .\\nThe graphical depiction of the LDS and its implied two-view model is illustrated in Figure 1.\\n3.1\\n\\nRelaxation\\n\\nTo tackle the estimation problem, we reformulate the estimation problem for this equivalent two-view\\nmodel of the LDS. Note that according to the two-view model (2), the conditional distribution (3) can\\nbe expressed as p(xt |?t?1 , ?t , C, E) = p(xt |E?t?1 ) = p(xt |C?t ). Substituting each of these in\\nthe summation (3) would result in a factor loading model that ignores the temporal correlation among\\ndata; therefore, to take the system dynamics into account we choose a balanced averaging of both\\nas log p(xt |?t?1 , ?t , C, E) = 12 log p(xt |E?t?1 ) + 21 log p(xt |C?t ), where the likelihood of an\\nobservation increases if it has high conditional likelihood given both ?t?1 and ?t .1 With this choice\\nand the exponential family specified by the log-normalizer (also called potential function) F : Rd ?\\nR, with the corresponding Bregman divergence defined as DF (?\\nzkz) := F (?\\nz) ? F (z) ? f (z)> (?\\nz ? z)\\n2\\nusing transfer function f = ?F , the log-likelihood separates into the two components\\nargmax\\nC,E,?\\n\\nT\\nX\\nt=1\\n\\nlog p(xt |?t?1 , ?t , C, E) = argmax 21\\nC,E,?\\n\\n= argmin\\n\\nT\\nX\\n\\nlog p(xt |E?t?1 ) + log p(xt |C?t )\\n\\nt=1\\nT\\nX\\n\\nDF (E?t?1 ||f\\n\\n?1\\n\\n(xt )) + DF (C?t ||f\\n\\n?1\\n\\n(xt ))\\n\\nC,E,? t=1\\n1\\nThe balanced averaging can be generalized to a convex combination of the log-likelihood which adds a\\nflexibility to the problem that can be tuned to improve performance. However, we found that the simple balanced\\ncombination renders the best experimental performance in most cases.\\n2\\nConsult Banerjee et al. (2005) for a complete overview of this correspondence.\\n\\n3\\n\\n\\x0cEach Bregman divergence term can be interpreted as the fitness measure for each view. For example,\\na Gaussian distribution can be expressed by an exponential family defined by F (z) = 21 kzk22 . The\\nabove derivation could be extended to different variance terms for \\x0f and \\x0f0 , which would result in\\ndifferent weights on the two Bregman divergences above. Further, we could also allow different\\nexponential families (hence different Bregman divergences) for the two distributions; however, there\\nis no clear reason why this would be beneficial over simply selecting the same exponential family,\\nsince both describe xt . In this work, therefore, we will explore a balanced loss, with the same\\nexponential family for each view.\\nIn order to obtain a low rank solution, one can relax the hard rank constraint and employ the block\\nPk\\nnorm k?k2,1 = j=1 k?j: k2 as the rank-reducing regularizer on the latent state.3 This regularizer\\noffers an adaptive rank reducing scheme that zeros out many of the rows of the latent states and\\nhence results a low rank solution without knowing the rank a priori. For the reconstruction models\\nC and E, we need to specify a prior that respects the conditional independence of the views xt and\\nxt+1 given ?t . This goal can be achieved if C and E are constrained individually so that they do not\\ncompete against each other to reconstruct their respective views (White et al., 2012). Incorporating\\nthe regularizer and constraints, the resulting optimization problem has the form\\nargmin\\n\\nT\\nX\\n\\nL1 (E?t?1 ; xt ) + L2 (C?t ; xt ) + ?\\n\\nC,E,? t=1\\n\\nk\\nX\\n\\nk?j: k2\\n\\n(4)\\n\\nj=1\\n\\ns.t.kC:j k2 ? ?1 , kE:j k2 ? ?2 ?j ? (1, k).\\nThe above constrained optimization problem is convex in each of the factor loading matrices {C, E}\\nand the state matrix ?, but not jointly convex in terms of all these variables. Nevertheless, the\\nfollowing lemma show that (4) admits a convex reformulation by change of variable.\\n\\x14 (1) \\x15\\n?\\n? (1) := C? and Z\\n? (2) := E? with their concatenated matrix Z\\n? := Z\\nLemma 1 Let Z\\n? (2) and\\nZ\\n\\x14 \\x15\\n\\x14 \\x15\\n1\\n0\\n(1)\\n(2)\\n(1)\\n(2)\\nZ := [x1:T ?1 ], Z := [x2:T ]. In addition, let?s define I := diag(\\n), I := diag(\\n),\\n0\\n1\\nthen the multi-view optimization problem (4) can be reformulated in the following convex form\\nmin\\nkC:j k2 ??1\\nkE:j k2 ??2\\n\\n\"min\\n#\\nC\\n?\\n?:\\n?=Z\\nE\\n\\nL1 (C?; Z(1) ) + L2 (E?; Z(2) ) + ?k?k2,1\\n\\n? (1) ; Z(1) ) + L2 (Z\\n? (2) ; Z(2) ) + ? max kU?1 Zk\\n? tr\\n= min L1 (Z\\n?\\n0???1\\n\\n?\\nZ\\n\\nwhere U? =\\n\\n? = PT Li (yt ; y?t ). Moreover, we can show\\n??2 I(2) and Li (Y; Y)\\nt=1\\n1??\\n? tr is concave in ?. The trace norm induces a low rank result.\\nkU?1\\nZk\\n?\\n\\n?\\n?1 I(1)\\n?\\n\\nthe regularizer term\\n\\n+\\n\\nProof: The proof can be readily derived from the results of White et al. (2012).\\n\\nthat\\n\\n\\x04\\n\\nIn the next section, we demonstrate how to obtain globally optimal estimates of E, C and ?.\\nRemark 1: This maximum likelihood formulation demonstrates how the distributional assumptions\\non the observations xt can be generalized to any exponential family. Once expressed as the above\\noptimization problem, one can further consider other losses and regularizers that may not immediately\\nhave a distributional interpretation, but result in improved prediction performance. This generalized\\nformulation of maximum likelihood for LDS, therefore, has the additional benefit that it can flexibly\\nincorporate optimization improvements, such as robust losses.4 Also a regularizer can be designed to\\ncontrol overfitting to noisy observation, which is an issue in LDS that can result in an unstable latent\\ndynamics estimate (Buesing et al., 2012a). Therefore, by controlling undesired overfitting to noisy\\nsamples one can also prevent unintended unstable model identification.\\n3\\n\\nThroughout this paper, Xi: (X:i ) is used to denote the ith row (ith column) of matrix X and also [X; Y]\\n([x; y]) denotes the matrix (vector) concatenation operator which is equal to [X> , Y> ]> ([x> , y> ]> ).\\n4\\nThus, we used L1 and L2 in (4) to generally refer to any loss function that is convex in its first argument.\\n\\n4\\n\\n\\x0cRemark 2: We can generalize the optimization further to learn an LDS with exogenous input: a\\ncontrol vector ut ? Rd that impacts both the hidden state and observations. This entails adding some\\nnew variables to the general LDS model that can be expressed as\\n?t+1 = A?t + But + ?t\\nxt = C?t + Dut + \\x0ft\\nwith additional matrices B ? Rk?d and D ? Rd?d . Again by multiplying the state evolution\\nequation by matrix C the resulting equations are\\nxt+1 = E?t + Fut + Dut+1 + \\x0f0t+1\\nxt = C?t + Dut + \\x0ft\\nwhere F := CB. Therefore, the loss can be generally expressed as\\nL1 (E?t?1 + Fut?1 + Dut ; xt ) + L2 (C?t + Dut ; xt ).\\nThe optimization would now be over the variables C, E, ?, D, F, where the optimization could\\nadditionally include regularizers on D and F to control overfitting. Importantly, the addition of these\\nvariables D, F does not modify the convexity properties of the loss, and the treatment for estimating\\nE, C and ? in section 4 directly applies. The optimization problem is jointly convex in D, F and\\nany one of E, C or ? and jointly convex in D and F. Therefore, an outer minimization over D and\\nF can be added to Algorithm 1 and we will still obtain a globally optimal solution.\\n\\n4\\n\\nLDS Estimation Algorithm\\n\\nTo learn the optimal parameters for the reformulated two-view model, we adopt the generalized conditional gradient (GCG) algorithm developed by Yu et al. (2014). GCG is designed for optimization\\nproblems of the form l(x) + f (x) where l(x) is convex and continuously differentiable with Lipschitz\\ncontinuous gradient and f (x) is a (possibly non-differentiable) convex function. The algorithm is\\ncomputationally efficient, as well providing a reasonably fast O(1/t) rate of convergence to the global\\nminimizer. Though we have a nonconvex optimization problem, we can use the convex reformulation\\nfor two-view low-rank matrix factorization and resulting algorithm in (Yu et al., 2014, Section\\n4). This algorithm includes a generic local improvement step, which significantly accelerates the\\nconvergence of the algorithm to a global optimum in practice. We provide a novel local improvement\\nupdate, which both speeds learning and enforces a sparser structure on ?, while maintaining the\\nsame theoretical convergence properties of GCG.\\nIn our experiments, we specifically address the setting when the observations are assumed to be\\nGaussian, giving an `2 loss. We also prefer the unconstrained objective function that can be efficiently\\nminimized by fast unconstrained optimization algorithms. Therefore, using the well-established\\nequivalent form of the regularizer (Bach et al., 2008), the objective (4) can be equivalently cast for\\nthe Gaussian distributed time series xt as\\nmin\\n\\nC,E,?\\n\\nT\\nX\\n\\nkE?t?1 ? xt k22 + kC?t ? xt k22 + ?\\n\\nt=1\\n\\nk\\nX\\n\\nk?j: k2 max( ?11 kC:j k2 , ?12 kE:j k2 ).\\n\\n(5)\\n\\nj=1\\n\\nThis product form of the regularizer is also preferred over the square form used in (Yu et al., 2014),\\nsince it induces row-wise sparsity on ?. Though the square form k?k2F admits efficient optimizers\\ndue to its smoothness, it does not prefer to zero out rows of ? while with the regularizer of the form\\n(5), the learned hidden state will be appropriately projected down to a lower-dimensional space where\\nmany dimensions could be dropped from ?, C and E giving a low rank solution. In practice, we\\nfound that enforcing this sparsity property on ? significantly improved stability.5 Consequently, we\\nneed optimization routines that are appropriate for the non smooth regularizer terms.\\nThe local improvement step involves alternating block coordinate descent between C, E and ?, with\\nan accelerated proximal gradient algorithm (FISTA) (Beck and Teboulle, 2009) for each descent step.\\nTo use the FISTA algorithm we need to provide a proximal operator for the non-smooth regularizer\\nin (5).\\n5\\n\\nThis was likely due to a reduction in the size of the transition parameters, resulting in improved re-estimation\\nof A and a corresponding reduction in error accumulation when using the model for forecasting.\\n\\n5\\n\\n\\x0cAlgorithm 1 LDS-DV\\nInput: training sequence {xt , t ? [1, T ]}\\nOutput: C, A, ?t , ?? , ?\\x0f\\nInitialize C0 , E0 , ?0\\n> >\\nU1 ? [C>\\nV1 ? ?>\\n0 ; E0 ] ,\\n0\\nfor i = 1, . . . do\\n\\n\\n\\x0b\\n(ui , vi ) ? arg minuv> ?A ?`(Ui , Vi ), uv> // compute polar\\n(?i , ?i ) ? arg min `((1 ? ?)Ui Vi> + ?ui vi> ) + ?((1 ? ?)?i + ?) // partially corrective up0???1,??0\\n\\ndate (PCU) ?\\n?\\n?\\n?\\nUinit ? [ 1 ? ?i Ui , ?i ui ], Vinit ? [ 1 ? ?i Vi , ?i vi ]\\n(Ui+1 , Vi+1 ) ? FISTA(Uinit Vinit )\\nP\\n2\\n2\\n?i = 12 i+1\\nj=1 (k(Ui+1 ):i k2v + k(Vi+1 ):i k2 )\\nend for\\n>\\n(C; E) ? Ui+1 , ? ? Vi+1\\n?\\nA ? ?2:T ? ?1:T ?1\\nestimate ?? , ?\\x0f by sample covariances\\n\\nLet the proximal operator of a convex and possibly non-differentiable function ?f (y) be defined as\\nprox?f (x) = arg min ?f (y) + 21 kx ? yk22 .\\ny\\n\\nFISTA is an accelerated version of ISTA (Iterative Shrinkage-Thresholding Algorithm) that iteratively performs a gradient descent update with the smooth component of the objective, and\\nthen applies the proximal operator\\x01 as a projection step. Each iteration updates the variable x as\\nxk+1 = prox?k f xk ? ?k ?l(xk ) , which converges to a fixed point. If there is no known form for\\nthe proximal operator, as is the case for our non-differentiable regularizer, a common strategy is to\\nnumerically calculate the proximal update. This approach, however, can be prohibitively expensive,\\nand an analytic (closed) form is clearly preferable. We derive such a closed form for (5) in Theorem 1.\\nh i\\nTheorem 1 For a vector v = vv12 composed of two subvectors v1 , v2 , define f (v) = ?kvk2v :=\\n? max(kv1 k2 , kv2 k2 ). The proximal operator for this function is\\n#\\n?\"\\nv1 max{1 ? kv?1 k , 0}\\n?\\n?\\n?\\nif kv1 k ? kv2 k\\n?\\n? v2 max{1 ? ??? , 0}\\nkv2 k\\n#\\nproxf (v) = \"\\n???\\n?\\nv1 max{1 ? kv\\n,\\n0}\\n?\\nk\\n?\\n1\\n?\\nif kv2 k ? kv1 k\\n? v max{1 ? ? , 0}\\n2\\nkv2 k\\nwhere ? := max{.5(kv1 k ? kv2 k + ?), 0} and ? := max{.5(kv2 k ? kv1 k + ?), 0}.\\n\\x04\\n\\nProof: See Appendix A.\\n\\nThis result can be further generalized to enable additional regularization components on C and E,\\nsuch as including an `1 norm on each column to further enforce sparsity (such as in the elastic net).\\nThere is no closed form for the proximal operator of the sum of two functions in general. We prove,\\nhowever, that for special case of a linear combination of the two-view norm with any norms on the\\ncolumns of C and E, the proximal mapping reduces to a simple composition rule.\\nTheorem 2 For norms R1 (v1 ) and R2 (v2 ), the proximal operator of the linear combination\\nRc (v) = ?kvk2v + ?\\x12\\x14\\n1 R1 (v1 ) + ?2 R\\x15\\x13\\n2 (v2 ) for ?1 , ?2 ? 0 admits the simple composition\\nprox?1 R1 (v1 )\\nproxRc (v) = prox?k.k2v\\n.\\nprox?2 R2 (v2 )\\n\\x04\\n\\nProof: See Appendix A.\\n4.1\\n\\nRecovery of the LDS model parameters\\n\\nThe above reformulation provides a tractable learning approach to obtain the optimal parameters for\\nthe two-view reformulation of LDS; given this optimal solution, we can then estimate the parameters\\n6\\n\\n\\x0cto the original LDS. The first step is to estimate the transition matrix A. A natural approach is to\\n? =C\\n? ?E\\n? for pseudoinverse C\\n? ? . This A,\\n? however, might be sensitive to inaccurate\\nuse (2), and set A\\nestimation of the (effective) hidden state dimension k. We found in practice that modifications from\\nthe optimal choice of k might result in unstable solutions and produce unreliable forecasts. Instead,\\n? can be learned from the hidden states themselves. This approach also focuses\\na more stable A\\nestimation of A on the forecasting task, which is our ultimate aim.\\nGiven the sequence of hidden states, ?1 , . . . , ?T , there are several strategies that could be used to\\nestimate A, including simple autoregressive models to more sophisticated strategies (Siddiqi et al.,\\n? = arg minA PT ?1 k?t+1 ? A?t k2 which\\n2007). We opt for a simple linear regression solution A\\n2\\nt=1\\n?\\nwe found produced stable A.\\n? t , \\x0ft = xt ? C?t . Having obtained\\nTo estimate the noise parameters ?? , ?\\x0f , recall ?t = ?t+1 ? A?\\n?\\nA, therefore, we can estimate the noise covariance matrices by computing their sample covariances\\nPT\\nPT\\n1\\n> ?\\n>\\n?? = 1\\nas ?\\nt=1 ?t ?t , ?\\x0f = T ?1\\nt=1 \\x0ft \\x0ft . The final LDS learning procedure is outlined in\\nT ?1\\nAlgorithm 1. For more details about polar computation and partially corrective subroutine see (Yu\\net al., 2014, Section 4).\\n\\n5\\n\\nExperimental results\\n\\nWe evaluate the proposed algorithm by comparing one step prediction performance and computation\\nspeed with alternative methods for real and synthetic time series. We report the normalized mean\\nPTtest\\nPTtest\\nkyt ??\\nyt k2\\n1\\nsquare error (NMSE) defined as NMSE = PTt=1\\nwhere ?y = Ttest\\ntest\\nt=1 yt .\\n2\\nt=1\\n\\nkyt ??y k\\n\\nAlgorithms: We compared the proposed algorithm to a well-established method-of moment-based\\nalgorithm, N4SID (Van Overschee and De Moor, 1994), Hilbert space embeddings of hidden Markov\\nmodels (HSE-HMM) (Song et al., 2010), expectation-maximization for estimating the parameters of\\na Kalman filter (EM) (Roweis and Ghahramani, 1999) and PEM (Ljung, 1999). These are standard\\nbaseline algorithms that are used regularly for LDS identification. The estimated parameters by\\nN4SID were used as the initialization point for EM and PEM algorithms in our experiments. We used\\nthe built-in functions, n4sid and pem, in Matlab, with the order selected by the function, for the\\nsubspace identification method and PEM, respectively. For our algorithm, we select the regularization\\nparameter ? using cross-validation. For the time series, the training data is split by performing the\\nlearning on first 80% of the training data and evaluating the prediction performance on the remaining\\n20%.\\nReal datasets: For experiments on real datasets we select the climate time series from IRI data\\nlibrary that recorded the surface temperature on the monthly basis for tropical Atlantic ocean (ATL)\\nand tropical Pacific ocean (CAC). In CAC we selected first 30 ? 30 grids out of the total 84 ? 30\\nlocations with 399 monthly samples, while in ATL the first 9 ? 9 grids out of the total 38 ? 25\\nlocations are selected each with timeseries of length 564. We partitioned each area to smaller areas\\nof size 3 ? 3 and arrange them to vectors of size 9, then seasonality component of the time series are\\nremoved and data is centered to have zero mean. We ran two experiments for each dataset. For the\\nfirst, the whole sequence is sliced into 70% training and 30% test. For the second, a short training set\\nof 70 samples is selected, with a test sequence of size 50.\\nSynthetic datasets: In the synthetic experiments, the datasets are generated by an LDS model (1) of\\ndifferent system orders, k, and observation sizes, d. For each test case, 100 data sequences of length\\n200 samples are generated and sliced to 70%, 30% ratios for training set and test set, respectively. The\\ndynamics matrix A is selected to produce a stable system: {|?i (A)| = s : s ? 1, ?i ? (1, k)} where\\n?i (A) is the ith eigen value of matrix A. The noise components are drawn from Gaussian distributions\\nand scaled so that p? := E{? > ?}/m and p\\x0f := E{\\x0f> \\x0f}/n. Each test is repeated with the following\\nsettings: {S1: s = 0.970, p? = 0.50 and p\\x0f = 0.1}, {S2: s = 0.999, p? = 0.01 and p\\x0f = 0.1}.\\nResults: The NMSE and run-time results obtained on real and synthetic datasets are shown in Table\\n1 and Table 2, respectively. In terms of NMSE, LDS-DV outperforms and matches the alternative\\nmethods. In terms of algorithm speed, the LDS-DV learns the model much faster than the competitors\\nand scales well to larger dimension models. The speed improvement is more significant for larger\\ndatasets and observations with higher dimensions.\\n7\\n\\n\\x0cLDS-MV\\nN4SID\\nEM\\nHSE-HMM\\nPEM-SSID\\n\\nATL(Long)\\nNMSE\\nTime\\n0.45?0.03\\n0.26\\n0.52?0.04\\n2.34\\n0.64?0.04\\n7.87\\n675.87?629.46\\n0.79\\n0.71?0.08\\n20.00\\n\\nTable 1: Real time series\\nATL(Short)\\nCAC(Long)\\nNMSE\\nTime\\nNMSE\\nTime\\n0.54?0.05\\n0.22\\n0.58?0.02\\n0.28\\n0.59?0.05\\n0.95\\n0.61?0.02\\n1.23\\n0.88?0.07\\n3.92\\n0.81?0.02\\n5.70\\n0.97?0.01\\n0.16\\n11.24?8.23\\n0.39\\n1.52?0.66 16.38\\n1.38?0.15\\n19.67\\n\\nCAC(Short)\\nNMSE\\nTime\\n0.63?0.03\\n0.14\\n0.84?0.07\\n1.08\\n1.02?0.08\\n4.12\\n2.82?1.60\\n0.17\\n2.68?0.78 20.58\\n\\nTable 2: Synthetic time series\\n(S1) d=5 , k=3\\n\\nNMSE\\nLDS-MV\\n0.12?0.01\\nN4SID\\n0.12?0.01\\nEM\\n0.18?0.01\\nHSE-HMM 2.4e+4?1.7e+4\\nPEM-SSID 0.14?0.01\\n\\n(S2) d=5 , k=3\\n\\nTime\\nNMSE\\n0.49 0.17?0.02\\n0.81 0.42?0.04\\n4.99 0.15?0.02\\n0.48 2.2e+7?2.2e+7\\n10.72 0.25?0.03\\n\\n(S1) d=8 , k=6\\n\\nTime\\nNMSE\\n0.36\\n0.08?0.00\\n0.76\\n0.11?0.00\\n4.62\\n0.14?0.01\\n0.50 7.8e+03?7.7e+03\\n9.08\\n0.12?0.01\\n\\nTime\\n0.66\\n1.45\\n6.01\\n0.49\\n15.22\\n\\n(S2) d=8 , k=6\\n\\nNMSE\\n0.04?0.00\\n0.39?0.04\\n0.04?0.00\\n0.65?0.02\\n0.08?0.01\\n\\n(S1) d=16 , k=9\\n\\nTime\\n0.52\\n1.38\\n5.03\\n0.55\\n13.97\\n\\nNMSE\\n0.07?0.00\\n0.10?0.00\\n0.13?0.00\\n22.92?21.83\\n0.09?0.01\\n\\nTime\\n1.01\\n4.29\\n19.21\\n0.53\\n38.39\\n\\n(S2) d=16 , k=9\\n\\nNMSE\\n0.03?0.00\\n0.42?0.04\\n0.03?0.00\\n0.71?0.01\\n0.06?0.02\\n\\nTime\\n1.72\\n4.40\\n19.83\\n0.61\\n41.10\\n\\n1\\n\\n10\\nSeconds\\n\\nNMSE\\n\\n12\\n\\nLDS-DV\\nN4SID\\nEM\\n\\n1.2\\n\\n0.8\\n\\n8\\n\\nPrediction MSE of LDS-MV\\n\\nResults for real and synthetic datasets are listed in Table 1 and Table 2, respectively. The first column of each\\ndataset is the average normalized MSE with standard error and the second column is the algorithm runtime in\\nCPU seconds. The best NMSE according to pairwise t-test with significance level of 5% is highlighted.\\nLDS-DV\\nN4SID\\nEM\\nHSE-HMM\\n\\n6\\n4\\n2\\n\\n0.6\\n100\\n\\n200\\n300\\n400\\n500\\n600\\nTraining Sequence Length (T)\\n\\n(a) NMSE\\n\\n0\\n100\\n\\n200\\n300\\n400\\n500\\nTraining Sequence Length(T)\\n\\n(b) Runtime\\n\\n600\\n\\n3.5\\n3\\n2.5\\n2\\n1.5\\n1\\n0.5\\n0.5\\n\\n1\\n1.5\\n2\\n2.5\\n3\\nPrediction MSE of n4SID\\n\\n3.5\\n\\n(c) Scatter plot of MSE\\n\\nFigure 2: a) NMSE of the LDS-DV for increasing length of training sequence. The difference between LDS-DV\\nand N4SID is more significant in shorter training length, while both converge to the same accuracy in large\\nT . HSE-HMM is omitted due to its high error. b) Runtime in CPU seconds for increasing length of training\\nsequence. LDS-DV scales well with large sample length. c) MSE of the LDS-DV versus MSE of N4SID. In\\nhigher values of MSE, the points are below identity function line and LDS-DV is more likely to win.\\n\\nFor test cases with |?i (A)| \\' 1, designed to evaluate the prediction performance of the methods for\\nmarginally stable systems, LDS-DV still can learn a stable model while the other algorithms might\\nnot learn a stable model. The proposed LDS-DV method does not explicitly impose stability, but the\\nregularization favors A that is stable. The regularizer on latent state encourages smooth dynamics\\nand controls overfitting: overfitting to noisy observations can lead to unstable estimate of the model\\n(Buesing et al., 2012a), and a smooth latent trajectory is a favorable property in most real-world\\napplications.\\nFigure 2(c) shows the MSE of LDS-DV versus N4SID, for all the CAC time-series. This figure\\nillustrates that for easier problems, LDS-DV and N4SID are more comparable. However, as the\\ndifficulty increase, and MSE increases, LDS-DV begins to consistently outperform N4SID.\\nFigures 2(a) and 2(b) illustrate the accuracy and runtime respectively of the algorithms versus training\\nlength. We used the synthetic LDS model under condition S1 with n = 8, m = 6. Values are\\naveraged over 20 runs with a test length of 50 samples. LDS-DV has better early performance, for\\nsmaller sample sizes. At larger sample sizes, they reach approximately the same error level.\\n\\n6\\n\\nConclusion\\n\\nIn this paper, we provided an algorithm for optimal estimation of the parameters for a time-invariant,\\ndiscrete-time linear dynamical system. More precisely, we provided a reformulation of the model as a\\ntwo-view objective, which allowed recent advances for optimal estimation for two-view models to be\\napplied. The resulting algorithm is simple to use and flexibly allows different losses and regularizers\\n8\\n\\n\\x0cto be incorporated. Despite this simplicity, significant improvements were observed over a widely\\naccepted method for subspace identification (N4SID), both in terms of accuracy for forecasting and\\nruntime.\\nThe focus in this work was on forecasting, therefore on optimal estimation of the hidden states and\\ntransition matrices; however, in some settings, estimation of noise parameters for LDS models is\\nalso desired. An unresolved issue is joint optimal estimation of these noise parameters. Though\\nwe do explicitly estimate the noise parameters, we do so only from the residuals after obtaining the\\noptimal hidden states and transition and observation matrices. Moreover, consistency of the learned\\nparameters by the proposed procedure of this paper is still an open problem and will be an interesting\\nfuture work.\\nThe proposed optimization approach for LDSs should be useful for applications where alternative\\nnoise assumptions are desired. A Laplace assumption on the observations, for example, provides a\\nmore robust `1 loss. A Poisson distribution has been advocated for count data, such as for neural\\nactivity, where the time series is a vector of small integers (Buesing et al., 2012b). The proposed\\nformulation of estimation for LDSs easily enables extension to such distributions. An important next\\nstep is to investigate the applicability to a wider range of time series data.\\nAcknowledgments\\nThis work was supported in part by the Alberta Machine Intelligence Institute and NSERC. During\\nthis work, M. White was with the Department of Computer Science, Indiana University.\\n\\nReferences\\nAndersson, S. (2009). Subspace estimation and prediction methods for hidden Markov models. The\\nAnnals of Statistics.\\n?str?m, K. (1980). Maximum likelihood and prediction error methods. Automatica, 16(5):551?574.\\nBach, F., Mairal, J., and Ponce, J. (2008). Convex sparse matrix factorizations. arXiv:0812.1869v1.\\nBanerjee, A., Merugu, S., Dhillon, I., and Ghosh, J. (2005). Clustering with Bregman divergences.\\nJournal of Machine Learning Research.\\nBeck, A. and Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding Algorithm for Linear\\nInverse Problems. SIAM Journal on Imaging Sciences, 2.\\nBoots, B. and Gordon, G. (2012). Two-manifold problems with applications to nonlinear system\\nidentification. In International Conference on Machine Learning.\\nBoyd, S. and Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press.\\nBuesing, L., Macke, J., and Sahani, M. (2012a). Learning stable, regularised latent models of neural\\npopulation dynamics. Network: Computation in Neural Systems.\\nBuesing, L., Macke, J., and Sahani, M. (2012b). Spectral learning of linear dynamics from generalisedlinear observations with application to neural population data. In Advances in Neural Information\\nProcessing Systems.\\nCram?r, H. (1946). Mathematical Methods of Statistics. Princeton University Press.\\nFoster, D., Rodu, J., and Ungar, L. (2012).\\narXiv:1203.6130v1.\\n\\nSpectral dimensionality reduction for HMMs.\\n\\nGelfand, A., Diggle, P., Guttorp, P., and Fuentes, M. (2010). Handbook of Spatial Statistics. CRC\\nPress.\\nGhahramani, Z. and Hinton, G. (1996). Parameter estimation for linear dynamical systems. Technical\\nreport.\\nHaeffele, B., Young, E., and Vidal, R. (2014). Structured Low-Rank Matrix Factorization: Optimality,\\nAlgorithm, and Applications to Image Processing. In International Conference on Machine\\nLearning.\\n9\\n\\n\\x0cHsu, D., Kakade, S., and Zhang, T. (2012). A spectral algorithm for learning Hidden Markov Models.\\nJournal of Computer and System Sciences.\\nKatayama, T. (2006). Subspace Methods for System Identification. Springer.\\nLjung, L. (1999). System Identification (2Nd Ed.): Theory for the User. Prentice Hall PTR.\\nMacke, J., Buesing, L., and Sahani, M. (2015). Estimating State and Model Parameters in State-Space\\nModels of Spike Trains. Advanced State Space Methods for Neural and Clinical Data.\\nMoonen, M. and Ramos, J. (1993). A subspace algorithm for balanced state space system identification. IEEE Transactions on Automatic Control.\\nParikh, N. and Boyd, S. (2013). Proximal Algorithms. Foundations and Trends in Optimization. Now\\nPublishers.\\nRoweis, S. and Ghahramani, Z. (1999). A unifying review of linear Gaussian models. Neural\\nComputation.\\nSiddiqi, S., Boots, B., and Gordon, G. (2007). A Constraint Generation Approach to Learning Stable\\nLinear Dynamical Systems. In Advances in Neural Information Processing Systems.\\nSong, L., Boots, B., Siddiqi, S., Gordon, G., and Smola, A. (2010). Hilbert space embeddings of\\nhidden Markov models. In International Conference on Machine Learning.\\nVan Overschee, P. and De Moor, B. (1994). N4SID: Subspace algorithms for the identification of\\ncombined deterministic-stochastic systems. Automatica.\\nViberg, M. (1995). Subspace-based methods for the identification of linear time-invariant systems.\\nAutomatica.\\nWhite, M., Wen, J., Bowling, M., and Schuurmans, D. (2015). Optimal estimation of multivariate\\nARMA models. In AAAI Conference on Artificial Intelligence.\\nWhite, M., Yu, Y., Zhang, X., and Schuurmans, D. (2012). Convex multi-view subspace learning. In\\nAdvances in Neural Information Processing Systems.\\nYu, H., Rao, N., and Dhillon, I. (2015). High-dimensional Time Series Prediction with Missing\\nValues. arXiv:1509.08333.\\nYu, Y., Zhang, X., and Schuurmans, D. (2014). Generalized Conditional Gradient for Sparse\\nEstimation. arXiv:1410.4828.\\nZhang, X., Yu, Y., and Schuurmans, D. (2012). Accelerated training for matrix-norm regularization:\\nA boosting approach. In Advances in Neural Information Processing Systems.\\nZhao, H. and Poupart, P. (2014). A sober look at spectral learning. arXiv:1406.4631.\\n\\n10\\n\\n\\x0c'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.paper_text.unique()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=data.paper_text.iloc[1]\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "6a7sjNaqbYsk",
        "outputId": "a5d8ba95-c2a6-4151-941a-246222329dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISUAL CORTEX\\nAND ITS APPLICATION TO ARTIFICIAL NEURAL NETWORKS*\\nChristopher L. Scofield\\nCenter for Neural Science and Physics Department\\nBrown University\\nProvidence, Rhode Island 02912\\nand\\nNestor, Inc., 1 Richmond Square, Providence, Rhode Island,\\n02906.\\nABSTRACT\\nA single cell theory for the development of selectivity and\\nocular dominance in visual cortex has been presented previously\\nby Bienenstock, Cooper and Munrol. This has been extended to a\\nnetwork applicable to layer IV of visual cortex 2 . In this paper\\nwe present a mean field approximation that captures in a fairly\\ntransparent manner the qualitative, and many of the\\nquantitative, results of the network theory. Finally, we consider\\nthe application of this theory to artificial neural networks and\\nshow that a significant reduction in architectural complexity is\\npossible.\\nA SINGLE LAYER NETWORK AND THE MEAN FIELD\\nAPPROXIMATION\\nWe consider a single layer network of ideal neurons which\\nreceive signals from outside of the layer and from cells within\\nthe layer (Figure 1). The activity of the ith cell in the network is\\nc\\'1 -- m\\'1 d + \"\"\\'\\n~ T .. c\\'\\n~J J\\'\\n\\nJ\\n\\n(1)\\n\\nEach cell\\nd is a vector of afferent signals to the network.\\nreceives input from n fibers outside of the cortical network\\nthrough the matrix of synapses mi\\' Intra-layer input to each cell\\nis then transmitted through the matrix of cortico-cortical\\nsynapses L.\\n? American Institute of Physics 1988\\n\\n\\x0c684\\n\\nAfferent\\nSignals\\n\\n>\\n\\n... ..\\n\\nm2\\n\\nm1\\n\\nmn\\n\\n~\\n\\nr;.\\n\\n\",...-\\n\\nd\\n\\n.L\\n:\\n\\n1\\n\\n,~\\n\\n2\\n\\n... ..\\n\\n, ...c.. ,\\n\\n~\\n\\n~\\n\\nFigure 1: The general single layer recurrent\\nnetwork.\\nLight circles are the LGN -cortical\\nsynapses.\\nDark circles are the (nonmodifiable) cortico-cortical synapses.\\nWe now expand the response of the i th cell into individual\\nterms describing the number of cortical synapses traversed by\\nthe signal d before arriving through synapses Lij at cell i.\\nExpanding Cj in (1), the response of cell i becomes\\nci\\n\\n=mi d + l: ~j mj d + l: ~jL Ljk mk d + 2: ~j 2Ljk L Lkn mn d +... (2)\\nJ\\n\\nJ\\n\\nK\\n\\nJ\\n\\nK\\' n\\n\\nNote that each term contains a factor of the form\\n\\nThis factor describes the first order effect, on cell q, of the\\ncortical transformation of the signal d.\\nThe mean field\\napproximation consists of estimating this factor to be a constant,\\nindependant of cell location\\n(3)\\n\\n\\x0c685\\n\\nThis assumption does not imply that each cell in the network is\\nselective to the same pattern, (and thus that mi = mj). Rather,\\nthe assumption is that the vector sum is a constant\\n\\nThis amounts to assuming that each cell in the network is\\nsurrounded by a population of cells which represent, on average,\\nall possible pattern preferences.\\nThus the vector sum of the\\nafferent synaptic states describing these pattern preferences is a\\nconstant independent of location.\\nFinally, if we assume that the lateral connection strengths are\\na function only of i-j then Lij becomes a circular matrix so that\\n\\nr. Lij ::: ~J Lji = Lo = constan t.\\n1\\n\\nThen the response of the cell i becomes\\n(4)\\n\\nfor I\\n\\n~\\n\\nI <1\\n\\nwhere we define the spatial average of cortical cell activity C = in\\nd, and N is the average number of intracortical synapses.\\nHere, in a manner similar to that in the theory of magnetism,\\nwe have replaced the effect of individual cortical cells by their\\naverage effect (as though all other cortical cells can be replaced\\nby an \\'effective\\' cell, figure 2). Note that we have retained all\\norders of synaptic traversal of the signal d.\\nThus, we now focus on the activity of the layer after\\n\\'relaxation\\' to equilibrium. In the mean field approximation we\\ncan therefore write\\n(5)\\n\\nwhere the mean field\\n\\na\\nwith\\n\\n=am\\n\\n\\x0c686\\n\\nand we asume that\\ninhibitory).\\n\\nAfferent\\nSignals\\nd\\n\\nLo < 0 (the network is,\\n\\non\\n\\naverage,\\n\\n>\\n\\nFigure 2: The single layer mean field network.\\nDetailed connectivity between all cells of the\\nnetwork is replaced with a single (nonmodifiable) synapse from an \\'effective\\' cell.\\nLEARNING IN THE CORTICAL NETWORK\\n\\nWe will first consider evolution of the network according to a\\nsynaptic modification rule that has been studied in detail, for\\nsingle cells, elsewhere!? 3.\\nWe consider the LGN - cortical\\nsynapses to be the site of plasticity and assume for maximum\\nsimplicity that there is no modification of cortico-cortical\\nsynapses. Then\\n(6)\\n\\n.\\n\\nLij = O.\\nIn what follows c denotes the spatial average over cortical cells,\\nwhile Cj denotes the time averaged activity of the i th cortical cell.\\nThe function cj> has been discussed extensively elsewhere.\\nHere\\nwe note that cj> describes a function of the cell response that has\\nboth hebbian and anti-hebbian regions.\\n\\n\\x0c687\\n\\nThis leads to a very complex set of non-linear stochastic\\nequations that have been analyzed partially elsewhere 2 . In\\ngeneral, the afferent synaptic state has fixed points that are\\nstable and selective and unstable fixed points that are nonselective!, 2. These arguments may now be generalized for the\\nnetwork. In the mean field approximation\\n(7)\\n\\nThe mean field, a has a time dependent component m. This\\nvaries as the average over all of the network modifiable\\nsynapses and, in most environmental situations, should change\\nslowly compared to the change of the modifiable synapses to a\\nsingle cell. Then in this approximation we can write\\n\\n?\\n\\n(mi(a)-a) = cj>[mi(a) - a] d.\\n\\n(8)\\n\\nWe see that there is a mapping\\nmi\\' <-> mica) - a\\n\\n(9)\\n\\nsuch that for every mj(a) there exists a corresponding (mapped)\\npoint mj\\' which satisfies\\n\\nthe original equation for the mean field zero theory. It can be\\nshown 2, 4 that for every fixed point of mj( a = 0), there exists a\\ncorresponding fixed point mj( a) with the same selectivity and\\nstability properties.\\nThe fixed points are available to the\\nneurons if there is sufficient inhibition in the network (ILo I is\\nsufficiently large).\\nAPPLICATION OF THE MEAN FIELD NETWORK TO\\nLAYER IV OF VISUAL CORTEX\\nNeurons in the primary visual cortex of normal adult cats are\\nsharply tuned for the orientation of an elongated slit of light and\\nmost are activated by stimulation of either eye. Both of these\\nproperties--orientation selectivity and binocularity--depend on\\nthe type of visual environment experienced during a critical\\n\\n\\x0c688\\n\\nperiod of early postnatal development. For example, deprivation\\nof patterned input during this critical period leads to loss of\\norientation selectivity while monocular deprivation (MD) results\\nin a dramatic shift in the ocular dominance of cortical neurons\\nsuch that most will be responsive exclusively to the open eye.\\nThe ocular dominance shift after MD is the best known and most\\nintensively studied type of visual cortical plasticity.\\nThe behavior of visual cortical cells in various rearing\\nconditions suggests that some cells respond more rapidly to\\nenvironmental changes than others.\\nIn monocular deprivation,\\nfor example, some cells remain responsive to the closed eye in\\nspite of the very large shift of most cells to the open eye- Singer\\net. al. 5 found, using intracellular recording, that geniculo-cortical\\nsynapses on inhibitory interneurons are more resistant to\\nmonocular deprivation than are synapses on pyramidal cell\\ndendrites. Recent work suggests that the density of inhibitory\\nGABAergic synapses in kitten striate cortex is also unaffected by\\nMD during the cortical period 6, 7.\\nThese results suggest that some LGN -cortical synapses modify\\nrapidly, while others modify relatively slowly, with slow\\nmodification of some cortico-cortical synapses. Excitatory LGNcortical synapses into excitatory cells may be those that modify\\nprimarily.\\nTo embody these facts we introduce two types of\\nLGN -cortical synapses:\\nthose (mj) that modify and those (Zk)\\nthat remain relatively constant. In a simple limit we have\\n\\nand\\n\\n(10)\\n\\nWe assume for simplicity and consistent with the above\\nphysiological interpretation that these two types of synapses are\\nconfined to two different classes of cells and that both left and\\nright eye have similar synapses (both m i or both Zk) on a given\\ncell. Then, for binocular cells, in the mean field approximation\\n(where binocular terms are in italics)\\n\\n\\x0c689\\n\\nwhere dl(r) are the explicit left (right) eye time averaged signals\\narriving form the LGN.\\nNote that a1(r) contain terms from\\nmodifiable and non-modifiable synapses:\\nal(r) =\\n\\na (ml(r) + zl(r?).\\n\\nUnder conditions of monocular deprivation, the animal is reared\\nwith one eye closed. For the sake of analysis assume that the\\nright eye is closed and that only noise-like signals arrive at\\ncortex from the right eye. Then the environment of the cortical\\ncells is:\\nd = (di, n)\\n\\n(12)\\n\\nFurther, assume that the left eye synapses have reached their\\n1\\n\\nr\\n\\nselective fixed point, selective to pattern d 1 ? Then (mi\\' m i )\\n(m:*, xi) with IXil ?lm!*1.\\nlinear analysis of the\\nthe closed eye\\n\\n<I> -\\n\\n=\\n\\nFollowing the methods of BCM, a local\\nfunction is employed to show that for\\n\\nXi =\\n\\na (1 - }..a)-li.r.\\n\\n(13)\\n\\nwhere A. = NmIN is the ratio of the number modifiable cells to the\\ntotal number of cells in the network. That is, the asymptotic\\nstate of the closed eye synapses is a scaled function of the meanfield due to non-modifiable (inhibitory) cortical cells. The scale\\nof this state is set not only by the proportion of non-modifiable\\ncells, but in addition, by the averaged intracortical synaptic\\nstrength Lo.\\nThus contrasted with the mean field zero theory the deprived\\neye LGN-cortical synapses do not go to zero.\\nRather they\\napproach the constant value dependent on the average inhibition\\nproduced by the non-modifiable cells in such a way that the\\nasymptotic output of the cortical cell is zero (it cannot be driven\\nby the deprived eye). However lessening the effect of inhibitory\\nsynapses (e.g. by application of an inhibitory blocking agent such\\nas bicuculine) reduces the magnitude of a so that one could once\\nmore obtain a response from the deprived eye.\\n\\n\\x0c690\\n\\nWe find, consistent with previous theory and experiment,\\nthat most learning can occur in the LGN-cortical synapse, for\\ninhibitory (cortico-cortical) synapses need not modify.\\nSome\\nnon-modifiable LGN-cortical synapses are required.\\nTHE MEAN FIELD APPROXIMATION AND\\nARTIFICIAL NEURAL NETWORKS\\nThe mean field approximation may be applied to networks in\\nwhich the cortico-cortical feedback is a general function of cell\\nactivity. In particular, the feedback may measure the difference\\nbetween the network activity and memories of network activity.\\nIn this way, a network may be used as a content addressable\\nmemory.\\nWe have been discussing the properties of a mean\\nfield network after equilibrium has been reached. We now focus\\non the detailed time dependence of the relaxation of the cell\\nactivity to a state of equilibrium.\\nHopfield8 introduced a simple formalism for the analysis of\\nthe time dependence of network activity.\\nIn this model,\\nnetwork activity is mapped onto a physical system in which the\\nstate of neuron activity is considered as a \\'particle\\' on a potential\\nenergy surface.\\nIdentification of the pattern occurs when the\\nactivity \\'relaxes\\' to a nearby minima of the energy.\\nThus\\nmlmma are employed as the sites of memories. For a Hopfield\\nnetwork of N neurons, the intra-layer connectivity required is of\\norder N2. This connectivity is a significant constraint on the\\npractical implementation of such systems for large scale\\nproblems. Further, the Hopfield model allows a storage capacity\\nwhich is limited to m < N memories 8, 9. This is a result of the\\nproliferation of unwanted local minima in the \\'energy\\' surface.\\nRecently, Bachmann et al. l 0, have proposed a model for the\\nrelaxation of network activity in which memories of activity\\npatterns are the sites of negative \\'charges\\', and the activity\\ncaused by a test pattern is a positive test \\'charge\\'. Then in this\\nmodel, the energy function is the electrostatic energy of the\\n(unit) test charge with the collection of charges at the memory\\nsites\\n\\nE = -IlL ~ Qj I J-l- Xj I - L,\\nJ\\n\\n(14)\\n\\n\\x0c691\\n\\nwhere Jl (0) is a vector describing the initial network activity\\ncaused by a test pattern, and Xj\\' the site of the jth memory. L is\\na parameter related to the network size.\\nThis model has the advantage that storage density is not\\nrestricted by the the network size as it is in the Hopfield model,\\nand in addition, the architecture employs a connectivity of order\\nm x N.\\nNote that at each stage in the settling of Jl (t) to a memory\\n(of network activity) Xj\\' the only feedback from the network to\\neach cell is the scalar\\n~\\n\\nJ\\n\\nQ. I Jl- X? I - L\\nJ\\n\\nJ\\n\\n(15)\\n\\nThis quantity is an integrated measure of the distance of the\\ncurrent network state from stored memories.\\nImportantly, this\\nmeasure is the same for all cells; it is as if a single virtual cell\\nwas computing the distance in activity space between the\\ncurrent state and stored states. The result of the computation is\\nThis is a\\nthen broadcast to all of the cells in the network.\\ngeneralization of the idea that the detailed activity of each cell in\\nthe network need not be fed back to each cell.\\nRather some\\nglobal measure, performed by a single \\'effective\\' cell is all that is\\nsufficient in the feedback.\\nDISCUSSION\\n\\nWe have been discussing a formalism for the analysis of\\nnetworks of ideal neurons based on a mean field approximation\\nof the detailed activity of the cells in the network. We find that\\na simple assumption concerning the spatial distribution of the\\npattern preferences of the cells allows a great simplification of\\nthe analysis. In particular, the detailed activity of the cells of\\nthe network may be replaced with a mean field that in effect is\\ncomputed by a single \\'effective\\' cell.\\nFurther, the application of this formalism to the cortical layer\\nIV of visual cortex allows the prediction that much of learning in\\ncortex may be localized to the LGN-cortical synaptic states, and\\nthat cortico-cortical plasticity is relatively unimportant. We find,\\nin agreement with experiment, that monocular deprivation of\\nthe cortical cells will drive closed-eye responses to zero, but\\nchemical blockage of the cortical inhibitory pathways would\\nreveal non-zero closed-eye synaptic states.\\n\\n\\x0c692\\n\\nFinally, the mean field approximation allows the development\\nof single layer models of memory storage that are unrestricted\\nin storage density, but require a connectivity of order mxN. This\\nis significant for the fabrication of practical content addressable\\nmemories.\\nACKNOWLEOOEMENTS\\nI would like to thank Leon Cooper for many helpful discussions\\nand the contributions he made to this work.\\n\\n*This work was supported by the Office of Naval Research and\\nthe Army Research Office under contracts #NOOOI4-86-K-0041\\nand #DAAG-29-84-K-0202.\\n\\nREFERENCES\\n[1] Bienenstock, E. L., Cooper, L. N & Munro, P. W. (1982) 1.\\nNeuroscience 2, 32-48.\\n[2] Scofield, C. L. (I984) Unpublished Dissertation.\\n[3] Cooper, L. N, Munro, P. W. & Scofield, C. L. (1985) in Synaptic\\nModification, Neuron Selectivity and Nervous System\\nOrganization, ed. C. Levy, J. A. Anderson & S. Lehmkuhle,\\n(Erlbaum Assoc., N. J.).\\n[4] Cooper, L. N & Scofield, C. L. (to be published) Proc. Natl. Acad.\\nSci. USA ..\\n[5] Singer, W. (1977) Brain Res. 134, 508-000.\\n[6] Bear, M. F., Schmechel D. M., & Ebner, F. F. (1985) 1. Neurosci.\\n5, 1262-0000.\\n[7] Mower, G. D., White, W. F., & Rustad, R. (1986) Brain Res. 380,\\n253-000.\\n[8] Hopfield, J. J. (1982) Proc. Natl. A cad. Sci. USA 79, 2554-2558.\\n[9] Hopfield, J. J., Feinstein, D. 1., & Palmer, R. O. (1983) Nature\\n304, 158-159.\\n[10] Bachmann, C. M., Cooper, L. N, Dembo, A. & Zeitouni, O. (to be\\npublished) Proc. Natl. Acad. Sci. USA.\\n\\n\\x0c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVqqEBsSNmMp"
      },
      "outputs": [],
      "source": [
        "newText = re.sub('[^\\w\\s\\d\\.]','',text)\n",
        "newText = ' '.join(newText.split())\n",
        "newText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AULYQIJBePSf",
        "outputId": "16ef5471-0f47-4541-8a5f-bbc84a12377d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "id               0\n",
              "year             0\n",
              "title            0\n",
              "event_type    1502\n",
              "pdf_name         0\n",
              "abstract         0\n",
              "paper_text       0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G1pssH2e7pQ"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    newText = text.lower()\n",
        "    newText = re.sub('[^\\w\\s\\d\\.]','',newText)\n",
        "    newText = ' '.join(newText.split())\n",
        "    tokens = [w for w in newText.split() if not w in STOP_WORDS]\n",
        "    long_words=[]\n",
        "    for i in tokens:\n",
        "        if len(i)>=3:\n",
        "            long_words.append(i)   \n",
        "    return (\" \".join(long_words)).strip()\n",
        "\n",
        "after_text = []\n",
        "for t in data['paper_text']:\n",
        "    after_text.append(clean_text(t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qY_fAMH3hcoM"
      },
      "outputs": [],
      "source": [
        "def clean_abstract(text):\n",
        "  newText = text.lower()\n",
        "  newText = re.sub('[^\\w\\s\\d\\.]','',newText)\n",
        "  newText = ' '.join(newText.split())\n",
        "  newText = '_START_ '+ newText + ' _END_'\n",
        "  return newText\n",
        "\n",
        "after_abstract = []\n",
        "for t in data['abstract']:\n",
        "    after_abstract.append(clean_abstract(t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZkicvK6smTO"
      },
      "outputs": [],
      "source": [
        "data['clean_text'] = after_text\n",
        "data['clean_abstract'] = after_abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dB9etdxuywDD"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "4db30c950a9c4c598102cf71920282ae",
            "6bebb4fcae1c4cb7a0517c248f7853c8",
            "8604cdd1b83d457b9dc8768a8b8724a9",
            "41efd80871264749857e7168244672bf",
            "171dc9c509db40ed8824a24e17de2069",
            "1087d6bb37324ab687cc11fe1d7df7e4",
            "55363d0b01954e6eb81aa4e2334ded12",
            "afb0ee99dc654dd0b0886cf7120c774d",
            "d58c76f502f548f1aecdb5de2a8e5ba6",
            "19f971159942481aabb62c16540c5197",
            "767ef8fc43c3435399927c749c98d071",
            "47c4c491ef0546f5803ad1106b9f40c6",
            "286b287f92394cf9a46fca98bcb1d302",
            "1440b743ca5b4b9f9853de78d9556b05",
            "36c6422a34e9495ab45d6d47b6d55643",
            "1a175943adee4997ae06258b14c9a89a",
            "5f0b13d323af45b3ab50ab7841b4b397",
            "2d6a561084ba48ad9a43779d9a187524",
            "d935b4debbe140c1820b20a0ac102fc9",
            "72066fb179eb470fb11620c6cad9a7d1",
            "3ce38325f22d4821bd7835e74a7d1c72",
            "c42540f3eec349bebc83c01f9d266643"
          ]
        },
        "id": "s3l_leF4yOH3",
        "outputId": "00d5662f-0e1c-416b-e8a2-a6d86ade6166"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4db30c950a9c4c598102cf71920282ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3924 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47c4c491ef0546f5803ad1106b9f40c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3924 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data.clean_text = data.clean_text.progress_apply(lambda x: x.split())\n",
        "data.clean_abstract = data.clean_abstract.progress_apply(lambda x: x.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOJoWGcfy8CK",
        "outputId": "b7ed128f-5a10-4a5d-8025-ab050410cdcc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "941     [algorithms, nonnegative, matrix, factorizatio...\n",
              "1067    [characterizing, neural, gain, control, spiket...\n",
              "2384    [competition, adds, complexity, judy, goldsmit...\n",
              "2385    [efficient, principled, learning, thin, juncti...\n",
              "2388    [regularized, boost, semisupervised, learning,...\n",
              "                              ...                        \n",
              "6943    [separability, loss, functions, revisiting, di...\n",
              "6944    [maxing, ranking, assumptions, moein, falahatg...\n",
              "6945    [clustering, networkvalued, data, soumendu, su...\n",
              "6946    [general, framework, robust, interactive, lear...\n",
              "6947    [multiview, matrix, factorization, linear, dyn...\n",
              "Name: clean_text, Length: 3924, dtype: object"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgJtsN49y-Ff",
        "outputId": "8902060e-ed94-4ec8-9991-d1e356a1cc05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "941     [_START_, nonnegative, matrix, factorization, ...\n",
              "1067    [_START_, spiketriggered, averaging, technique...\n",
              "2384    [_START_, it, is, known, that, determinining, ...\n",
              "2385    [_START_, we, present, the, first, truly, poly...\n",
              "2388    [_START_, semisupervised, inductive, learning,...\n",
              "                              ...                        \n",
              "6943    [_START_, we, revisit, the, classical, analysi...\n",
              "6944    [_START_, pac, maximum, selection, maxing, and...\n",
              "6945    [_START_, community, detection, which, focuses...\n",
              "6946    [_START_, we, propose, a, general, framework, ...\n",
              "6947    [_START_, we, consider, maximum, likelihood, e...\n",
              "Name: clean_abstract, Length: 3924, dtype: object"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.clean_abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iKJ30WBZRdnW"
      },
      "outputs": [],
      "source": [
        "data.paper_text.iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmeoTzV8zOai"
      },
      "outputs": [],
      "source": [
        "data.to_csv('paper_new.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "paper_data_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1087d6bb37324ab687cc11fe1d7df7e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1440b743ca5b4b9f9853de78d9556b05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d935b4debbe140c1820b20a0ac102fc9",
            "max": 3924,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72066fb179eb470fb11620c6cad9a7d1",
            "value": 3924
          }
        },
        "171dc9c509db40ed8824a24e17de2069": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19f971159942481aabb62c16540c5197": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a175943adee4997ae06258b14c9a89a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "286b287f92394cf9a46fca98bcb1d302": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f0b13d323af45b3ab50ab7841b4b397",
            "placeholder": "",
            "style": "IPY_MODEL_2d6a561084ba48ad9a43779d9a187524",
            "value": "100%"
          }
        },
        "2d6a561084ba48ad9a43779d9a187524": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36c6422a34e9495ab45d6d47b6d55643": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ce38325f22d4821bd7835e74a7d1c72",
            "placeholder": "",
            "style": "IPY_MODEL_c42540f3eec349bebc83c01f9d266643",
            "value": " 3924/3924 [00:00&lt;00:00,  4.37it/s]"
          }
        },
        "3ce38325f22d4821bd7835e74a7d1c72": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41efd80871264749857e7168244672bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19f971159942481aabb62c16540c5197",
            "placeholder": "",
            "style": "IPY_MODEL_767ef8fc43c3435399927c749c98d071",
            "value": " 3924/3924 [00:01&lt;00:00, 3046.66it/s]"
          }
        },
        "47c4c491ef0546f5803ad1106b9f40c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_286b287f92394cf9a46fca98bcb1d302",
              "IPY_MODEL_1440b743ca5b4b9f9853de78d9556b05",
              "IPY_MODEL_36c6422a34e9495ab45d6d47b6d55643"
            ],
            "layout": "IPY_MODEL_1a175943adee4997ae06258b14c9a89a"
          }
        },
        "4db30c950a9c4c598102cf71920282ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6bebb4fcae1c4cb7a0517c248f7853c8",
              "IPY_MODEL_8604cdd1b83d457b9dc8768a8b8724a9",
              "IPY_MODEL_41efd80871264749857e7168244672bf"
            ],
            "layout": "IPY_MODEL_171dc9c509db40ed8824a24e17de2069"
          }
        },
        "55363d0b01954e6eb81aa4e2334ded12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f0b13d323af45b3ab50ab7841b4b397": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bebb4fcae1c4cb7a0517c248f7853c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1087d6bb37324ab687cc11fe1d7df7e4",
            "placeholder": "",
            "style": "IPY_MODEL_55363d0b01954e6eb81aa4e2334ded12",
            "value": "100%"
          }
        },
        "72066fb179eb470fb11620c6cad9a7d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "767ef8fc43c3435399927c749c98d071": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8604cdd1b83d457b9dc8768a8b8724a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afb0ee99dc654dd0b0886cf7120c774d",
            "max": 3924,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d58c76f502f548f1aecdb5de2a8e5ba6",
            "value": 3924
          }
        },
        "afb0ee99dc654dd0b0886cf7120c774d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c42540f3eec349bebc83c01f9d266643": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d58c76f502f548f1aecdb5de2a8e5ba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d935b4debbe140c1820b20a0ac102fc9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}